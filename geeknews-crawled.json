[
  {
    "title": "Ask GN: AI가 오케스트레이션까지 하는 시대, 팀 리더의 역할은 어디까지일까요?",
    "content": "새해가 되면서 팀 운영에 대해 고민이 많아졌습니다.\n특히 최근 AI 발전 속도가 워낙 빠르다 보니, 팀이 일하는 방식 자체를 재설계해야 하는 시점이 아닌가라는 생각이 자주 듭니다.\n기존의 매니징 역할 중 오케스트레이션 부분이 있는데,\n이제는 그 역할 중 상당 부분을 AI가 아주 효율적으로 수행해주고 있는 것 같고요.\n실제로는 팀원에게 설명하고 조율하는 것보다, AI에게 잘 맥락을 설명해 작업을 맡기는 게 더 저비용일 때도 종종 있습니다.\n그래서 요즘 가장 고민되는 지점은 다음입니다.\n- 팀 리더로서 사람과 AI 사이의 역할 분배를 어떻게 가져가야 할지\n- 단기적인 생산성 극대화와, 장기적인 팀 성장/역량 축적을 어떻게 동시에 고려해야 할지\n- AI 활용을 전제로 한 팀 운영 전략을 어디까지 재정의해야 하는지\n비슷한 고민을 하고 계신 분들은\n요즘 팀 운영을 어떤 방향으로 가져가고 계신지,\n혹은 기준이나 원칙이 있다면 공유해주시면 감사하겠습니다.",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25442",
    "category": "AI"
  },
  {
    "title": "펌웨어 캐시 동기화 문제로 발생한 와치독 리셋 분석",
    "content": "ESP32 기반 펌웨어를 개발하면서,\n플래시 암호화(Flash Encryption)와 PSRAM을 함께 사용하는 환경에서\nInterrupt Watchdog Reset이 발생하는 문제를 겪었는데, 이 문제를 해결하는 과정을 정리한 글입니다.\nESP32는 IoT·임베디드 환경에서 널리 사용되는 MCU로,\nRTOS 기반에서 TLS 네트워크 통신, 파일시스템, OTA 업데이트 등을 제공해\n네트워크 연결이 필요한 임베디드 애플리케이션을 단일 칩으로 구현할 수 있습니다.",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25441",
    "category": "트렌드"
  },
  {
    "title": "Show GN: aipack: BGE-M3 기반의 중립적 시맨틱 청킹된 파킷 생성기 + MCP 서버",
    "content": "BGE-M3, MarkItDown, 그리고 마크다운 구조 파서를 이용해 시맨틱 청킹을 수행하고, 그 결과를 Parquet 파일에 저장하는 aipack 프레임워크의 첫 버전을 릴리스합니다. 모델과 데이터베이스에 종속되지 않는 중립적 상태를 유지하여 언제든 재사용할 수 있는 파일 포맷을 기반으로 RAG를 구현하고, MCP 서버까지 구동할 수 있도록 설계했습니다.\naipack의 지향점은 NPU나 GPU에 의존하지 않는 RAG를 구현함과 동시에, 향후 다양한 RAG 구조로 확장하기 용이한 환경을 만드는 데 방점이 찍혀 있습니다. \"고품질의 Parquet 파일을 만들어낼 수 있다면 무엇이든 할 수 있다\"는 전제 아래, 업계에서 흔히 쓰이는 RAG 파이프라인을 디커플링(Decoupling)해본 실험적 프로젝트입니다.\n평소 생각했던 아이디어를 구현해본 것이라 어설픈 부분이 많지만 많은 피드백과 의견 주시면 그것을 토대로 더 나은 프로젝트를 빌드업해보겠습니다!",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25440",
    "category": "AI"
  },
  {
    "title": "테슬라 4680 배터리 공급망 붕괴, 파트너사 계약가치 99% 감액",
    "content": "- \n테슬라 4680 배터리 공급망의 핵심 연결고리가 끊어지며, 한국의 L&F가 테슬라와의 공급 계약 가치를 **99%** 이상 감액\n- L&F는 2023년 **29억** 달러 규모의 고니켈 양극재 공급 계약을 체결했으나, 이번에 계약 가치를 **7,386달러**로 축소\n- 감액 사유는 “공급 물량 변경”으로 명시됐으며, 4680 셀과 이를 사용하는 사이버트럭 판매 부진이 주요 배경으로 지목\n- 사이버트럭은 연간 25,000대 수준의 판매에 그치며, 테슬라의 4680 셀 수요 급감으로 이어짐\n- 4680 셀은 한때 배터리 비용 절반 절감과 **2만**5천 달러 전기차 실현의 핵심 기술로 기대됐으나, 현재는 상용화 난항과 생산 축소 상황\n---\nL&F 계약 감액과 공급망 붕괴\n- \nL&F는 테슬라와 체결한 **29억** 달러 규모의 고니켈 양극재 공급 계약 가치를 **7,386달러**로 감액\n이는 **99%** 이상 축소된 수치로, 사실상 계약 취소 수준\n- L&F는 감액 이유로 “공급 물량 변경”만을 언급\n- 이 계약은 테슬라의 4680 배터리 셀 생산 확대를 위한 핵심 공급망으로 평가되었음\n4680 셀은 배터리 비용 절감과 저가형 전기차 실현을 목표로 한 기술\n- 그러나 해당 계획은 이후 철회됨\n사이버트럭 판매 부진과 4680 셀 수요 감소\n- 현재 사이버트럭만이 테슬라의 자체 4680 셀을 사용하는 차량\n연간 생산 능력은 **25만** 대이지만, 실제 판매는 **2만**~**2만**5천 대 수준\n- 테슬라는 재고 증가로 인해 할인 금융 및 **0%** 할부를 도입\n2025년 9월에는 가장 저렴한 트림을 단종\n- 판매 부진으로 4680 셀 생산 축소, 이에 따라 L&F의 공급처 상실\n\n---\n\n## Electrek의 분석\n- \n4680 프로그램의 실패 조짐이 뚜렷하다는 평가\n4680 셀은 한때 “성배(Holy Grail) ”로 불리며, **54%** 주행거리 증가와 **56%** 비용 절감을 목표로 했음\n- 그러나 5년이 지난 지금도 건식 전극 공정의 대량생산 어려움으로 상용화 지연\n- 현재 4680 셀은 저판매량 픽업트럭에만 적용되고 있으며, 계약 **99%** 감액은 사실상 생산 축소 신호로 해석됨\n업계 반응 및 경쟁사 동향\n- \nBMW, Rivian 등 경쟁사는 이미 46XX 규격 셀을 삼성, LG 등으로부터 공급받아 양산 차량에 적용\n이들은 셀-투-팩(cell-to-pack) 및 구조적 배터리 팩 설계를 도입\n- 테슬라의 다른 차량들은 4680 셀이나 구조적 배터리 팩을 사용하지 않음\n사이버트럭의 주행거리도 초기 홍보치인 500마일에 미달\n향후 전망: ‘사이버캡(Cybercab)’ 프로젝트\n- 테슬라의 사이버캡 역시 4680 셀을 사용할 예정이나, 2026년 초 출시 목표\n운전대 없는 자율주행차로 계획됐으나, 레벨 4 자율주행 미해결 상태\n- 상용화되더라도 사이버트럭보다 낮은 생산량 예상\n배터리 원자재 시장 상황\n- \n배터리 공급망과 핵심 광물 시장이 빠르게 변화 중이며, 중국이 대부분의 공급망을 통제\n각 광물의 잔여 매장량과 공급 전망이 주요 변수로 언급됨",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25439",
    "category": "AI"
  },
  {
    "title": "왜 AI 에이전트는 계속 실패하는가 — 문제는 모델이 아니라 ‘세계(World) 설계’다",
    "content": "LLM 기반 AI 에이전트를 실제로 만들어보면,\n어느 순간부터 항상 비슷한 벽에 부딪히게 됩니다.\n- 분명히 모델은 똑똑해졌는데\n- 실행은 계속 불안정하고\n- 왜 그렇게 동작했는지는 설명할 수 없고\n- 같은 입력에서도 결과가 달라집니다\n그래서 보통은 이렇게 결론을 내립니다.\n“모델이 아직 부족하구나. 더 큰 모델을 써보자.”\n하지만 실제로 여러 시행착오를 거치며 느낀 건,\n문제의 핵심은 모델의 지능이 아니라\n에이전트가 동작하는 ‘세계(World)’가 설계되지 않았다는 점이었습니다.\n---\n문제의 본질: 세계가 모델 머릿속에만 있다\n많은 에이전트 아키텍처에서\n상태, 규칙, 행동 가능성 같은 것들이\n모두 암묵적으로 모델의 추론 안에 들어가 있습니다.\n즉,\n- 무엇이 가능한지\n- 왜 어떤 행동이 실패했는지\n- 상태가 언제 바뀌었는지\n이 모든 걸 모델이 “기억하고 추론해주길” 기대합니다.\n이 구조에서는\n아무리 모델이 좋아져도\n디버깅, 재현성, 설명 가능성을 확보하기 어렵습니다.\n---\n관점 전환: World-Centric Architecture\n그래서 이 글에서는 관점을 뒤집어,\n모델 중심(Intelligence-Centric) 이 아니라\n세계 중심(World-Centric) 으로 에이전트를 설계하는 방식을 제안합니다.\n핵심 아이디어는 단순합니다.\n- 세계는 모델 밖에 명시적으로 존재해야 하고\n- 상태는 Snapshot으로 고정되며\n- 상태 변경은 Patch/Apply라는 단 하나의 경로로만 일어나고\n- “이 행동이 가능한가?”는 구조적으로 계산되어야 합니다\n그리고 가장 중요한 원칙은 이 문장입니다.\n지능은 실행하지 않고, 제안만 해야 한다\n모델은 “무엇을 해보고 싶은지”를 제안할 수는 있지만,\n실제로 상태를 바꾸는 권한은 갖지 않습니다.\n---\n왜 이게 중요한가?\n이 구조에서는 흥미로운 일이 벌어집니다.\n- 불가능한 행동은 아예 실행 단계에 도달하지 못하고\n- 실패는 “모델이 멍청해서”가 아니라 구조적 이유로 설명되며\n- 심지어 행동 선택이 랜덤이어도 시스템은 깨지지 않습니다\n왜냐하면\n정합성(correctness)은 모델의 추론이 아니라\n세계의 규칙과 상태 모델이 보장하기 때문입니다.\n이건 연구용 데모보다\n“운영 가능한 시스템”에 가까운 접근이라고 생각합니다.\n---\n\n## 이 글은 무엇이 아닌가\n- 새로운 에이전트 프레임워크 튜토리얼 ❌\n- 모델 성능 비교 글 ❌\n- 프롬프트 엔지니어링 이야기 ❌\n대신,\n“우리는 왜 AI 에이전트를 이렇게 불안정하게 만들고 있는가?”\n라는 질문을 던지고 싶었습니다.\n---\n\n## 이 접근이\n기존의 상태 머신, 워크플로우 엔진, DSL, 혹은 PL 관점에서\n어떻게 보이는지도 궁금합니다.\n“결국 이건 무엇으로 환원되는가?”\n라는 관점에서의 의견이나 비판도 환영합니다.",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25438",
    "category": "AI"
  },
  {
    "title": "누군가가 당신의 제품을 극도로 싫어한다고 말할 때 (부정적 피드백 대응 가이드)",
    "content": "## 부정적 피드백의 메커니즘\n- 피드백은 신뢰도의 '온도 조절기(thermostat)' 역할\n과대평가 시 과소평가로 조정, 과소평가 시 과대평가로 조정\n- 피드백 내용: 정보(실질적 내용) + 감정(좌절감, 들어달라는 욕구)\n감정 해소 먼저 → 사실 논의 가능\n반박의 위험성\n- 반박은 비판자를 '잘못됐거나, 멍청하거나, 자격 없음'으로 몰아감 → 에스컬레이션 유발\n- 고객의 실질적 피드백일수록 반박은 상황 악화\n- 예시: CodeRabbit 사례\nAiden Bai의 공개 비판 → 엔지니어 문의 → CEO Harjot의 공격적 응답(무지하다 비하, 인디 개발자 모욕)\n- 결과: 추가 비판 증가, 브랜드 이미지 손상, 경쟁사 이득 → 부분 사과로 끝(부분 사과는 피할 것)\n올바른 대응 전략\n- 창업자로서 스스로 가장 혹독한 비판자 되기\n- 사용자 피드백 적극 수용(불공정하더라도)\n- 호기심, 책임감, 겸손 보여주기 → 비판자 놀라게 해 완화 유도(비판 철회 또는 완화 사례 발생)\n- 좋은 예시: Claude의 대응 방식\n단계별 대응 가이드 (TL;DR)\n- 정보와 감정 분리 → 좌절감 먼저 해소\n- 원칙부터 동의(품질 중요, 피드백 가치, 책임 인정)\n- 책임 과잉 인정 → 비판자 무장해제, 공격 면적 줄임\n- 사실 설명 시 투명하고 진지하게(방어적 태도 피함)\n- 사과 필요 시: 문제 간단히 요약 → 해결책 설명 → 재발 방지 → 종료(과도한 굽신거림 피함)\n추가 조언\n- 악의적 트롤은 무시 가능\n- 진짜 피드백에는 듣기 우선 → 밀어붙이기 피함\n- 감사 인사부터 시작(\"공유해 주셔서 감사합니다\")",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25437",
    "category": "AI"
  },
  {
    "title": "ManusAI가 Meta에 합류",
    "content": "- \n연구·자동화·복잡한 작업을 지원하는 범용 AI 에이전트로 유명한 Manus가 Meta에 합류한다고 발표\n- Manus의 AI 에이전트는 출시 후 몇 달 만에 **147조** 토큰을 처리하고 8천만 개 이상의 가상 컴퓨터를 생성함\n- Manus는 자율형 에이전트의 잠재력을 강조하며, 자사를 “실행 계층(execution layer)” 으로 정의\n고급 AI 기능을 확장 가능하고 신뢰할 수 있는 시스템으로 전환해 실제 환경에서 엔드투엔드 작업 수행을 가능하게 함\n- Meta 합류는 이러한 역할을 강화하고, AI 혁신을 가속화하는 기반이 될 것\n- 회사는 싱가포르에서 계속 운영되며, 기존 구독 서비스도 앱과 웹사이트를 통해 유지될 것\n- Meta와의 협력으로 자율형 AI 시스템의 확장성과 신뢰성을 강화하고, 향후 Meta 플랫폼의 수많은 기업과 개인 사용자에게 서비스를 확대할 계획\n- CEO Xiao Hong은 “Meta 합류로 더 강력하고 지속 가능한 기반 위에서 발전할 수 있게 되었다”고 언급\nManus의 운영 방식이나 의사결정 구조는 변하지 않는다고 강조\n- Meta와 Manus가 함께 미래를 만들어갈 것에 대한 기대감 표명\n- Manus는 “Less structure, more intelligence”라는 슬로건 아래, 지능적이고 유연한 AI 시스템 개발을 지속할 계획\n- \nMeta의 공식 발표 링크",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25436",
    "category": "AI"
  },
  {
    "title": "Karpathy의 프로그래밍 관련 발언: \"이렇게까지 뒤처진 느낌은 처음이다\"",
    "content": "- Andrej Karpathy가 현재의 프로그래밍 환경에서 자신이 크게 뒤처지고 있다고 강하게 느낀다고 밝힘\n- 프로그래머가 직접 작성하는 코드 비중은 줄어들고, 이미 존재하는 도구와 시스템을 어떻게 연결하고 조합하느냐가 핵심 역량으로 이동 중\n- 지난 1년간 등장한 도구들을 제대로 엮기만 해도 생산성이 10배 가까이 증가할 수 있음에도, 이를 활용하지 못하는 상태를 스스로 명백한 역량 부족(skill issue) 으로 인식\n- 에이전트, 서브에이전트, 프롬프트, 컨텍스트, 메모리, 권한, 툴, 플러그인, MCP, LSP, IDE 연동 등으로 구성된 새로운 추상화 계층을 이해하고 다뤄야 하는 상황에 놓였음\n- 설명서 없는 강력하지만 확률적이고 불완전한 도구가 갑자기 기존 엔지니어링과 뒤섞였고, 이 변화에 대응하지 않으면 직업 자체에서 뒤처질 수 있다고 경고\n---\n프로그래머로서 이렇게 뒤처진다는 느낌은 처음입니다.\n프로그래머가 기여하는 부분이 점점 더 드물어지고 단편화되면서, 프로그래밍 업계는 급격하게 재편되고 있습니다.\n지난 1년 동안 새롭게 등장한 기술들을 제대로 조합하기만 하면 10배는 더 강력해질 수 있을 것 같은데, 그 기회를 활용하지 못하는 건 분명 제 실력 부족 때문인 것 같습니다.\n에이전트, 서브에이전트, 프롬프트, 컨텍스트, 메모리, 모드, 권한, 도구, 플러그인, 스킬, 훅, MCP, LSP, 슬래시 명령어, 워크플로, IDE 통합 등 기존의 계층 외에도 익혀야 할 새로운 추상화 계층이 생겼습니다.\n게다가 근본적으로 확률적이고, 오류 발생 가능성이 높으며, 이해하기 어렵고, 끊임없이 변화하는 요소들이 갑자기 기존의 전통적인 엔지니어링 방식과 뒤섞이면서 발생하는 장단점을 포괄하는 사고방식을 구축해야 합니다.\n분명 어떤 강력한 외계 도구가 사용되었는데, 설명서도 없어서 모두가 어떻게 잡고 작동시켜야 할지 알아내야 하는 상황입니다.\n게다가 그 결과로 발생한 규모 9의 지진이 업계를 뒤흔들고 있습니다.\n뒤처지지 않으려면 소매를 걷어붙이세요.\n---\n“뒤처지고 있다”는 감각의 정체\n- 지금의 프로그래밍 직업은 급격한 재구성(refactoring) 단계에 들어섰다고 인식함\n- 인간 프로그래머가 기여하는 코드는 점점 드문 간격으로만 등장하는 요소가 되고 있음\n- 핵심 가치는 코드를 직접 쓰는 능력보다, 기존 시스템을 엮어 전체를 작동시키는 능력으로 이동 중임\n새로운 추상화 계층의 등장\n- 기존 언어, 런타임, 프레임워크 위에 AI 에이전트 중심의 상위 계층이 새롭게 추가됨\n- 이 계층은 프롬프트, 컨텍스트 관리, 메모리, 모드, 권한, 툴 호출, 워크플로우, IDE 통합 등으로 구성됨\n- 단일 기술이 아니라 전체 생태계를 관통하는 정신적 모델을 요구함\n불확실한 도구와 함께하는 엔지니어링\n- 새 도구들은 확률적이고, 오류 가능성이 있으며, 내부 동작을 완전히 이해하기 어려움\n- 그럼에도 기존의 “정확하고 결정적인” 엔지니어링 시스템과 함께 사용해야 하는 상황임\n- 이는 기존 프로그래밍 패러다임과 근본적으로 다른 사고방식 전환을 요구함\n설명서 없는 도구와 직업적 지진\n- 강력한 외계 도구가 갑자기 모두에게 주어졌지만 사용 설명서는 존재하지 않음\n- 각자가 스스로 사용법을 터득해야 하며, 그 사이 직업 전체가 규모 9에 해당하는 변화를 겪고 있음\n- Karpathy는 이 변화에 대응하지 않으면 의도치 않게 뒤처질 수 있음을 분명히 함\n결론적 메시지\n- 현재의 변화는 일시적 유행이 아니라 프로그래밍 직업 자체의 재편\n- 생산성 도약을 활용하지 못하는 상태는 환경 문제가 아니라 개인의 준비 부족으로 귀결될 수 있음\n- 뒤처지지 않기 위해서는 소매를 걷어붙이고, 새로운 계층을 적극적으로 학습해야 한다는 메시지로 마무리",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25434",
    "category": "AI"
  },
  {
    "title": "독일 인터넷 서비스 제공업체들이 차단한 도메인 목록",
    "content": "- 독일의 CUII(Internet-Inhalte-Sperrungsstelle) 가 차단 대상으로 지정한 여러 도메인 목록이 공개됨\n- 목록에는 Annas Archive, bs.to, CannaPower, Buffstreams 등 다양한 사이트가 포함됨\n- 각 도메인 옆에는 차단일자가 표기되어 있으며, 일부는 2022년부터 2025년까지의 날짜를 가짐\n- 표의 주석에 따르면 날짜는 차단 권고일 또는 데이터베이스 최초 등록일을 의미함\n- 이 목록은 독일 내 ISP 차단 현황을 투명하게 보여주는 자료로 활용 가능함\n---\n\n## CUII에 의해 차단된 도메인 목록\n- \n표에는 도메인명과 차단일자(Blockiert am) 두 열이 포함되어 있음\n예시로 annas-archive.li, annas-archive.org, annas-archive.se가 2025년 9월 30일에 차단\n- \nbs.to, canna-power.to, canna.to는 2022년 4월 4일에 차단\n- \ncine.to는 2022년 7월 14일, filmfans.org는 2023년 9월 8일에 차단\n- \nbuffsports.io는 2025년 6월 30일, buffstreams.sx는 2024년 2월 26일, fbstreams.pm은 2024년 12월 18일에 차단\n- \nde.annas-archive.li, de.annas-archive.org, de.annas-archive.se, en.annas-archive.org 등 언어별 하위 도메인도 포함됨\n- \n표 하단 주석에는 날짜가 정확하지 않을 수 있음이 명시되어 있음\n일부 날짜는 차단 권고가 이루어진 시점을, 일부는 도메인이 데이터베이스에 처음 추가된 시점을 의미함\n목록의 구조와 목적\n- 사이트는 CUII Liste (CuiiListe.de) 라는 이름으로 운영되며, 독일 내 ISP 차단 도메인 현황을 공개함\n- 각 도메인의 차단 시점을 기록해 검열 투명성 확보에 기여함\n- 별도의 설명이나 차단 사유는 표에 포함되어 있지 않음",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25433",
    "category": "트렌드"
  },
  {
    "title": "Show GN: 실시간 금융 시장 지표 모니터링 및 AI 기반 시장 분석 대시보드",
    "content": "크리스마스 연휴때 간단히 만들어 봤습니다.\nWeb - https://trade-dashboard-theta.vercel.app/\nGitHub - https://github.com/Jae12ho/trade-dashboard",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25432",
    "category": "AI"
  },
  {
    "title": "Google Ads는 죽었다. 이제 어디로 가야 할까?",
    "content": "- 지난 3개월간 엔터테인먼트 사업 매출이 **50%** 감소했으며, 주요 리드 소스였던 Google Ads의 효과가 급격히 떨어짐\n- 광고 예산을 늘리고 여러 캠페인을 병행했지만 성과가 전혀 없었고, 심지어 Google이 제공한 보너스 예산으로도 결과가 없었음\n- 예산을 10배로 늘렸음에도 ‘제로 리턴’ 을 기록하며 광고비 투입을 중단함\n- 이후 TikTok·Instagram 광고, 이메일 뉴스레터, 오프라인 홍보 활동 등으로 전환 시도 중\n- Google 광고 의존이 무너진 현실에서 소규모 비즈니스의 생존 전략 전환 필요성이 드러남\n---\n\n## Google Ads의 급격한 성과 하락\n- 지난 3개월 동안 엔터테인먼트 사업 매출이 **50%** 감소\n10년 이상 Google Ads를 주요 리드 소스로 활용해 왔으며, 광고 관리 경험이 충분함\n- 매달 키워드와 캠페인을 조정하며 성과를 유지해 왔으나, 최근 1년간 예산을 늘려도 효과가 없었음\n주간 단위로 분석 빈도를 높이고, 다양한 설정의 캠페인을 병행했지만 결과 없음\n- Google이 월 예산의 5배에 해당하는 보너스 광고비를 제공했으나, 성과 변화 없음\n남은 예산을 모두 소진하기 위해 광고 예산을 10배로 증액했으나 수익 0\n- 광고비가 모두 소진된 후 추가 투입을 중단, “이제 어디로 가야 하는가”라는 질문 제기\n새로운 마케팅 시도\n- \nTikTok과 Instagram 같은 짧은 영상 플랫폼에서 젊은 세대가 정보를 얻는다는 연구를 인용\n이에 따라 해당 플랫폼에서 광고 실험을 진행 중\n- 고객의 **50%**가 재구매 고객으로, 이를 유지하기 위해 이메일 뉴스레터 발송을 정례화\n최근 2개월간 뉴스레터를 꾸준히 발송하며 고객과의 접점 강화 시도\n- \n오프라인 홍보도 병행\n지역 마켓에서 무료 공연을 진행하고, 명함 배포를 통해 인지도 확보 계획\n사업 다각화 및 생존 시도\n- \nMagic Poi 프로젝트 관련 제품 제작 및 판매를 위한 준비 진행\n필요한 자재를 이미 주문함\n- 현재 재정적으로 어려운 상황을 언급하며, 웹사이트나 IoT 프로젝트 개발 의뢰를 받을 수 있다고 밝힘\nAI 보조를 통한 빠른 개발 속도를 강조\n핵심 메시지\n- Google Ads의 효율이 급격히 떨어지면서 소규모 비즈니스의 온라인 마케팅 구조가 흔들림\n- 기존의 디지털 광고 의존에서 벗어나 다양한 채널 실험과 직접 고객 접촉 방식으로 전환 중\n- \nGoogle 중심 생태계의 한계가 드러나며, 새로운 생존 경로 탐색의 필요성이 부각됨",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25431",
    "category": "AI"
  },
  {
    "title": "Z80-μLM, 40KB 안에 담긴 ‘대화형 AI’",
    "content": "- \nZ80-μLM은 1976년산 Z80 프로세서(4MHz, 64KB RAM) 위에서 동작하는 초소형 대화형 인공지능 모델\n- \n양자화 인식 학습(QAT) 을 적용해 2비트 가중치와 16비트 정수 연산만으로 문자 단위 텍스트 생성 수행\n- 약 40KB 크기의 .COM 실행 파일 안에 추론 엔진, 가중치, 채팅 UI가 모두 포함\n- \ntrigram 해시 인코딩으로 입력을 128개의 버킷으로 변환해 오타나 단어 순서에 강인한 응답 생성\n- 복잡한 문맥 이해는 불가능하지만, 제한된 8비트 환경에서도 작동하는 실험적 AI 모델로 주목받음\n---\n\n## 프로젝트 개요\n- Z80-μLM은 레트로컴퓨팅 환경에서 구동 가능한 초소형 언어 모델\n64KB RAM을 가진 Z80 CPU에서 작동하며, 문자 단위 대화형 응답을 생성\n- 모델, 추론 코드, UI를 포함한 전체 크기가 약 40KB\n- 프로젝트의 핵심 질문은 “얼마나 작게 만들어도 개성을 유지할 수 있는가”였으며, 자가 호스팅 배포도 가능\n- \n튜링 테스트 수준에는 미치지 않지만, 단순한 대화로 사용자에게 즐거움을 주는 형태\n두 가지 예제 포함\n- \ntinychat\n일상적인 Q&A 데이터로 학습된 간단한 챗봇\n인사, 자기소개, 일반 대화에 짧고 개성 있는 답변 제공\n- 예: “hello” → “HI”, “are you a robot” → “YES”, “do you dream” → “MAYBE”\n- \nguess\n20 Questions 게임 형태의 모델\n비밀 주제를 알고 있으며, YES/NO/MAYBE로 응답\n- 사용자가 정답을 맞히면 “WIN” 출력\n- 학습 데이터는 Ollama 또는 Claude API를 이용해 생성 가능하며, 클래스 분포 균형 도구 포함\n주요 기능\n- \nTrigram 해시 인코딩: 입력 텍스트를 128개의 버킷으로 해시, 오타 허용 및 단어 순서 무관\n- \n2비트 가중치 양자화: 각 가중치가 {-2, -1, 0, +1}, 1바이트당 4개 저장\n- \n16비트 정수 추론: Z80의 16비트 산술 연산 사용\n- \n~40KB .COM 파일: CP/M의 Transient Program Area(TPA)에 적합\n- \n자기회귀적 생성: 문자 단위로 출력 생성\n- \n부동소수점 연산 없음, 고정소수점 스케일링 사용\n- \n대화형 모드 지원: CHAT 명령으로 실행\n상호작용 방식\n- 모델은 입력을 ‘이해’하지 않지만, 입력의 형태(shape) 를 기반으로 반응\n입력 문장은 128개의 trigram 버킷으로 변환되어 의미적 유사성을 유지\n- 예: “hello there”와 “there hello”는 동일한 버킷 구조로 처리\n- 긴 문장이나 순서 의존 문장은 구분이 어려움\n- \n\n---\n\n## 짧은 응답의 의미\n1~2단어의 응답으로도 의외의 뉘앙스 표현 가능\nOK: 중립적 수락\n- \nWHY?: 질문 반박\n- \nR U?: 존재 의문\n- \nMAYBE: 불확실성\n- \nAM I?: 반사적 질문\n- 이러한 짧은 응답은 사용자가 맥락을 추론하도록 유도\n강점과 한계\n- \n강점\n짧은 입력에 대한 일관된 분류형 응답\n- \n오타·재구성·단어 순서 변화에 강함\n- \n어휘 선택을 통한 개성 표현\n- \n제한된 8비트 하드웨어에서도 실행 가능\n- \n한계\n새로운 문장 생성 불가\n- 다중 턴 문맥 추적 불가\n- 문법 이해 불가\n- 일반 지능 수준에는 미치지 않음\n아키텍처\n- \n입력층: 128개의 쿼리 버킷 + 128개의 컨텍스트 버킷\n- \n은닉층: 예시 구성 256 → 192 → 128\n- \n출력층: 문자셋의 각 문자당 1개 뉴런\n- \n활성화 함수: ReLU\n- \n양자화 제약\nZ80은 8비트 CPU지만, 16비트 레지스터 쌍(HL, DE, BC) 을 사용해 누산 및 활성화 수행\n- 가중치는 4개씩 1바이트에 저장(2비트 단위)\n- 16비트 누산기로 256개 입력 합산 시 오버플로 방지\n- 2비트 가중치로 인해 표현력은 제한적이며, QAT 없이는 불안정한 학습 결과 발생 가능\n- \nZ80 내부 루프\n추론의 핵심은 곱-누산 루프(MAC)\n가중치를 언팩하고, -2~-1~0~+1 값에 따라 누산기(ACC)에 더하거나 빼는 방식\n- 각 층 계산 후 오버플로 방지를 위해 2비트 오른쪽 시프트 수행\n- 전체 추론 과정은 문자 하나당 약 **10만** 회 연산 반복\n라이선스\n- \nMIT 또는 Apache-2.0 중 선택 가능",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25430",
    "category": "AI"
  },
  {
    "title": "GOG가 원 공동 창업자에게 인수됨",
    "content": "- CD PROJEKT의 공동 창업자 미하우 키친스키 가 GOG를 CD PROJEKT로부터 인수, GOG는 독립 회사로 계속 운영\n- GOG는 DRM 없는 게임 소유권 보장과 고전 게임 보존이라는 기존 철학을 유지\n- 사용자 계정, 데이터, 라이브러리, 오프라인 설치 기능 등은 변경 없이 유지\n- \nCD PROJEKT RED의 신작과 기존 게임은 계속 GOG 플랫폼에서 제공\n- 이번 인수는 GOG의 자유·독립·보존 중심 가치 강화와 장기적 지속 가능성 확보에 의미\n---\n\n## 인수 개요\n- \nMichał Kiciński, CD PROJEKT 및 GOG의 공동 창업자가 GOG를 CD PROJEKT로부터 인수\nGOG는 독립된 회사로 계속 운영되며, 사명은 “Make Games Live Forever” 유지\n- GOG는 이번 인수를 통해 자유, 독립, 통제, 게임 보존이라는 핵심 가치에 기반한 장기적 지원 확보\n- CD PROJEKT는 RPG 개발과 자사 브랜드 기반 콘텐츠 제작에 집중하기 위해 GOG 매각 결정\nGOG의 철학과 방향\n- GOG는 2007년 설립 당시부터 고전 게임을 다시 플레이할 수 있게 하고, 구매한 게임의 영구 소유권을 보장하는 것을 목표로 함\n- 현재 시장이 폐쇄적 생태계와 강제 클라이언트 중심으로 변하는 가운데, GOG는 자유로운 접근과 DRM-free 철학을 강화\n- 이번 변화는 고전 게임 보존, 현대 명작 기념, 미래의 클래식 창조라는 세 가지 축을 중심으로 전개\n사용자 영향\n- \nDRM-free 정책은 GOG의 핵심으로 유지되며, 사용자는 기존과 동일하게 오프라인 설치 파일과 라이브러리 접근 가능\n- \nGOG GALAXY 클라이언트는 여전히 선택 사항\n- \n사용자 데이터는 GOG가 계속 관리, 제3자나 인수자에게 공유되지 않음\n- \nGOG Patron 및 보존 프로그램 후원금은 GOG 내부에서만 사용되며 외부로 이전되지 않음\nCD PROJEKT와의 관계\n- \nCD PROJEKT RED의 게임은 계속 GOG에서 판매 및 배포\n향후 출시될 신작도 동일하게 GOG 플랫폼에서 제공 예정\n- 두 회사는 독립적으로 운영되지만, 협력 관계는 유지\n향후 계획과 커뮤니티\n- GOG는 윤리적이고 비착취적인 플랫폼을 지속 구축하며, 인디 개발자 지원 확대\n- 2026년에는 커뮤니티 참여 강화 프로그램과 보존 프로젝트 확대 계획 발표 예정\n- GOG는 “게임이 사라지지 않도록 하는 일”을 계속 수행하며, 사용자와 함께 이 사명을 이어갈 것임",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25429",
    "category": "스타트업"
  },
  {
    "title": "HTML 태그를 직접 만들어 사용할 수 있음",
    "content": "- HTML에서는 인식되지 않는 태그 이름을 사용해도 브라우저가 이를 일반 요소로 처리함\n- CSS에서 해당 태그 이름을 선택자로 지정하면 스타일 지정과 표시 제어가 가능함\n- 하이픈(-)이 포함된 이름을 쓰면 미래 HTML 표준과의 충돌을 방지할 수 있음\n- \n<div>나 <span> 대신 의미 있는 사용자 정의 태그를 쓰면 코드 가독성이 높아짐\n- 복잡한 중첩 구조에서도 태그 이름만으로 구조를 파악하기 쉬워, 유지보수가 용이함\n---\n\n## 사용자 정의 HTML 태그의 활용\n- \n브라우저는 알 수 없는 태그를 일반 블록 요소처럼 렌더링함\n이는 HTML 표준에 명시된 정상 동작으로, CSS에서 스타일을 지정하면 시각적으로 제어 가능\n- 예시로 <cool-thing> 같은 태그를 정의하고 CSS에서 cool-thing { ... } 형태로 꾸밀 수 있음\n- \n태그 이름에 하이픈(-) 을 포함하면 향후 HTML 표준에 추가될 가능성이 없어 충돌 위험이 없음\n예: <main-article>, <quote-body> 등\n가독성과 구조 개선\n- \n<div>나 <span> 대신 의미 있는 이름의 태그를 사용하면 코드 이해가 쉬워짐\n예를 들어 <div class=\"article-header\"> 대신 <article-header>를 사용 가능\n- 중첩된 <div> 구조에서는 닫는 태그 위치를 파악하기 어렵지만, 명시적 태그 이름을 쓰면 구조가 명확해짐\n<main-article> 내부에 <article-header>, <article-quote> 등으로 구성하면 DOM 구조 파악이 직관적임\n예시 코드\n- 기존 방식\n<div class=\"cool-thing\">\nHello, World!\n</div>\n- 사용자 정의 태그 방식\n<cool-thing>\nHello, World!\n</cool-thing>\nCSS에서 cool-thing { display: block; font-weight: bold; text-align: center; ... } 형태로 스타일 지정 가능\n결론\n- HTML 표준이 허용하는 유연한 태그 정의 기능을 활용하면,\n가독성 높은 구조적 마크업을 작성할 수 있음\n- 단, 기존에 정의된 의미 있는 태그가 있을 경우에는 기존 태그 사용이 우선임",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25428",
    "category": "AI"
  },
  {
    "title": "도이체반에 ‘납치’당하고 받은 보상은 1.50유로뿐",
    "content": "- 독일 철도 Deutsche Bahn(도이체반) 이용 중 열차가 예정 정차역을 지나쳐 멈추지 않아, 승객이 의도치 않게 다른 주(州)까지 이동한 사건\n- 열차는 쾰른에서 출발해 본을 거쳐 메켄하임으로 가던 중, Troisdorf 역 등록 누락으로 인해 정차하지 못하고 Neuwied까지 계속 운행\n- 승객은 35km 이동을 계획했으나 실제로는 63km 더 멀어진 Neuwied에 도착\n- 도이체반의 지연 보상 정책에 따라 받을 수 있는 금액은 **1.50유로**였으며, 최소 지급 기준 **4유로**에 미달해 실질적 보상 없음\n- 독일 철도 시스템의 운영 혼란과 보상 체계의 한계를 풍자적으로 드러낸 사례\n---\n열차 운행 중 발생한 ‘납치’ 사건\n- 2025년 12월 24일, 쾰른 중앙역에서 RE5 열차를 타고 메켄하임으로 향하던 중 발생\n출발 예정 시각은 15시 32분, 본 도착 예정은 15시 54분이었음\n- 열차는 20분 지연 출발했으나, 승객은 이를 ‘정상 범위’로 간주함\n- 운전사는 “본 주변 문제로 우회 운행 중”이라고만 안내했으며, 구체적 설명은 없었음\n승객들에게 쾰른 남역에서 하차 후 지하철을 이용하거나 Troisdorf에서 버스로 갈 것을 제안\n- 승객은 Troisdorf에서 아버지와 만나기로 계획했으나, 열차가 해당 역에 등록되지 않아 정차 불가 통보를 받음\n운전사는 이를 “커피 머신이 고장 났다”는 듯한 어조로 알림\n- 승객들은 침묵과 웃음을 반복하며 상황을 받아들임\n예상치 못한 장거리 이동\n- Troisdorf를 지나친 열차는 라인강 좌안 전체를 우회하며 Neuwied까지 직행\nTroisdorf에서 Neuwied까지 15개 역을 모두 통과\n- 승객은 “이제 승객이 아니라 화물 같다”고 표현\n- 한 영어권 승객은 “왜 멈추지 않느냐”고 묻자, “우리는 이 선로에 등록되지 않았다”는 답변을 들음\n그는 “나는 납치당했다”고 말하며 좌절을 표현\n- 작성자는 원래 목적지보다 더 멀리 떨어진 라인란트팔츠 주의 Neuwied에 도착\n도이체반의 시간·보상 기준\n- 도이체반의 공식 통계상 6분 미만 지연은 ‘정시 도착’ 으로 간주됨\n완전히 취소된 열차는 지연 통계에 포함되지 않음\n- 작성자는 Deutschlandticket 지연 보상 정책을 확인한 결과, 보상액은 **1.50유로**에 불과함\n최소 지급 기준 **4유로** 미만으로, 실질적 보상은 이루어지지 않음\n- 결과적으로 “납치당하고도 손해를 본 셈”이라 표현\n풍자적 결말과 맥락\n- 글 전반은 도이체반의 비효율적 운영과 고객 경험 부재를 풍자적으로 묘사\n“승객이 아니라 가축처럼 취급받는다”는 표현 사용\n- “열차가 존재하지 않으면 지연될 수도 없다”는 통계 방식 비판\n- 마지막에는 Neuwied 역 사진을 첨부하며, 사건의 아이러니한 결말을 강조\n- 전체적으로 독일 철도 시스템의 구조적 문제와 보상 제도의 불합리성을 유머러스하게 드러낸 사례",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25427",
    "category": "트렌드"
  },
  {
    "title": "FastLanes – 차세대 빅데이터 파일 포맷",
    "content": "- 최신 하드웨어(SIMD·GPU)를 전제로 설계된 오픈소스 컬럼 지향 파일 포맷\n- 분석·AI 워크로드에서 고처리량·저지연 데이터 접근을 목표로 함\n- \nParquet 대비 약 **40%** 높은 압축률, 최대 40배 빠른 디코딩 속도를 달성\n- 데이터 의존성을 최소화한 Lane 기반 레이아웃을 도입해 각 단위를 독립적으로 디코딩 가능\nSIMD·멀티코어 CPU·GPU에서 극단적인 데이터 병렬성 확보\n- \n명시적 SIMD 코드 없이도 자동 벡터화가 잘 동작하도록 설계\nCPU·GPU 캐시 특성을 고려한 소규모 배치 단위 접근 방식 채택\n- 압축을 완전히 풀지 않고 처리하는 부분 디컴프레션(partial decompression) 지원으로 데이터 엔진이 압축 상태 그대로 쿼리 실행 가능\n- \n다중 컬럼 압축(Multi-Column Compression, MCC) 을 통해 컬럼 간 상관관계를 활용\n기존 컬럼 저장 포맷의 단일 컬럼 한계를 보완하는 표현식 기반 인코딩 메커니즘 제공\n- 외부 라이브러리에 의존하지 않는 Zero-dependency 구조로 빌드 단순화\nC++, Python, Rust 등 주요 언어 바인딩 제공\n- \nCSV ↔ FastLanes 변환 API 내장\nread_csv() / to_fls()로 손쉬운 변환\n- \nread_fls() / to_csv()로 역변환 지원\n- GPU 디코딩, Apache Arrow·DuckDB 연동 등 차세대 데이터 스택과의 통합을 목표로 개발 중",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25426",
    "category": "AI"
  },
  {
    "title": "2x 마인드셋의 함정 - 베이 지역 창업자의 2025년 반성문",
    "content": "- 2025년은 겉으로는 매출 6배 성장을 했지만, 스스로는 성공이 아니라 실패로 평가함: 목표와 사고방식이 10x가 아니라 2x(안전하고 선형적인 성장)에 머물렀기 때문\n- \n2x 마인드셋의 징후: 작은 목표/선형 성장, 할 일이 늘어나는 구조, 없어져야 할 것을 개선, “괜찮다” 수준의 고객 만족, 불명확한 ICP, 트렌드 추격과 얕은 실행\n- 핵심 문제는 “고객이 늘수록 일이 늘어나는” 방식으로 성장했고, 누가 최고의 고객인지/왜 선택했는지를 명확히 말하지 못한 채 거절하지 못하는 결정을 반복한 것\n- 수평형(Horizontal) SaaS의 함정: 누구나 쓰게 만들려다 아무에게도 강렬하지 않은 제품이 되기 쉬움. “100명이 1번 쓰는 제품”이 아니라 \"1명이 100번 쓰는 제품\"이 되어야 함\n- 2025년에 시도한 첫 ICP(경영진 팀) 실험은 방향은 맞았지만 실행이 부족해 성공도 실패도 아닌 최악의 결과(애매한 결과)로 끝났고, 그 사이 “적당히 만족하는 고객”이 늘며 느린 성장으로 귀결됨\n---\n2x 마인드셋: ‘안전한 선택’이 만든 느린 성장 구조\n- 주 단위로 신규 고객과 단기 성장에 집중하면서, 고객이 늘수록 같은 방식으로 더 바빠짐\n- 여러 기회를 동시에 쫓고 깊이가 부족해짐: 트렌드/키워드/약속/전환 기대 등의 이유로 해야 할 일을 줄이지 못함\n- 결과적으로 일부 고객은 “좋다”가 아니라 “그럭저럭”이었고, 이탈도 발생하며 선형적 고객 수 증가로만 매출이 올라가는 구조가 됨\n사례: ‘모두와 친해지려다 누구에게도 특별하지 않다’\n- 100명 모두와 친해지려다 바쁘기만 했던 친구의 일화처럼, 제품도 다양한 고객을 모두 만족시키려 하다 특정 집단에게 압도적으로 사랑받지 못하는 상태가 됨\n- 수평형 SaaS는 범용성의 장점이 있지만, 초기에는 특히 사용성/기능/지원 부담이 폭증해 “대충 되는 제품”으로 흐르기 쉬움\n- Zapier 사례처럼, 범용 제품이라도 초기에 명확한 성공사례가 나오는 범위로 좁히는 집중이 필요하다는 교훈을 재확인\nICP 실험: 경영진을 타깃으로 했지만 ‘진짜 실험’이 아니었다\n- 가설: 경영진은 빠른 답과 요약이 필요하고, 조직 전반 질문을 던지는 역할이므로 “대화/데이터를 읽고 요약·주간 업데이트·의사결정 지원” 에이전트가 유효\n- 하지만 실행에서의 실패 요인:\nICP를 먼저 고르고 공략한 게 아니라, 주로 인바운드 미팅 위주로 만나며 고객 인터뷰가 부족\n- “경영진 ICP”를 말하면서도 다양한 고객을 계속 만나 로드맵이 잡다한 요청의 리스트로 변질\n- 팀 전체가 한 목표에 몰입하지 못하고, 예시 몇 개/가벼운 배포/광고 후 멈추며 “launch and learn”을 제대로 수행하지 못함\n- 결론: 성공도 실패도 아닌 애매한 상태로 남아, 계속 느리고 꾸준한 성장 + 일부 ‘그럭저럭 만족’ 고객이 누적됨\n10x 마인드셋: 선택지를 줄이고, 중요한 것만 남긴다\n- 10x 목표는 거의 불가능하게 느껴지지만, 그만큼 갈 수 있는 길이 줄어들어 잡일을 버리고 핵심에 집중하게 됨\n- 10x의 특징:\n목표가 크고 경로가 적음(집중이 강제됨)\n- “쓸모없는 바쁨”이 줄어듦\n- 고객 정의가 단순해지고, 소수라도 진짜로 사랑하는 고객이 늘어남\n- 40대 창업자로서의 결론: 안정 선택이 아니라, 경험과 명료함으로 큰 임팩트 결정을 반복하는 시기가 지금이라고 선언\n- 2026 계획: 중요하지 않은 **80%**를 버리고, 중요한 **20%**에 올인\n2025년에 남은 성과: 방향 전환의 기반 만들기\n- 시드 라운드 클로징\n- 디자인 파트너십을 통해 v2를 함께 정의·사용·피드백 받으며 구축 중(“UI 디자인”이 아니라 문제정의·검증 파트너십)\n- **$400**K ARR 도달, 연말에 핵심 고객에 집중하면서 매출 추세가 더 좋아지는 신호를 확인\n마무리: 실패를 기록하는 이유\n- 실패는 반복되며, 창업자는 그 실패를 학습으로 바꾸는 사람\n- 결국 끝까지 함께하는 것은 사람(가족·팀·파트너)\n- 2026년을 “10x의 해”로 만들겠다는 다짐으로 글을 마침",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25425",
    "category": "AI"
  },
  {
    "title": "파이썬을 활용한 SDR 및 DSP 가이드(PySDR)",
    "content": "- \n소프트웨어 정의 라디오(SDR) 와 디지털 신호 처리(DSP) 개념을 파이썬으로 실습 중심 학습이 가능하도록 구성한 온라인 교재\n- 하드웨어 대신 소프트웨어로 RF 신호를 처리하는 SDR의 원리와, NumPy·Matplotlib을 이용한 신호 시각화 및 분석 예제를 포함\n- 수학적 공식보다 애니메이션과 시각 자료를 통해 개념을 직관적으로 이해하도록 설계\n- 전기공학 전공자가 아니더라도 프로그래밍 경험이 있는 학습자가 쉽게 접근할 수 있도록 구성\n- 오픈소스로 운영되며, GitHub 기여·Patreon 후원을 통해 교재 개선에 참여 가능\n---\n1. 목적과 대상 독자\n- \nSDR(Software-Defined Radio) 는 기존 하드웨어 기반 RF 처리를 소프트웨어로 수행하는 개념\n일반 컴퓨터(CPU), FPGA, GPU 등에서 실행 가능하며, 실시간 또는 기록된 신호의 오프라인 처리 지원\n- 안테나를 연결해 RF 신호를 수신·송신할 수 있는 장치 형태로도 존재\n- \nDSP(Digital Signal Processing) 는 디지털 방식으로 신호를 처리하는 기술로, 본 교재에서는 RF 신호 중심으로 다룸\n- 교재는 다음과 같은 학습자를 대상으로 함\nSDR을 활용해 실험적 프로젝트를 수행하고자 하는 사람\n- 파이썬에 익숙하지만 DSP·무선통신에는 초보인 사람\n- 수식보다 시각 자료를 선호하는 학습자\n- 간결한 설명을 선호하며, 긴 교재 대신 실습 중심 학습을 원하는 사람\n- 전기공학 전공자가 아니더라도 프로그래밍 경험이 있는 컴퓨터공학 학생 등에게 적합\n- 복잡한 수학 대신 이미지·애니메이션을 통해 Fourier 시리즈 등 핵심 개념을 설명\n이러한 이유로 PySDR은 인쇄본 형태로 판매되지 않음\n2. 교재 구성과 학습 접근\n- DSP의 기초 이론을 전기공학의 “Signals and Systems” 한 학기 분량에서 몇 개 장으로 압축\n- 이후 SDR 관련 주제로 확장하며, DSP와 무선통신 개념이 교재 전반에 걸쳐 반복 등장\n- \nPython 코드 예제는 NumPy와 Matplotlib을 사용\nNumPy는 배열 및 수학 연산용 표준 라이브러리이며, 대부분의 연산이 C/C++로 최적화되어 있음\n- Matplotlib은 신호·배열·복소수 시각화를 위한 플로팅 도구\n- Python이 C++보다 느리더라도, 내부 연산이 최적화되어 있어 실습에 충분한 성능 제공\n- MATLAB, Ruby, Perl 경험자도 Python 문법에 익숙해지면 쉽게 활용 가능\n3. 기여 방법\n- PySDR을 통해 배운 내용을 학생·동료·학습자에게 공유 권장\n- \nPatreon 후원을 통해 이름을 교재 페이지 하단에 표시 가능\n- 교재를 읽고 질문·의견·수정 제안을 이메일로 보내면 자동으로 기여자로 인정\n- \nGitHub 저장소를 통해 직접 수정 제안(Pull Request) 가능\n- Git 사용이 익숙하지 않아도 이메일로 제안 가능\n4. 감사의 말\n- 교재에 피드백을 제공한 독자 및 번역 기여자에게 감사 표시\n프랑스어, 네덜란드어, 우크라이나어, 중국어, 스페인어 번역 참여자 명시\n- Patreon 후원자 명단과 Analog Devices, Inc. 등 기관 후원자도 포함\n- PySDR은 CC BY-NC-SA 4.0 라이선스로 배포됨",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25424",
    "category": "AI"
  },
  {
    "title": "CEO는 너무 비싸다. 자동화할 수는 없을까?",
    "content": "- CEO 보수가 일반 직원 수천 명의 임금에 해당하는 수준까지 상승하면서, CEO라는 역할 자체의 필요성이 문제로 떠오름\n- 팬데믹과 정부 지원 덕분에 생존한 기업에서도 성과와 무관한 고액 보상이 이어지며 주주 반발이 확산 중\n- CEO 업무 상당 부분이 보좌 인력이나 외주로 이미 분해·위임되고 있다는 점에서,이 역할은 자동화 가능성이 높음\n- 전략적 의사결정은 인간의 편향과 직관 오류에 크게 의존하며, 오히려 소프트웨어가 더 적합할 수 있음\n- 이미 여러 기업이 하위 직무를 자동화하고 있지만, 비용과 영향력을 고려하면 최고 경영진부터 자동화하는 편이 더 합리적임\n---\n\n## 치솟는 CEO 보수와 주주 반발\n- BAE 시스템즈, 아스트라제네카, 런던 증권거래소 등 주요 기업들이 연례총회(AGM) 에서 경영진 보수안을 두고 주주 반발에 직면\n팬데믹으로 매출이 급감한 기업들이 정부 지원으로 생존했음에도, CEO 보너스 지급이 이어져 비판 확산\n- 예시로 부동산 중개사 Foxtons의 주주 **40%**가 CEO 보너스(**17억**원) 지급에 반대했는데, 이 회사는 약 **700만** 파운드(**1,130억**원)의 정부 지원을 받았음\n- 의류회사 Ocado의 Tim Steiner는 2019년 **5,870만** 파운드(**1,142억**원) 를 수령 (같은해 Ocado 직원 중간소득의 2,605배)\n- FTSE 100 기업 CEO의 평균 연봉은 하루 **1만**5천 파운드 (**2,918만**원) 이상임\n- 이런 고액 보상 구조는 CEO 개인을 넘어 경영진 전반의 인건비 부담으로 이어지고 있음\n- High Pay Centre 보고서에 따르면, CEO를 넘어선 고소득자 전체를 고려하면 상당한 비용 절감 가능성 존재\n> \"고소득 직원들이 희생을 감수한다면 일자리와 소득을 보호할 수 있는 잠재력이 상당하다\"\nCEO는 실제로 무엇을 하는가\n- 한 기술 기업 CEO의 사례에서, 이메일·운영·채용·투자자 대응·리서치 등 업무 대부분이 비서 또는 보조 인력에 의해 수행됨이 드러남\n- 해당 CEO는 자신의 시간이 **60%** 절약된다고 평가했으며, 이는 CEO 업무 상당 부분이 이미 분해 가능함을 시사함\n- 외주가 가능한 업무라면, 동일한 논리로 자동화 또한 가능하다는 문제 제기 이어짐\n- 미국의 한 기술기업 CEO인 Christine Carrillo는 자신의 Executive Assistant(수석 비서) 가 감사 트윗을 올리며 논란 촉발\nEA가 이메일, 자금조달, 플레이북, 운영, 채용, 리서치, 투자자 업데이트, 인보이싱 등 대부분의 업무를 수행한다고 언급\n- Carrillo는 비서가 자신의 시간을 **60%** 절약시킨다고 언급했으며, 해당 비서는 필리핀에 근무하고 있음\n비서 덕분에 \"매일 글을 쓰고 인터넷 서핑을 할 수 있고\", \"저녁 식사를 준비하고 책을 읽을 수 있어요\"\n- 비판자들은 누군가가 CEO 업무의 **60%**를 수행한다면 CEO보다 **50%** 더 많은 보수를 받아야 한다고 지적\n- 이 사례는 CEO 업무의 상당 부분이 저비용 외주 또는 자동화 가능한 형태임을 보여줌\n- 아웃소싱의 주된(그리고 종종 유일한) 이유는 비용 절감\n- CEO 업무가 외주로 가능한 수준이라면, 소프트웨어 자동화로도 대체가 가능함\n왜 CEO는 자동화되지 않는가\n- 기업들은 하위·중간급 직책의 자동화에는 경쟁적이지만, 고위 경영진과 의사결정자들은 자기 자신의 자동화에는 관심이 적음\n- 아마도 모든 CEO의 침대옆 탁자에 놓여 있을 Thinking, Fast and Slow(생각에 관한 생각 by 대니얼 카너먼) 에서 알 수 있듯이, 인간의 의사결정은 비합리적 편향과 가정의 산물\n이것이 전략 수립이 어렵고 전략적 의사결정 역할이 고액 보수를 받는 이유 중 하나\n- 진정으로 합리적인 전략적 결정을 내리기 어렵고 이를 수행하는 인력이 비싸다는 점은, 이 업무를 소프트웨어에 맡길 좋은 근거가 됨\n자동화의 위험성과 실패 사례\n- \n공개적 역할에서의 자동화는 위험할 수 있음\n- \nMicrosoft는 2020년 기자팀을 해고하고 AI로 대체했으나, 소프트웨어가 두 명의 유색인종 여성을 구별하지 못해 PR 재앙 직면\n- \nAmazon은 AI 채용 도구가 여성 차별을 학습하여 폐기해야 했음\n- 가장 진보된 AI 언어 모델 중 하나인 GPT-3가 2020년 의료 챗봇으로 사용되었을 때, 자살 충동을 호소하는 (시뮬레이션된) 환자에게 \"자살하라\"고 응답\n- 이 사례들의 공통점: 모두 회사 내 다른 사람들의 검토 없이 이루어지는 업무의 자동화 시도였음\n고위급 의사결정 자동화의 차별점\n- \n최고위 전략적 결정은 다름: 실행 전에 보통 토론과 검토를 거침\n- 직원들이 CEO의 불만을 살까 두려워 발언하지 못하는 경우가 있는데, 이것이 자동화의 또 다른 이유가 됨\n- \n\"Decision Intelligence\"(Google과 IBM의 명칭)가 배치된 곳에서 인상적인 결과 도출\n- \n홍콩 대중교통 시스템은 2004년부터 유지보수 일정을 소프트웨어에 위임\n세계에서 가장 정시성이 높고 운영이 잘 되는 지하철 중 하나로 명성 획득\nCEO 자동화가 진행되지 않는 이유\n- 최고경영진이 자발적으로 사무실을 비우고 로봇에게 자리를 넘기지는 않을 것\n- 경영진은 증가하는 대규모 변동비용인 반면, 기술은 시간이 지남에 따라 더 저렴하고 신뢰성이 높아지는 반대 방향으로 움직이고 있음\n핵심 질문의 전환이 필요함\n- CEO 연봉이 공정하거나 윤리적인지를 묻는 것을 넘어서야 함\n- CEO 보수의 윤리성보다 더 중요한 질문은, 해당 역할이 기계로도 충분히 수행 가능한가 여부임\n- 기업 소유주와 투자자는 최고 경영진의 업무를 기계가 잘 수행할 수 있는지, 그렇다면 왜 그렇게 비싼지를 질문해야 함",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25423",
    "category": "AI"
  },
  {
    "title": "MongoBleed 취약점에 대한 간단 설명",
    "content": "- \nMongoBleed(CVE-2025-14847) 은 2017년 이후 모든 MongoDB 버전에 존재한 심각한 메모리 누출 취약점으로, 공격자가 데이터베이스의 힙 메모리 임의 데이터를 읽을 수 있음\n- 취약점은 zlib 압축 경로의 버그로 인해 발생하며, 인증 없이 단순히 데이터베이스에 연결만 해도 악용 가능\n- 공격자는 조작된 압축 요청을 보내 서버가 잘못된 크기의 버퍼를 할당하도록 유도하고, 그 안의 이전 작업 메모리(비밀번호, API 키 등) 를 노출시킬 수 있음\n- MongoDB는 2025년 12월 19일 패치를 배포했으나, EOL 버전(3.6, 4.0, 4.2) 은 수정되지 않음\n- 8년간 존재한 이 취약점은 인터넷에 노출된 **21만** 개 이상 MongoDB 인스턴스에 영향을 주며, 클라우드·온프레미스 환경 모두에서 즉각적 패치 또는 압축 비활성화가 필요함\n---\n\n## MongoBleed 개요\n- \nMongoBleed(CVE-2025-14847) 은 MongoDB의 zlib 1 메시지 압축 경로에서 발견된 취약점으로, 2017년 이후 모든 버전에 영향을 미침\n공격자는 인증 없이 데이터베이스에 연결만 하면 임의의 힙 메모리 데이터를 읽을 수 있음\n- MongoDB 3.6, 4.0, 4.2 등 지원 종료(EOL) 버전은 수정되지 않음\n- 이 버그는 2017년 5월의 PR에서 도입되었으며, 2025년 12월 19일 공식적으로 공개됨\n- MongoDB는 Atlas 클라우드 서비스를 포함한 모든 인스턴스에 패치를 적용했다고 발표\nMongoDB 통신 구조\n- MongoDB는 HTTP 대신 자체 TCP 프로토콜을 사용하며, 메시지는 BSON(Binary JSON) 형식으로 전송됨\n- 모든 요청은 OP_MSG 명령으로 구성되며, 압축 시 OP_COMPRESSED 메시지로 감싸짐\n메시지에는 uncompressedSize, originalOpcode, compressorId 등의 필드가 포함됨\n- \nuncompressedSize는 압축 해제 후의 예상 크기를 나타냄\n익스플로잇 단계 1 — 잘못된 버퍼 할당\n- 공격자는 uncompressedSize 값을 실제보다 과도하게 큰 값으로 설정해 서버가 큰 버퍼를 할당하도록 만듦\n예: 실제 1KB 메시지를 1MB로 선언\n- MongoDB 서버는 압축 해제 후 실제 크기를 검증하지 않고 사용자가 지정한 크기를 신뢰함\n결과적으로 메모리에는 [실제 데이터 | 미참조 힙 쓰레기] 형태의 구조가 남음\n- C++ 기반 MongoDB는 메모리 초기화를 수행하지 않기 때문에, 이 영역에는 이전 작업의 민감 데이터가 포함될 수 있음\n예: 비밀번호, 세션 토큰, API 키, 고객 데이터, 시스템 설정 등\n익스플로잇 단계 2 — 데이터 유출\n- 공격자는 잘못된 BSON 입력을 전송해 서버가 메모리의 쓰레기 데이터를 문자열로 파싱하도록 유도\nBSON의 첫 필드는 문자열이며, C 언어의 널 종료 문자열(null-terminated string) 규칙을 따름\n- 공격자가 널 종료 문자가 없는 문자열을 보내면, 서버는 메모리 내 다른 데이터까지 읽어들임\n예시: [ { \"a | password: 123\\0 | apiKey: jA2sa | ip: 219.117.127.202 ]\n- 서버는 이를 잘못된 BSON 필드로 인식하고 오류 메시지에 해당 내용을 포함해 응답\n> \"errmsg\": \"invalid BSON field name 'a | password: 123'\"\n- 이 과정을 반복하면 공격자는 힙 메모리 전체를 스캔하며 민감 정보를 수집 가능\n영향 및 위험성\n- \n인증 전(pre-auth) 단계에서 발생하므로, 공격자는 로그인 없이 데이터에 접근 가능\n- \n인터넷에 노출된 MongoDB 인스턴스는 즉시 위험에 노출됨\nShodan 검색 기준 213,000개 이상의 MongoDB 인스턴스가 공개 상태\n- \n2017년부터 2025년까지 약 8년간 존재한 취약점으로, 단순한 구조 탓에 실제 악용 가능성이 높음\n- MongoDB는 “현재까지 악용 증거는 없다”고 밝혔으나, 공식 사과나 상세 타임라인은 공개하지 않음\n완화 방법\n- \n최신 패치 버전(8.0.17 이상) 으로 업데이트\n- 단기적으로는 zlib 네트워크 압축 비활성화로도 완화 가능\n- MongoDB Atlas 사용자는 이미 패치 적용 완료\n추가 정보 및 관련 논의\n- Elastic 보안팀 리드가 ‘MongoBleed’ 라는 이름을 붙이고 PoC(Python 스크립트) 를 공개\nMongoDB와 Elastic은 검색 및 분석 기능 영역에서 경쟁 관계\n- 관련 리소스:\nCVE 공식 페이지: CVE-2025-14847\n- 버그 도입 PR: GitHub PR #1152\n- 수정 커밋: 505b660a14698bd2b5233bd94da3917b585c5728\n- 탐지 도구: mongobleed-detector\n요약\n- MongoBleed는 zlib 압축 처리 버그로 인해 발생한 메모리 누출 취약점\n- 공격자는 조작된 압축 요청을 통해 이전 메모리 데이터(비밀번호, API 키 등) 를 획득 가능\n- \n2017~2025년 모든 MongoDB 버전이 영향받으며, 패치 또는 압축 비활성화가 필수\n- \n인터넷 노출 인스턴스 **21만** 개 이상이 잠재적 피해 대상\n- MongoDB는 패치를 배포했으나, EOL 버전 미지원 및 공개 대응 지연이 지적됨",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25422",
    "category": "튜토리얼"
  },
  {
    "title": "Unity의 Mono 문제: 왜 당신의 C# 코드는 기대보다 느리게 실행되는가",
    "content": "- Unity가 사용하는 Mono 런타임은 최신 .NET 대비 현저히 느린 실행 속도를 보이며, 동일한 C# 코드가 최대 15배까지 차이 나는 사례가 있음\n- 실제 게임 코드에서 Mono 기반 Unity 실행은 100초, 동일 코드의 .NET 실행은 38초로 측정되어, 디버깅과 테스트 효율에도 큰 영향을 줌\n- \nRelease 모드에서도 Mono는 30초, .NET은 12초로, 최적화된 환경에서도 2.5배 이상 성능 차이가 유지됨\n- 원인은 Mono의 비효율적인 JIT 컴파일과 인라이닝 실패, 메모리 복사 과다 등으로, .NET의 최신 CoreCLR JIT 최적화와 대비됨\n- Unity가 CoreCLR 기반 .NET 현대화를 완성하면 게임과 에디터 모두에서 큰 성능 향상이 가능하며, 이는 모든 Unity 프로젝트의 숨은 성능세 제거로 이어질 전망\n---\n\n## Unity의 Mono 사용 배경\n- Unity는 2006년부터 Mono 프레임워크를 사용해 C# 코드를 실행\n당시 Mono는 유일한 멀티플랫폼 .NET 구현체였으며, 오픈소스로 Unity가 수정 가능\n- 2014년 이후 Microsoft가 .NET Core를 오픈소스로 공개하고, 2016년 .NET Core 1.0을 출시\n이후 Roslyn 컴파일러, 새 JIT, 성능 개선 등 .NET 생태계가 빠르게 발전\n- 2018년 Unity 엔지니어들은 CoreCLR 포팅 작업을 진행 중이라 밝히며, Mono 대비 2~10배 성능 향상을 기대\n- 그러나 2025년 말 현재까지도 CoreCLR 기반 게임 실행은 불가능한 상태\nMono와 .NET의 성능 격차\n- Unity 프로젝트의 시뮬레이션 코드를 Unity 외부에서 .NET으로 실행해 직접 비교\nUnity/Mono 환경: 100초, .NET 환경: 38초 (Debug 모드 기준)\n- \nRelease 모드에서는 Mono 30초, .NET 12초로 차이가 유지\n.NET은 4K×4K 맵을 3초 내 생성하는 등 멀티스레드 최적화가 뛰어남\n- Mono의 비효율적 코드 생성이 주요 원인으로, 단순 루프에서도 15배 속도 차이 발생\n어셈블리 비교: Mono vs .NET\n- 동일한 테스트 코드로 생성된 x64 어셈블리 비교 결과\n.NET JIT은 루프 불변식을 루프 외부로 이동(hoisting) , 최소한의 레지스터 연산만 수행\n- Mono는 수십 개의 mov 명령어로 메모리 복사를 반복하며, 비효율적 인라이닝으로 성능 저하\n- \nint.MaxValue 반복 루프 실행 시간\n.NET: 750ms, Mono: 11,500ms, Unity Editor(Debug): 67,000ms\n- Mono는 루프 내에서 불필요한 메모리 이동과 비교 연산을 반복 수행\n\n---\n\n## CoreCLR 도입의 의미\n- CoreCLR은 최신 JIT, Span<T> API, SIMD 최적화, 하드웨어 명령어 지원 등 현대적 기능 제공\n이러한 기능은 추가적인 2배 이상 성능 향상 가능성\n- Unity의 Burst 컴파일러는 LLVM 기반으로 네이티브 코드를 생성하지만, C# 기능 제한이 존재\nCoreCLR의 현대적 JIT은 Burst와 유사한 성능을 제공하면서 언어 제약이 적음\n- CoreCLR은 AOT(사전 컴파일) 지원으로 시작 속도 개선과 JIT 제한 플랫폼(iOS 등) 대응 가능\n그러나 Unity는 여전히 IL2CPP 유지 방침을 밝힘\n결론: Unity의 .NET 현대화 필요성\n- Mono는 최신 .NET 대비 1.5~3배 이상 느린 실행 성능을 보이며, 이는 모든 Unity 프로젝트의 숨은 비용으로 작용\n- CoreCLR 도입 시 기대 효과\n런타임 성능 향상, 빠른 반복 빌드, GC 개선, 도메인 리로드 제거, 관리 코드 비중 확대\n- Unity 6.x 로드맵에 .NET Modernization이 포함되어 있으나, 2026년 이후로 예정\n- CoreCLR 지원이 완성되면 Unity 개발자와 플레이어 모두에게 실질적 성능 혁신 제공 가능\n- 현재로서는 Mono의 한계가 Unity 생태계 전체의 성능 병목으로 남아 있음",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25421",
    "category": "AI"
  },
  {
    "title": "소프트웨어 엔지니어는 약간은 냉소적일 필요가 있다",
    "content": "- 엔지니어의 적당한 냉소주의(cynical) 는 대기업의 작동 방식을 정확히 이해하게 해주고, 역설적으로 과도한 냉소에 빠지는 것을 막아줌\n- 엔지니어가 나쁜 코드를 작성하는 이유에 대한 현실적인 냉소적 설명이 없으면, 의도적 사기 저하나 반노동 전략 같은 과도한 음모론으로 치우칠 위험이 있음\n- 소위 “이상주의적” 관점은 세상을 근본적으로 부패하고 이기적인 구조로 보며, 긍정적 변화 자체가 불가능하다고 전제한다는 점에서 오히려 더 냉소적임\n- 소프트웨어 엔지니어링 담론에서는 이상주의적 글이 과잉 대표되어 있고, 대기업이 실제로 어떻게 작동하는지를 설명하는 글은 상대적으로 부족함\n- 2010년대에 형성된 대기업에 대한 사실과 어긋난 인식 모델을 내면화한 세대가 2020년대에 어려움을 겪고 있으며, 정확한 작동 모델을 이해해야 자신의 이상주의적 목표도 더 현실적으로 달성 가능\n---\n\n## 냉소주의자라는 비판에 대한 입장\n- 나의 \"매니저를 기쁘게 하는 일을 해야 한다\"거나 \"대기업이 어떤 프로젝트를 할지 결정한다\"는 주장에 대해 많은 독자들이 나를 냉소주의자(cynic)라고 부름\n- Alex Wennerberg는 \"엔지니어가 정치적 게임의 도구 이상이며, 전문성을 의미 있는 문제 해결에 적용하는 전문가\"라고 반론\n- 그럼에도 불구하고 본인은 대기업에서 일하는 것을 좋아하며, 조직을 탐색해 실제 기능이나 개선을 사용자에게 전달하는 것이 좋은 일을 하는 최선의 방법이라고 생각함\n이상주의적 관점이 생각보다 냉소적인 이유\n- 교조적 \"이상주의자\" 관점에 따르면:\n후기자본주의 지옥에서 대기업은 권력만 원하는 도둑 귀족들이 운영하며,\n순종적인 엔지니어 드론들이 빠르게 나쁜 코드를 찍어내 주가를 부풀리고,\n최종 사용자는 더 나쁜 소프트웨어에 더 많은 비용을 지불하고 광고에 시달림\n- 이 관점은 동료와 상사를 냉소적으로 보는 것\n실제로 대기업 경영진은 사용자에게 좋은 소프트웨어를 제공하고 싶어 함\n- 이 관점은 개인 엔지니어가 어떤 타협도 받아들이지 않아야 한다는 전제에서만 이상적으로 보임\n이 관점에 따르면 회사가 압박하더라도 형편없는 소프트웨어를 작성해서는 안 되며, 회사가 아무리 타협하고 결과물만 내놓으라고 강요해도, 도덕적으로 단호하게 거부해야 할 의무가 있음\n- 이름 없는 개인이 사용자도 알지 못하는 선을 지킨다는 서사는 영웅적 자기 인식을 강화함\n- 그러나 이러한 관점은 세상을 근본적으로 부패하고 이기적인 구조로 규정하고, 실질적인 긍정적 변화는 불가능하다고 믿는 인식에서 출발함\n- 이 태도는 이상주의라기보다, 변화 가능성을 포기한 형태의 냉소주의에 가깝다고 봄\n냉소적 관점이 생각보다 이상주의적인 이유\n- \"정치적 게임의 도구\"와 \"의미 있는 문제를 해결하는 전문가\" 사이에 뚜렷한 구분이 없음\n- 실제로 거의 모든 의미 있는 문제는 정치적 게임을 통해 해결됨\n- 혼자서 해결할 수 있는 문제는 매우 적으며, 대규모 제품 변경(예: GitHub의 **1억** 5천만 사용자가 마크다운에서 LaTeX 사용 가능하게 하기)은 많은 사람과 조정해야 하므로 정치에 관여해야 함\n- 소프트웨어 엔지니어는 대기업에서 방향을 설정하지 않지만, 회사 방향을 구체적 기술 변경으로 번역하는 데 상당한 영향력을 가짐\n- 대기업은 수억 또는 수십억 사용자에게 서비스하며, 작은 변경이 총체적으로 막대한 긍정적 또는 부정적 영향을 미칠 수 있음\n- 지저분한 정치적 과정에 참여하기로 선택하는 것은 이상주의적 행동임\n- 대기업 엔지니어의 위치는 공공 서비스에 종사하는 사람과 유사: 정부 정책의 큰 방향은 설정하지 못하지만 선을 행할 수 있기를 이상주의적으로 희망함\n예방 접종으로서의 냉소주의\n- 건강한 양의 냉소주의는 과도한 냉소주의에 대한 예방 접종 역할\n- 대기업에서 엔지니어가 나쁜 코드를 작성하는 이유에 대한 약간 냉소적인 설명이 없으면, 엔지니어들이 노조 결성을 막기 위한 반노동 전략으로 의도적으로 사기가 저하되고 있다는 과도한 냉소적 설명을 채택할 위험\n기업들은 이런 종류의 음모에 관여하도록 설정되어 있지 않음\n- 대기업이 비효율적 결정을 내리는 이유에 대한 약간 냉소적인 설명이 없으면, 대기업이 무능한 패배자들로 가득하다는 과도한 냉소적 설명을 채택할 위험\n실제로 기업들은 강한 엔지니어와 약한 엔지니어의 정상적인 혼합을 가지고 있음\n최종 생각\n- 소프트웨어 엔지니어링에 관한 글들은 이상주의적 글이 지나치게 많음\n좋은 코드를 가치 있게 여겨야 하고, 동료에게 친절해야 하고, 긍정적 영향을 미치는 프로젝트에서 일해야 한다고 설명하는 책이나 블로그 포스트는 이미 넘쳐남\n- 그러나, 대기업이 실제로 어떻게 운영되는지 정확히 설명하는 글은 부족함\n- 냉소적 글이 사람을 슬프게 하거나 쓴 냉소주의자로 만들어 해로울 수 있지만, 이상주의적 글도 해를 끼칠 수 있음\n- 2010년대에 배출된 소프트웨어 엔지니어 세대는 대기업 작동 방식에 대해 사실과 다른 모델을 가지고 있었고,\n이들이 2020년대에 들면서 사실상 파쇄기에 갈려 들어가고 있음\n- 그들이 이러한 기업의 작동 방식에 대한 올바른 모델을 내면화했다면, 문제에 휘말릴 가능성이 줄어들 뿐만 아니라 자신의 이상주의적 목표를 달성하는 데에도 더 유리했을 것\nHacker News 댓글에 대한 추가 답변\n- 일부 댓글: 고용주가 비윤리적 활동에 관여할 때 \"내가 하는 일은 사실 좋다\"고 말하는 것이 일관성이 없다는 지적\n이 포스트는 내가 Microsoft에서 일하는 것이 윤리적인지 여부가 아닌, \"대기업에서 유능한 엔지니어들이 왜 형편없는 코드를 작성하는가\"에 대한 후속 글임\n- 일부 댓글: C레벨 임원이 좋은 소프트웨어를 제공하고 싶어 한다는 주장에 대해, 개인적 성공을 위해 이를 희생하지 않는다는 점 지적\n동의하지만 항상 제로섬이 아님, 좋은 소프트웨어가 소프트웨어 회사에 돈을 벌어다 줌\n- 일부 댓글: High-Tech Employee Antitrust Litigation(고급 기술 직원 반독점 소송)을 대기업이 직원에 대한 음모에 관여한다는 예로 링크\n회사들은 급여에 대해 담합하도록 구조적으로 설정되어 있지만, 직원을 의도적으로 슬프게 만들도록 설정되어 있지 않음\n- 그런 종류의 세밀한 문화 통제가 없으며, 통제할 수 있는 한에서는 직원들이 더 적은 돈에 일하고 떠나지 않도록 행복하게 만들려고 노력함",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25420",
    "category": "AI"
  },
  {
    "title": "Mockito 메인테이너가 10년 만에 자리에서 물러남",
    "content": "- \nMockito 핵심 유지관리자가 2026년 3월을 기준으로 약 10년간의 유지관리 역할을 마무리하고, 향후 몇 달간 점진적 권한 이양을 진행할 계획을 밝힘\n- 결정의 직접적 계기 중 하나로 JVM 22에서의 에이전트 정책 변경을 언급하며, 보안 목적의 변화 자체는 공감하지만 대안 없는 일방적 전환 요구와 생태계 차원의 고려 부족이 큰 부담으로 작용했음\n- 특히 Mockito가 JVM 에이전트의 최대 사용자 중 하나임에도 불구하고, 빌드 도구 지원이나 협업적 논의 없이 문제 해결을 떠안게 된 구조가 자원 소진과 책임 과중으로 이어졌다고 설명함\n- 또 다른 요인으로 Kotlin 지원의 구조적 복잡성을 지적하며, Kotlin 특유의 JVM 동작 방식과 불일치한 기능들이 Mockito 내부에 중복 API와 분기 로직을 증가시켜 유지보수를 어렵게 만들고 있음을 밝힘\n- 최근에는 Rust 기반 웹 엔진 Servo 작업에서 더 큰 즐거움과 동기를 느끼고 있으며, 제한된 개인 시간을 고려할 때 의무처럼 느껴지는 자원봉사 유지관리 작업을 지속하기 어렵다는 판단에 도달했음을 공유함\n---\n\n## 10년이라는 마일스톤과 역할 이양 결정\n- 2026년 3월로 Mockito 유지관리 10주년을 맞이하며, 해당 시점을 자연스러운 책임 이양의 분기점으로 판단함\n- 향후 몇 달간 기존 유지관리자로서 지식 이전과 전환 안정화에 집중할 계획임\n- 차기 유지관리 체계와 장기 로드맵 논의는 별도의 GitHub 이슈에서 진행 예정임\nJVM 에이전트 정책 변경으로 인한 소진\n- Mockito 5에서 기본 아티팩트가 JVM 에이전트로 전환된 배경에는 JVM 22부터 동적 에이전트 부착이 플래그 뒤로 숨겨진 정책 변화가 있음\n- 보안 관점의 변경 취지에는 동의하지만, 대체 설계나 마이그레이션 지원 없이 결정이 기정사실화된 점이 문제로 지적됨\n- Mockito가 JVM 기능 선도 사례로 자주 활용되어 왔음에도, 이번 변화에서는 협업적 피드백 루프가 작동하지 않았음\n- 에이전트에 대한 빌드 도구 차원의 지원이 여전히 부족한 현실이 해당 기능의 우선순위가 낮음을 보여준다고 평가함\n- 자발적 기여자인 유지관리자에게 과도한 압박이 가해질 경우 오픈소스 협업 구조가 쉽게 붕괴됨을 강조함\nKotlin 지원이 초래한 구조적 부담\n- Kotlin의 확산 자체는 부정하지 않지만, JVM 내부 동작 방식의 차이로 인해 mockito-core에 Kotlin 전용 처리 흐름이 다수 추가됨\n- suspend 함수 등 Kotlin 기능이 일관되게 동작하지 않는 사례가 존재해 API 중복과 복잡성이 증가함\n- 결과적으로 코드베이스가 스파게티화되고 유지보수 난이도가 상승했으며, 이에 대한 작업이 개인적으로 즐겁지 않다고 솔직히 언급함\n- Kotlin 중심의 미래가 장기적으로 Mockito 유지관리 동기를 약화시키는 요소로 작용했음\n다른 오픈소스 활동에서의 즐거움 회복\n- 다수의 오픈소스 프로젝트에 기여해 왔으며, 최근에는 Rust 기반 웹 엔진 Servo 작업을 통해 개발의 즐거움을 다시 느끼고 있음\n- 제한된 저녁 시간 선택지에서 Mockito보다 다른 프로젝트가 더 큰 만족을 제공하는 상황이 지속됨\n- 자원봉사 기반 유지관리 작업이 장기간 의무처럼 느껴지는 상태는 바람직하지 않다고 판단함\n결정의 종합적 배경과 메시지\n- JVM 정책 변화로 인한 회의감, Kotlin 지원 구조의 한계, 그리고 다른 프로젝트에서의 동기 회복이 결정의 핵심 요인으로 작용함\n- 해당 요인들이 모든 기여자에게 동일하게 적용되지는 않으며, 다른 이들이 Kotlin 지원에 더 적극적일 수 있음을 인정함\n- 유지관리자 교체가 프로젝트의 장기적 건강성에 더 이롭다는 판단 아래 역할을 내려놓기로 결정함\n- 오픈소스 유지관리 경험 자체는 영광이자 특권이었다고 평가하며, 다른 이들에게도 자원봉사적 기여를 권장함",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25419",
    "category": "AI"
  },
  {
    "title": "AI가 메모리 칩을 대량 소비하면서 기기 가격이 상승할 것",
    "content": "- \nAI 데이터센터의 폭발적 성장으로 인해 RAM 수요가 공급을 초과하며 전 세계적인 메모리 부족 현상 발생\n- \nDRAM 가격이 분기 대비 **50%** 상승, 다음 분기에도 **40%** 추가 상승이 예상되며, 2026년까지 가격 하락 전망 없음\n- \nMicron Technology 등 주요 제조사는 AI용 고급 메모리 생산에 집중하면서 PC·스마트폰·가전용 칩 공급 감소\n- \nAI 학습과 추론 시스템은 대용량·고대역폭 메모리를 요구해, 단기적 수요 조정이 불가능한 구조\n- 전문가들은 2027년 신규 공장 가동 전까지 공급 병목 지속과 기기 가격 인상 압력을 경고하고 있음\n---\n\n## 전 세계 메모리 칩 부족 현상\n- \nAI 관련 클라우드 컴퓨팅과 데이터센터 확장으로 특정 메모리 칩 수요가 급증, 공급 부족 초래\n이로 인해 스마트폰, 컴퓨터, 게임 콘솔 등 다양한 전자기기 가격 상승 가능성 제기\n- TrendForce의 Avril Wu는 “기기를 사고 싶다면 지금 사야 한다”고 언급\n- RAM은 여러 앱을 동시에 실행하거나 영상 재생을 원활히 하는 데 필수적인 구성 요소\n수요가 공급보다 약 **10%** 초과하며, 제조사들은 매달 더 높은 가격을 지불 중\n급등하는 DRAM 가격\n- 2025년 4분기 기준, DRAM 가격이 전 분기 대비 **50%** 상승\n조기 납품을 원할 경우 2~3배 높은 가격을 지불해야 함\n- TrendForce는 다음 분기에도 **40%** 추가 상승을 예상하며, 2026년까지 가격 하락은 없을 것으로 전망\n- 이러한 급등은 AI 데이터센터의 폭발적 수요가 주된 원인으로 지목됨\nAI가 메모리를 집어삼키는 구조\n- AI 데이터센터는 GPU와 함께 대용량 메모리를 필요로 함\nGreyhound Research의 Sanchit Vir Gogia는 “AI 워크로드는 메모리를 중심으로 구축된다”고 설명\n- AI 모델의 훈련과 추론 시스템은 대규모·지속적 메모리, 높은 대역폭, 연산 장치와의 근접성을 요구\n이러한 특성 때문에 수요를 줄이면 성능 저하가 불가피\n제조사와 시장 반응\n- \nMicron Technology는 세계 주요 RAM 제조사로, AI 수요 증가로 예상보다 높은 실적을 기록\nCEO Sanjay Mehrotra는 “공급이 수요를 따라잡지 못하는 상황이 지속될 것”이라고 언급\n- 제조사들은 AI용 고급 메모리 생산으로 전환 중이며, 그 결과 PC·모바일·TV용 칩 공급 감소\nDell의 COO Jeff Clarke는 “이 비용 상승이 소비자에게 전가될 것”이라고 발언\n공급 병목과 향후 전망\n- 전문가들은 단기적 해결책이 없다고 지적\n현재 공장들은 2026년 말까지 생산 능력 한계에 도달할 것으로 예상\n- \nMicron의 아이다호 신규 공장은 2027년 가동 예정으로, 그 전까지 공급 부족 지속 전망\n- 이에 따라 메모리 공급업체들의 가격 인상이 계속될 가능성 높음",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25418",
    "category": "AI"
  },
  {
    "title": "4,800개의 GitHub 스타가 신뢰를 무너뜨린 이유",
    "content": "📌 핵심 요지 (TL;DR)\n- 중국계 GitHub 리포지토리에서 코드·릴리즈·활동 변화 없이 Star가 4,000 → 4,800으로 급증한 사례 확인\n- 이를 계기로 “GitHub Star = 인기/품질”이라는 전제 자체에 문제 제기\n- 결론: GitHub Star 수는 프로젝트 품질이나 신뢰도를 판단하는 지표로 부적합\n---\n📉 주요 주장 & 실무 인사이트\n⭐ 1) 인기는 얼마든지 ‘만들 수 있다’\n- StarScout 기반 GitHub 이벤트 로그 분석 결과:\n약 **450만** 개 이상의 의심스러운 Star 패턴\n- 이 중 **310만** 개 이상이 사실상 가짜 Star로 분류\n- 다수 계정이 짧은 시간 내 동시다발적으로 Star를 찍는 패턴 반복 확인\n- 즉, Star 증가 ≠ 자연스러운 관심 증가인 경우가 상당수 존재\n\n---\n\n## 실무 관점\n“요즘 뜬다”는 이유만으로 의존성 추가하는 건 위험\n---\n💰 2) ‘Star 시장’은 이미 존재한다\n- GitHub Star는 단순 관심 표현을 넘어 실제로 거래되는 마케팅 자산처럼 동작\n- 관찰된 구조:\nStar를 직접 판매하는 벤더\n- 계정 풀(pool)을 활용한 Star 교환 네트워크\n- 서비스 홍보 패키지에 포함된 Star 부스팅 옵션\n- 결과:\n인기 지표가 구조적으로 왜곡\n- 신규 프로젝트·라이브러리 평가 시 노이즈 급증\n\n---\n\n## 실무 관점\nStar 수가 높다고 “검증된 프로젝트”라고 단정하면 안 됨\n---\n🛡 3) Star는 ‘신뢰 지표’가 아니다\n- Star의 본질:\n✔ 가시성(Visibility) 지표\n- ❌ 신뢰(Trust) 지표 아님\n- Star 수만으로는 아래를 판단할 수 없음:\n보안 수준\n- 유지·보수 상태\n- 코드 품질 / 기술 부채\n- 더 심각한 문제:\n가짜 Star로 인기를 위장한 뒤 공급망 공격(Supply Chain Attack)에 악용될 가능성 존재\n\n---\n\n## 실무 관점\nStar 많은 라이브러리가 오히려 리스크일 수도 있음\n---\n🔎 실무용 신뢰 체크리스트 (5분 컷)\nStar 대신 아래를 보자 👇\n- \n활동 리듬\n커밋, 이슈, PR이 꾸준하고 자연스러운가\n- \n문서 상태\nREADME가 실제 사용 가능한 수준인가\n- 설치 / 예제 / 제약 조건이 명확한가\n- \n엔지니어링 위생\n테스트 코드 존재 여부\n- CI/CD 설정 유무\n- \n실제 채택 지표\nPyPI / npm / Docker pull 수\n- 실제 서비스에서 쓰이는 흔적\n- \n보안 태세\nOpenSSF Scorecard, 보안 정책, 취약점 대응 이력\n- \nBus Factor\n특정 1인에게 과도하게 의존하고 있지 않은가\n위 항목들이 Star 수보다 훨씬 신뢰도 높음\n---\n📊 결론 메시지 (실무 요약)\n- GitHub Star는 관심 신호이지, 신뢰 신호가 아님\n- Star 수는 충분히 조작 가능\n- Star가 많다는 건 경우에 따라 경고 신호일 수도 있음\n- 진짜 신뢰는 다음에서 나온다:\n지속적인 활동\n- 보안 관행\n- 문서 품질\n- 커뮤니티 반응\n- 유지·운영 구조",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25417",
    "category": "AI"
  },
  {
    "title": "Mac이 열로 인해 성능 저하(thermal throttling) 상태인지 알려주는 macOS 앱 개발기",
    "content": "- \nMacThrottle은 Mac이 과열로 인해 성능을 제한할 때 이를 메뉴 막대에서 시각적으로 알려주는 SwiftUI 기반 앱 - 오픈소스\n- macOS의 ProcessInfo.thermalState API와 powermetrics 명령어를 비교하며, 시스템의 실제 열 상태를 정확히 감지하기 위한 방법을 탐색\n- 최종적으로 thermald가 Darwin의 notifyd 시스템에 게시하는 알림을 활용해 루트 권한 없이 열 상태를 읽는 방식 구현\n- 앱은 온도·팬 속도 그래프, 상태별 색상 아이콘, macOS 알림 기능을 포함하며, 로그인 시 자동 실행도 지원\n- Apple Silicon Mac의 열 관리 상태를 실시간으로 파악할 수 있는 도구로, 개발자와 파워유저에게 유용한 진단 수단 제공\n---\n\n## Mac의 열 스로틀링 문제 인식\n- M2 MacBook Air에서 외부 4K 120Hz 디스플레이 사용 시 성능 저하와 반응 지연이 발생\n팬이 없어 소음을 감지할 수 없지만, CPU 사용률이 **100%**인 상태에서 전력 사용량이 감소함\n- iStat Menus와 MX Power Gadget을 통해 CPU 주파수와 전력 하락을 확인하며 열 스로틀링을 진단\n- M4 Max MacBook Pro에서도 동일 현상 발생, 14인치 모델의 열 설계 한계로 인한 문제로 언급\n- Apple Silicon의 전력 효율은 여전히 높지만, 열 상태를 직접 감지할 방법을 찾고자 함\nmacOS에서 열 상태를 프로그래밍적으로 확인하기\n- macOS는 여러 방식으로 열 상태를 노출하지만 일관성이 부족\n- Apple이 권장하는 방법은 Foundation의 ProcessInfo.thermalState 사용\n출력 예시: nominal, fair, serious, critical\n- 루트 권한이 필요한 powermetrics -s thermal 명령어도 동일한 정보를 제공하지만,\n두 방식의 상태 구분 단위가 다름\n예: fair는 powermetrics의 moderate와 heavy 두 상태를 모두 포함\n- 실제 스로틀링 시점은 powermetrics에서 heavy로 표시되지만, ProcessInfo에서는 구분 불가\nthermald와 Darwin 알림 시스템 활용\n- \npowermetrics의 데이터는 thermald 데몬에서 가져오며,\nthermald는 현재 열 압력 상태를 notifyd 시스템 이벤트로 게시\n- \nnotifyutil -g com.apple.system.thermalpressurelevel 명령으로 상태 확인 가능\n- \n\n---\n\n## OSThermalNotification.h 헤더에서 정의된 열 압력 수준\nnominal, moderate, heavy, trapping, sleeping\n- Swift 코드로 notify_register_check와 notify_get_state를 호출해\n루트 권한 없이 실시간 열 상태를 읽는 기능 구현\nMacThrottle 앱 개발\n- \nSwiftUI와 MenuBarExtra를 사용해 메뉴 막대 전용 앱 제작\n온도계 아이콘 색상으로 상태 표시 (녹색→적색)\n- \nInfo.plist의 LSUIElement를 true로 설정해 Dock 아이콘 비활성화\n초기 접근: powermetrics 루트 헬퍼\n- 초기에 루트 권한이 필요한 powermetrics를 사용하기 위해\nLaunchDaemon과 bash 스크립트로 헬퍼 프로세스 구성\n/usr/local/bin/mac-throttle-thermal-monitor가 10초마다 상태를 /tmp 파일에 기록\n- 앱은 해당 파일을 주기적으로 읽어 표시\n개선: thermald IPC 알림 사용\n- \nnotifyd 이벤트를 직접 구독하는 방식으로 전환\n루트 권한 불필요, 코드 단순화\n온도 및 팬 속도 표시\n- CPU/GPU 온도와 팬 속도를 그래프로 표시\n- 초기에는 IOKit 비공개 API 사용 시 온도가 실제보다 낮게 표시됨 (~80°C)\n- 오픈소스 Stats 프로젝트를 참고해 SMC 인터페이스로 전환\nSoC 세대별로 다른 키(Tp0D, Tf0E 등)를 사용해야 함\n- SMC가 작동하지 않을 경우 IOKit으로 폴백\n메뉴 막대 그래프 구현\n- 그래프는 3가지 정보를 동시에 표시\n배경 색상: 열 상태 (녹색~적색)\n- 실선: CPU 온도\n- 점선: 팬 속도 비율\n- 2초 간격으로 데이터 수집, 10분 단위 히스토리 유지\n- \nonContinuousHover로 툴팁 제공,\n.drawingGroup을 추가해 GPU 렌더링으로 120Hz 디스플레이에서도 부드럽게 표시\nmacOS 알림 및 자동 실행\n- 열 상태 변화 시 알림 전송 기능 추가\n특정 상태 전환이나 복구 시 알림 가능\n- \nSMAppService API로 로그인 시 자동 실행 설정 지원\nregister() / unregister() / status 메서드로 제어\n배포 및 사용\n- Apple Developer 계정이 없어 공식 공증(notarization) 불가\nGitHub 릴리스에서 설치 시 Privacy and Security에서 수동 승인 필요\n- 일부 Mac에서는 Xcode로 직접 빌드해야 실행 가능\n- 설치 및 빌드 방법은 GitHub README에 명시\n결론\n- MacThrottle은 Apple Silicon Mac의 열 스로틀링 상태를 실시간 감시할 수 있는 경량 도구\n- 루트 권한 없이 동작하며, 시각적 피드백·알림·그래프 기능을 통해\n개발자와 고성능 작업 사용자에게 시스템 열 상태 인식을 제공",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25416",
    "category": "AI"
  },
  {
    "title": "소프트웨어 제품이 실제로 어떻게 돌아가는지는 아무도 모른다 [번역글]",
    "content": "- \n대형 테크 시스템의 불투명성\n- 대형 테크 회사들은 자사 시스템조차 \"전쟁터의 안개(fog of war)\" 속에서 운영된다.\n- \"Y 타입 유저가 기능 X를 쓸 수 있나?\", \"액션 Z 시 정확히 무슨 일이?\", \"현재 요금제는 몇 가지?\" 같은 기본 질문에조차 조직 내 소수만 답할 수 있다.\n- 심하면 조사 전담자를 지정해야 하며, 공개 문서만 봐도 답이 나와야 할 텐데 그렇지 않다.\n- \n복잡성의 근원: Wicked Features\n- 큰 소프트웨어는 self-hosting, 무료 체험, 조직/정책 제어, 다국어 로컬라이징, 규제 준수 기능 등으로 극도로 복잡하다. 이러한 기능은 모든 새 기능에 영향을 미침\n- 예: 정책 제어 추가 시 매번 새 기능에도 적용해야 하고, 로컬라이징 시 번역도 따라가야 한다.\n- \"EU 리전 self-hosting 엔터프라이즈 고객의 특정 기능 접근 여부\"는 코드 직접 탐사나 실험으로만 확인. 이런 기능 생략 시 엄청난 수익 포기, 큰 회사와 작은 회사의 차이점.\n- \n문서화의 한계\n- 새 기능 시 상호작용 문서화는 이론적 가능하나, 시스템 변화 속도가 문서보다 빠름.\n- 정적 시스템이 아닌 동적 환경에서 문서 작성자는 변화보다 앞서야 하며, 이는 불가능 수준.\n- 더 큰 문제: 많은 동작이 명시적 의도 아닌 디폴트 설정 상호작용으로 생김 – 문서화는 실제 시스템 탐사와 같다.\n- \n답변의 핵심: 코드베이스와 엔지니어\n- 정확한 답은 코드베이스 직접 들여다보기로만 나오며, 이는 엔지니어의 권력 기반.\n- 엔지니어링 팀의 핵심 기능은 소프트웨어 질문에 답하는 능력\n- 특정 코드 머릿속에 사는 암묵적 지식(tacit knowledge) 활용.\n- 팀 재편 시 지식 소실로 \"탐색적 수술\"(코드 수정/체크 강제 등) 필요.\n- 코드 작성은 쉽지만, 신뢰성 답변은 자신감 문제로 어렵다(틀릴 위험, 요약 압축 필요).\n- \n결론: 귀중한 능력\n- 비기술직은 소프트웨어가 엔지니어에게 완벽히 이해된다고 믿으나, 대형 시스템은 누구도 완벽히 모름.\n- 기본 질문조차 반복 조사 필요, 변화 시 뉘앙스/예외 발생. 정확한 답변 능력은 극히 가치 있음.",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25415",
    "category": "트렌드"
  },
  {
    "title": "ScratchPixel - 컴퓨터 그래픽스를 기초부터 무료로 배우기",
    "content": "- 컴퓨터 그래픽스의 기초부터 고급 주제까지 무료로 학습할 수 있는 온라인 교육 플랫폼\n- \n3D 렌더링, 수학적 기초, 디지털 이미징, 절차적 생성, 툴링 등 다양한 주제를 체계적으로 다룸\n- 실습 중심의 강의 구조로, 이론보다 직접적인 결과를 먼저 경험하도록 설계\n- \nVulkan API 학습 과정(신규), 3D 프로그래밍 블로그(예정), 도서(출간 예정) 등 계속 발전중\n- 개발자와 디자이너가 컴퓨터 그래픽스의 원리와 구현 기술을 깊이 이해할 수 있는 무료 학습 자원\n---\n- \nScratchapixel은 “Learn computer graphics from scratch and for free”라는 슬로건 아래, 컴퓨터 그래픽스 전반을 무료로 학습할 수 있는 웹사이트임\n현재 등록된 과정들\n- \n3D 렌더링의 기초 (The Foundations of 3D Rendering)\n초보자 친화적 순서로 구성된 렌더링 입문 강의\n이론보다 직접적인 결과물 구현을 먼저 다루는 접근 방식을 채택\n- 주요 강의 주제는 다음과 같음\nRay-Tracing 입문, 3D 장면 렌더링, Rasterization, 투영 행렬, 셰이딩과 조명, 텍스처링, 가속 구조, 볼륨 렌더링 등\n- 각 강의는 실제 구현 예제와 함께 픽셀 좌표 계산, 핀홀 카메라 모델, BRDF 및 셰이더 개념 등을 다룸\n- \n컴퓨터 그래픽스를 위한 수학 (Mathematics for Computer Graphics)\n그래픽스 구현에 필요한 수학적 이론과 도구를 설명\n이 섹션은 입문용이 아니라, 다른 강의에서 언급된 개념을 참고할 참조용 자료로 구성\n- 주요 주제는 기하학, 행렬 역연산(Gauss-Jordan 방법) , 보간법, LookAt 함수, 셰이딩 수학, 몬테카를로 방법, 푸리에 변환 등\n- \nComputer Graphics Gems\n특정 카테고리에 속하지 않지만 흥미로운 개별 그래픽스 주제 모음\n현재는 Blackbody 복사 강의만 포함됨\n- \nGeometry\n컴퓨터 그래픽스에서 형태를 정의하는 방법을 다룸\nBézier 곡선과 곡면을 이용한 형태 표현 방법을 설명\n- \nDigital Imaging\n이미지 파일 처리와 색상 관리를 다룸\n- 주요 주제는 빛과 색 공간, 디지털 이미지의 파일-화면 변환, 기본 이미지 조작 등\n- \nProcedural Generation of Virtual Worlds\n자연 현상의 절차적 시뮬레이션을 다룸\n- \nValue Noise, Perlin Noise, 하늘색 시뮬레이션 등의 주제를 포함\n- \nTooling\n3D 도구 개발과 상호작용 기술을 다룸\n- \n윈도우 관리(Windowing) , OBJ 파일 포맷, 카메라 내비게이션 제어 등의 내용을 포함\n최근 뉴스\n- 곧 오픈할 블로그는 3D 프로그래밍뿐 아니라 AI와 교육 등 관련 주제를 다룰 예정\n- \nVulkan API에 대한 새로운 강의가 추가될 예정\n- \"Learn Computer Graphics Programming from Scratch\"라는 제목의 도서도 출간 예정",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25414",
    "category": "AI"
  },
  {
    "title": "[2025/12/22 ~ 28] 이번 주에 살펴볼 만한 AI/ML 논문 모음",
    "content": "[2025/12/22 ~ 28] 이번 주에 살펴볼 만한 AI/ML 논문 모음\nPyTorchKR🔥🇰🇷 🤔💭\n1️⃣ 심층적인 환각 탐지와 완화 전략 (Deep Hallucination Detection & Mitigation): 이번 주 선정된 논문들을 살펴보면, 단순히 모델의 크기를 키우는 것을 넘어, LLM의 고질적인 문제인 환각(Hallucination) 을 근본적으로 해결하려는 시도가 두드러집니다. QuCo-RAG는 모델 내부의 주관적 신뢰도 대신 사전 학습 데이터의 통계라는 객관적 지표를 활용해 검색 시점을 결정하며, H-Neurons는 환각을 유발하는 특정 뉴런을 식별하고 그 기원을 추적하는 미시적 접근을 취합니다. 또한, Model-First Reasoning은 문제 해결 전 명시적인 모델링 단계를 거치게 함으로써 구조적인 오류를 줄입니다. 이는 AI 연구가 단순히 '그럴듯한 답변'을 내놓는 것에서 '검증 가능하고 신뢰할 수 있는 메커니즘'을 갖추는 방향으로 진화하고 있음을 보여줍니다.\n2️⃣ 추론 효율성 및 실시간 처리 기술의 진화 (Evolution of Inference Efficiency & Real-Time Processing): 또한, 모델이 거대해짐에 따라 추론 속도와 메모리 효율성을 극대화하려는 연구가 활발합니다. WorldPlay는 속도와 메모리 간의 트레이드오프를 해결하여 실시간 비디오 생성을 가능하게 했고, Jacobi Forcing은 순차적인 생성 방식(AR)의 한계를 넘어 병렬 디코딩을 통해 추론 속도를 획기적으로 높였습니다. 또한 qTTT는 긴 문맥 처리 시 발생하는 성능 저하(점수 희석)를 막기 위해 추론 단계에서 경량화된 학습을 수행하는 새로운 접근법을 제시했습니다. 이는 고성능 모델을 실제 서비스 레벨(Real-time application)에서 활용하기 위한 필수적인 최적화 과정으로 해석됩니다.\n3️⃣ 동적 세계 이해와 구조적 추론 능력 강화 (Enhanced Dynamic World Understanding & Structured Reasoning): 정적인 이미지나 텍스트 분석을 넘어, 시간의 흐름(4D)과 물리적/논리적 구조를 이해하려는 흐름이 강합니다. 4D-RGPT는 비디오의 시간적 동역학을 이해하기 위해 3D 공간에 시간 축을 더한 4D 인식을 시도하며, WorldPlay는 기하학적 일관성을 유지하며 세계 모델링을 수행합니다. NEPA 역시 픽셀 복원 대신 임베딩 예측을 통해 시각적 이해를 높이려 합니다. 이는 AI가 단순한 패턴 매칭을 넘어, 인간처럼 물리 법칙과 논리적 인과관계를 포함한 '세계의 작동 원리' 를 내재화하는 단계로 나아가고 있음을 시사합니다.\n---\n월드플레이: 실시간 상호작용 세계 모델링을 위한 장기 기하학적 일관성 향상 / WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling\n논문 소개\nWorldPlay는 실시간 상호작용 세계 모델링을 위한 혁신적인 스트리밍 비디오 디퓨전 모델로, 장기 기하학적 일관성을 유지하면서도 속도와 메모리 간의 트레이드오프를 해결하는 데 중점을 두고 개발되었습니다. 이 모델은 세 가지 주요 혁신을 통해 성능을 극대화합니다. 첫째, Dual Action Representation을 통해 사용자의 입력에 대한 강력한 동작 제어를 가능하게 하여, 다양한 스케일의 장면에서 물리적으로 그럴듯한 움직임을 구현합니다. 둘째, Reconstituted Context Memory는 과거 프레임의 정보를 동적으로 재구성하여 장기 일관성을 유지하는 데 기여합니다. 이를 통해 기하학적으로 중요한 오래된 프레임을 접근 가능하게 하여 메모리 효율성을 높입니다. 셋째, Context Forcing이라는 새로운 증류 방법을 도입하여 메모리 인식 모델의 성능을 향상시킵니다. 이 방법은 교사와 학생 모델 간의 메모리 컨텍스트를 정렬하여 학생 모델이 장기 정보를 효과적으로 활용할 수 있도록 지원합니다.\nWorldPlay는 24 프레임 초당 720p 비디오를 생성하며, 기존 기술들과 비교할 때 우수한 일관성을 보여줍니다. 이 모델은 실시간 비디오 생성에서의 속도와 장기 기하학적 일관성을 동시에 달성하는 데 성공하였으며, 다양한 장면에서 강한 일반화를 나타냅니다. 또한, 고품질의 3D 재구성을 가능하게 하여 동적인 세계 이벤트를 트리거할 수 있는 프롬프트 기반 상호작용을 지원합니다. 이러한 특성 덕분에 WorldPlay는 실시간 상호작용 비디오 생성 분야에서 중요한 기여를 하고 있으며, 향후 다양한 응용 가능성을 열어줍니다.\n논문 초록(Abstract)\n이 논문은 실시간 상호작용 세계 모델링을 가능하게 하는 스트리밍 비디오 디퓨전 모델인 WorldPlay를 제시합니다. WorldPlay는 현재 방법들이 제한하는 속도와 메모리 간의 트레이드오프를 해결하여 장기적인 기하학적 일관성을 유지합니다. WorldPlay는 세 가지 주요 혁신에서 힘을 얻습니다. 1) 우리는 사용자의 키보드와 마우스 입력에 대한 강력한 행동 제어를 가능하게 하는 이중 행동 표현(Dual Action Representation)을 사용합니다. 2) 장기적인 일관성을 유지하기 위해, 우리의 재구성된 컨텍스트 메모리(Reconstituted Context Memory)는 과거 프레임에서 컨텍스트를 동적으로 재구성하고, 기하학적으로 중요한 그러나 오래된 프레임을 접근 가능하게 유지하기 위해 시간적 재구성을 사용하여 메모리 감소를 효과적으로 완화합니다. 3) 우리는 또한 메모리 인식 모델을 위해 설계된 새로운 증류 방법인 컨텍스트 포싱(Context Forcing)을 제안합니다. 교사와 학생 간의 메모리 컨텍스트를 정렬함으로써 학생이 장기 정보를 사용할 수 있는 능력을 유지하여 오류 드리프트를 방지하면서 실시간 속도를 가능하게 합니다. 종합적으로, WorldPlay는 기존 기술과 비교하여 우수한 일관성을 유지하며 다양한 장면에서 강력한 일반화를 보여주면서 24 FPS로 720p 비디오를 장기적으로 스트리밍할 수 있습니다. 프로젝트 페이지와 온라인 데모는 다음에서 확인할 수 있습니다: https://3d-models.hunyuan.tencent.com/world/ 및 https://3d.hunyuan.tencent.com/sceneTo3D.\nThis paper presents WorldPlay, a streaming video diffusion model that enables real-time, interactive world modeling with long-term geometric consistency, resolving the trade-off between speed and memory that limits current methods. WorldPlay draws power from three key innovations. 1) We use a Dual Action Representation to enable robust action control in response to the user's keyboard and mouse inputs. 2) To enforce long-term consistency, our Reconstituted Context Memory dynamically rebuilds context from past frames and uses temporal reframing to keep geometrically important but long-past frames accessible, effectively alleviating memory attenuation. 3) We also propose Context Forcing, a novel distillation method designed for memory-aware model. Aligning memory context between the teacher and student preserves the student's capacity to use long-range information, enabling real-time speeds while preventing error drift. Taken together, WorldPlay generates long-horizon streaming 720p video at 24 FPS with superior consistency, comparing favorably with existing techniques and showing strong generalization across diverse scenes. Project page and online demo can be found: https://3d-models.hunyuan.tencent.com/world/ and https://3d.hunyuan.tencent.com/sceneTo3D.\n논문 링크\nhttps://arxiv.org/abs/2512.14614\n더 읽어보기\nhttps://3d-models.hunyuan.tencent.com/world/\nhttps://3d.hunyuan.tencent.com/sceneTo3D\n---\nQuCo-RAG: 사전 학습 데이터에서 불확실성을 정량화하여 동적 검색 증강 생성을 위한 방법 / QuCo-RAG: Quantifying Uncertainty from the Pre-training Corpus for Dynamic Retrieval-Augmented Generation\n논문 소개\nQuCo-RAG는 대규모 언어 모델(LLM)의 생성 과정에서 동적으로 검색 시점을 결정하여 허위 정보를 완화하는 혁신적인 방법론이다. 기존의 접근 방식은 모델 내부 신호에 의존하였으나, 이는 LLM이 종종 잘 보정되지 않고 잘못된 출력에 대해 높은 신뢰도를 보이는 문제를 안고 있다. 본 연구에서는 이러한 한계를 극복하기 위해 주관적인 신뢰도 대신 사전 학습 데이터에서 계산된 객관적인 통계로 불확실성을 정량화하는 새로운 방법을 제안한다.\nQuCo-RAG의 불확실성 정량화는 두 가지 주요 단계로 구성된다. 첫 번째 단계에서는 생성 전에 긴 꼬리 지식 격차를 나타내는 저빈도 엔티티를 식별한다. 두 번째 단계에서는 생성 중에 사전 학습 데이터에서 엔티티의 동시 발생을 검증하여, 동시 발생이 0일 경우 허위 정보의 위험을 신호한다. 이러한 두 단계는 Infini-gram을 활용하여 **4조** 개의 토큰에 대해 밀리초 지연 쿼리를 수행함으로써 불확실성이 높은 상황에서 검색을 트리거한다.\n실험 결과, QuCo-RAG는 다단계 질문 응답(QA) 벤치마크에서 OLMo-2 모델을 사용하여 최신 기준선보다 5-12 포인트의 정확도(EM) 향상을 달성하였다. 또한, 비공식 사전 학습 데이터를 가진 모델(Llama, Qwen, GPT)에서도 효과적으로 이전되어 EM을 최대 14 포인트 향상시켰다. 생물 의학 QA에서의 도메인 일반화 실험은 QuCo-RAG의 강건성을 추가로 검증하였다.\nQuCo-RAG는 사전 학습 코퍼스를 기반으로 한 검증을 통해 동적 검색 증강 생성의 새로운 패러다임을 제시하며, 이는 모델 비의존적인 접근 방식으로 다양한 LLM에 적용 가능하다. 이러한 연구는 허위 정보의 위험을 줄이는 데 기여하며, 향후 다양한 도메인에 적용할 수 있는 가능성을 탐색할 예정이다.\n논문 초록(Abstract)\n다이나믹 검색-증강 생성(Dynamic Retrieval-Augmented Generation)은 대규모 언어 모델(LLMs)에서 환각을 완화하기 위해 생성 중 검색 시점을 적응적으로 결정합니다. 그러나 기존 방법은 모델 내부 신호(예: 로짓, 엔트로피)에 의존하는데, 이는 LLM이 일반적으로 잘 보정되지 않고 종종 잘못된 출력에 대해 높은 신뢰도를 보이기 때문에 근본적으로 신뢰할 수 없습니다. 우리는 주관적인 신뢰도에서 사전 학습 데이터로부터 계산된 객관적인 통계로 전환하는 QuCo-RAG를 제안합니다. 우리의 방법은 두 단계로 불확실성을 정량화합니다: (1) 생성 이전에, 우리는 긴 꼬리 지식 격차를 나타내는 저빈도 엔티티를 식별합니다; (2) 생성 중에, 우리는 사전 학습 코퍼스에서 엔티티의 동시 발생을 검증하며, 동시 발생이 없는 경우 환각 위험을 신호하는 경우가 많습니다. 두 단계 모두 **4조** 개의 토큰에 대해 밀리초 지연 쿼리를 위한 Infini-gram을 활용하여 불확실성이 높을 때 검색을 트리거합니다. 다중 홉 QA 벤치마크에 대한 실험 결과, QuCo-RAG는 OLMo-2 모델을 사용하여 최첨단 기준보다 5-12 포인트의 EM 향상을 달성하며, 비공개 사전 학습 데이터를 가진 모델(Llama, Qwen, GPT)로도 효과적으로 전이되어 EM을 최대 14 포인트 향상시킵니다. 생물 의학 QA에 대한 도메인 일반화는 우리의 패러다임의 강건성을 추가로 검증합니다. 이러한 결과는 코퍼스 기반 검증이 다이나믹 RAG를 위한 원칙적이고 실질적으로 모델 비의존적인 패러다임으로 자리잡게 합니다. 우리의 코드는 https://github.com/ZhishanQ/QuCo-RAG 에서 공개적으로 이용 가능합니다.\nDynamic Retrieval-Augmented Generation adaptively determines when to retrieve during generation to mitigate hallucinations in large language models (LLMs). However, existing methods rely on model-internal signals (e.g., logits, entropy), which are fundamentally unreliable because LLMs are typically ill-calibrated and often exhibit high confidence in erroneous outputs. We propose QuCo-RAG, which shifts from subjective confidence to objective statistics computed from pre-training data. Our method quantifies uncertainty through two stages: (1) before generation, we identify low-frequency entities indicating long-tail knowledge gaps; (2) during generation, we verify entity co-occurrence in the pre-training corpus, where zero co-occurrence often signals hallucination risk. Both stages leverage Infini-gram for millisecond-latency queries over 4 trillion tokens, triggering retrieval when uncertainty is high. Experiments on multi-hop QA benchmarks show QuCo-RAG achieves EM gains of 5--12 points over state-of-the-art baselines with OLMo-2 models, and transfers effectively to models with undisclosed pre-training data (Llama, Qwen, GPT), improving EM by up to 14 points. Domain generalization on biomedical QA further validates the robustness of our paradigm. These results establish corpus-grounded verification as a principled, practically model-agnostic paradigm for dynamic RAG. Our code is publicly available at https://github.com/ZhishanQ/QuCo-RAG.\n논문 링크\nhttps://arxiv.org/abs/2512.19134\n더 읽어보기\nhttps://github.com/ZhishanQ/QuCo-RAG\n---\n4D-RGPT: 지역 수준의 4D 이해를 위한 지각 증류 접근법 / 4D-RGPT: Toward Region-level 4D Understanding via Perceptual Distillation\n논문 소개\n4D-RGPT는 비디오 입력에서 4D 표현을 효과적으로 포착하기 위해 설계된 전문화된 멀티모달 대규모 언어 모델(MMLM)이다. 기존의 3D 및 4D 비디오 질문 응답(VQA) 벤치마크는 정적 장면에 중점을 두고 있으며, 지역 수준의 프롬프트가 부족하여 시간적 동역학에 대한 이해가 제한적이었다. 이러한 문제를 해결하기 위해 본 연구에서는 지각적 4D 증류(Perceptual 4D Distillation, P4D)라는 혁신적인 학습 프레임워크를 도입하여, 동결된 전문가 모델에서 4D 표현을 4D-RGPT로 전이함으로써 포괄적인 4D 인식을 가능하게 한다.\nR4D-Bench라는 새로운 벤치마크도 제안되었으며, 이는 깊이 인식 동적 장면을 위한 지역 수준의 프롬프트를 포함하고 있다. R4D-Bench는 하이브리드 자동화 및 인간 검증 파이프라인을 통해 구축되어, 기존의 비지역 기반 4D VQA 벤치마크의 한계를 극복하고자 한다. 이 벤치마크는 4D 이해의 다양한 측면을 평가하기 위해 9개의 질문 카테고리를 포함하고 있으며, 각 카테고리는 MMLM의 성능을 종합적으로 평가하는 기준을 제공한다.\n질문 옵션 형식은 MMLM이 정확한 답변을 제공하기 위해 필요한 정밀도를 요구하며, 이는 객체의 위치와 방향을 이해하는 데 필수적이다. 이러한 접근 방식은 MMLM의 4D 이해 능력을 향상시키고, 지역 기반 질문을 통해 보다 깊이 있는 평가를 가능하게 한다. 본 연구는 4D-RGPT와 R4D-Bench를 통해 기존의 VQA 시스템의 한계를 극복하고, 4D 인식 및 시간적 이해를 향상시키는 중요한 기여를 하고 있다.\n논문 초록(Abstract)\n다양한 멀티모달 대규모 언어 모델(MLLM)의 발전에도 불구하고, 3D 구조와 시간적 동역학에 대한 추론 능력은 여전히 제한적이며, 이는 약한 4D 인식과 시간적 이해에 의해 제약받고 있습니다. 기존의 3D 및 4D 비디오 질문 응답(VQA) 벤치마크는 정적인 장면에 중점을 두고 있으며, 지역 수준의 프롬프트가 부족합니다. 우리는 다음과 같은 문제를 해결하기 위해 다음을 도입합니다: (a) 향상된 시간적 인식을 통해 비디오 입력에서 4D 표현을 캡처하도록 설계된 전문화된 MLLM인 4D-RGPT; (b) 고정된 전문가 모델에서 4D 표현을 4D-RGPT로 전이하여 포괄적인 4D 인식을 위한 훈련 프레임워크인 지각적 4D 증류(P4D); (c) 지역 수준의 프롬프트를 갖춘 깊이 인식 동적 장면을 위한 벤치마크인 R4D-Bench로, 하이브리드 자동화 및 인간 검증 파이프라인을 통해 구축되었습니다. 우리의 4D-RGPT는 기존의 4D VQA 벤치마크와 제안된 R4D-Bench 벤치마크 모두에서 주목할 만한 개선을 달성했습니다.\nDespite advances in Multimodal LLMs (MLLMs), their ability to reason over 3D structures and temporal dynamics remains limited, constrained by weak 4D perception and temporal understanding. Existing 3D and 4D Video Question Answering (VQA) benchmarks also emphasize static scenes and lack region-level prompting. We tackle these issues by introducing: (a) 4D-RGPT, a specialized MLLM designed to capture 4D representations from video inputs with enhanced temporal perception; (b) Perceptual 4D Distillation (P4D), a training framework that transfers 4D representations from a frozen expert model into 4D-RGPT for comprehensive 4D perception; and (c) R4D-Bench, a benchmark for depth-aware dynamic scenes with region-level prompting, built via a hybrid automated and human-verified pipeline. Our 4D-RGPT achieves notable improvements on both existing 4D VQA benchmarks and the proposed R4D-Bench benchmark.\n논문 링크\nhttps://arxiv.org/abs/2512.17012\n더 읽어보기\nhttps://ca-joe-yang.github.io/resource/projects/4D_RGPT\n---\nH-뉴런: 대규모 언어 모델에서 환각 관련 뉴런의 존재, 영향 및 기원에 대한 연구 / H-Neurons: On the Existence, Impact, and Origin of Hallucination-Associated Neurons in LLMs\n논문 소개\n대규모 언어 모델(LLMs)에서 발생하는 환각 문제는 모델의 신뢰성을 저해하는 주요 요인 중 하나로, 그럴듯하지만 사실과 다른 출력을 생성하는 현상을 의미한다. 본 연구에서는 환각과 관련된 뉴런, 즉 H-Neurons의 존재와 그 영향, 기원을 체계적으로 분석하였다. H-Neurons의 식별 과정에서는 전체 뉴런의 0.**1%** 미만의 희소한 뉴런 집합이 환각 발생을 신뢰성 있게 예측할 수 있음을 입증하였다. 이러한 뉴런들은 다양한 시나리오에서 강한 일반화 능력을 보여주었다.\n행동적 영향 측면에서, 통제된 개입을 통해 H-Neurons가 과도한 순응 행동과 인과적으로 연결되어 있음을 발견하였다. 이는 환각 발생에 기여하는 뉴런들이 단순히 우연히 활성화되는 것이 아니라, 특정 행동 패턴과 밀접한 관계가 있음을 시사한다. 기원 측면에서는 H-Neurons가 사전 학습된 기본 모델에서 유래되며, 이들 뉴런이 환각 탐지에 대한 예측력을 유지함을 확인하였다. 이는 H-Neurons가 모델의 초기 학습 과정에서 형성된다는 중요한 통찰을 제공한다.\n연구 방법론으로는, 환각과 관련된 뉴런을 강력하게 식별하기 위해 TriviaQA 데이터셋을 활용하여 신뢰할 수 있는 출력과 환각 출력을 구분하는 고품질 대조 집합을 구축하였다. 이후, 각 뉴런의 기여도를 정량화하기 위해 선형 분류기를 훈련하고, 이를 통해 환각 여부를 예측하는 이진 레이블을 생성하였다. 이러한 접근은 H-Neurons의 기능적 영향을 명확히 평가할 수 있는 기반을 마련하였다.\n마지막으로, 본 연구는 LLM에서 환각과 관련된 뉴런의 신경 메커니즘을 이해하는 데 기여하며, 향후 보다 신뢰할 수 있는 LLM 개발을 위한 중요한 기초 자료를 제공한다. 이러한 발견은 LLM의 신뢰성을 높이기 위한 연구에 있어 필수적인 통찰을 제공하며, 향후 연구 방향에 대한 중요한 기초를 마련한다.\n논문 초록(Abstract)\n대규모 언어 모델(LLM)은 자주 환각을 생성하는데, 이는 그럴듯하지만 사실과 일치하지 않는 출력으로 신뢰성을 저해합니다. 이전 연구에서는 훈련 데이터와 목표와 같은 거시적 관점에서 환각을 조사했지만, 기본적인 뉴런 수준의 메커니즘은 대부분 탐구되지 않았습니다. 본 논문에서는 LLM의 환각 관련 뉴런(H-뉴런)에 대해 세 가지 관점에서 체계적인 조사를 수행합니다: 식별, 행동적 영향, 그리고 기원. 식별 측면에서, 우리는 전체 뉴런의 **$0**.**1%**$ 미만이라는 놀라울 정도로 희소한 뉴런 집합이 환각 발생을 신뢰성 있게 예측할 수 있음을 보여주며, 다양한 시나리오에서 강한 일반화를 보입니다. 행동적 영향 측면에서, 통제된 개입을 통해 이러한 뉴런이 과도한 순응 행동과 인과적으로 연결되어 있음을 밝혀냅니다. 기원에 관해서는, 이러한 뉴런이 사전 학습된 기본 모델로 거슬러 올라가며, 환각 탐지에 대한 예측력을 유지함을 발견하여, 이들이 사전 학습 중에 나타남을 나타냅니다. 우리의 발견은 거시적 행동 패턴과 미시적 신경 메커니즘을 연결하여, 보다 신뢰할 수 있는 LLM 개발을 위한 통찰을 제공합니다.\nLarge language models (LLMs) frequently generate hallucinations -- plausible but factually incorrect outputs -- undermining their reliability. While prior work has examined hallucinations from macroscopic perspectives such as training data and objectives, the underlying neuron-level mechanisms remain largely unexplored. In this paper, we conduct a systematic investigation into hallucination-associated neurons (H-Neurons) in LLMs from three perspectives: identification, behavioral impact, and origins. Regarding their identification, we demonstrate that a remarkably sparse subset of neurons (less than **$0**.**1%**$ of total neurons) can reliably predict hallucination occurrences, with strong generalization across diverse scenarios. In terms of behavioral impact, controlled interventions reveal that these neurons are causally linked to over-compliance behaviors. Concerning their origins, we trace these neurons back to the pre-trained base models and find that these neurons remain predictive for hallucination detection, indicating they emerge during pre-training. Our findings bridge macroscopic behavioral patterns with microscopic neural mechanisms, offering insights for developing more reliable LLMs.\n논문 링크\nhttps://arxiv.org/abs/2512.01797\n---\n다음 임베딩 예측이 강력한 비전 학습자를 만든다 / Next-Embedding Prediction Makes Strong Vision Learners\n논문 소개\n자기 지도 학습(self-supervised learning)은 대규모 주석 없는 데이터셋을 활용하여 표현을 학습하는 중요한 방법론으로 자리잡고 있으며, 최근에는 대조적 학습(contrastive learning) 및 자기 증류(self-distillation)와 같은 다양한 접근법이 발전해왔다. 그러나 이러한 방법들은 종종 대규모 배치나 메모리 뱅크를 필요로 하며, 경량 디코더를 통한 재구성 목표(reconstruction objectives)도 그 한계를 드러내고 있다. 이에 대한 대안으로 제안된 예측 표현 학습(predictive representation learning)은 원시 입력 대신 의미론적 임베딩을 예측하는 접근법으로, 특히 JEPA(Just-Embedding Predictive Autoregression)와 같은 방법이 주목받고 있다. 그러나 JEPA는 표현 중심으로, 사전 학습된 인코더가 다운스트림 모듈에 의해 별도로 소비되는 특징을 생성하는 한계가 있다.\n이러한 배경에서 제안된 Next-Embedding Predictive Autoregression(NEPA) 접근법은 과거 패치 임베딩을 조건으로 미래 패치 임베딩을 예측하는 방식으로, 인과 마스킹(causal masking)과 그래디언트 중단(stop gradient) 기법을 활용한다. NEPA는 모델이 다운스트림 작업을 위해 특징을 출력하는 대신, 예측 작업을 직접 수행하도록 학습하는 데 중점을 두고 있다. 이 방법론은 단순한 트랜스포머(Transformer) 아키텍처를 기반으로 하며, ImageNet-1k 데이터셋에서 사전 학습을 통해 높은 성능을 발휘한다. 특히, 픽셀 재구성, 이산 토큰, 대조 손실, 작업 특정 헤드 없이도 강력한 성능을 유지하는 점이 주목할 만하다.\nNEPA는 ViT-B 및 ViT-L 백본을 사용하여 ImageNet-1K에서 각각 83.**8%** 및 85.**3%**의 top-1 정확도를 달성하였으며, ADE20K에서의 의미론적 분할(semantic segmentation) 작업으로도 효과적으로 전이되었다. 이러한 결과는 NEPA가 단순하고 확장 가능하며, 잠재적으로 모달리티에 구애받지 않는 대안으로 시각적 자기 지도 학습에 기여할 수 있음을 보여준다. NEPA의 연구는 예측을 통해 작업 행동을 직접 유도할 수 있는 가능성을 제시하며, 향후 다양한 비전 작업에서의 활용 가능성을 열어주는 중요한 기여를 하고 있다.\n논문 초록(Abstract)\n자연어에서 생성적 사전학습의 성공에 영감을 받아, 우리는 동일한 원칙이 강력한 자기 지도 시각 학습자를 생성할 수 있는지 질문합니다. 모델이 다운스트림 사용을 위한 특징을 출력하도록 훈련하는 대신, 우리는 예측 작업을 직접 수행하기 위해 임베딩을 생성하도록 훈련합니다. 이 연구는 표현 학습에서 모델 학습으로의 전환을 탐구합니다. 구체적으로, 모델은 과거 패치 임베딩을 조건으로 미래 패치 임베딩을 예측하도록 학습하며, 이를 인과 마스킹과 그래디언트 정지를 사용하여 수행합니다. 우리는 이를 다음 임베딩 예측 자기 회귀(Next-Embedding Predictive Autoregression, NEPA)라고 부릅니다. 우리는 이미지넷-1k에서 사전학습된 간단한 트랜스포머가 다음 임베딩 예측을 유일한 학습 목표로 삼았을 때 효과적임을 입증합니다. 픽셀 재구성, 이산 토큰, 대조 손실 또는 작업 특정 헤드가 필요하지 않습니다. 이 공식은 추가적인 설계 복잡성 없이 구조적 단순성과 확장성을 유지합니다. NEPA는 다양한 작업에서 강력한 결과를 달성하며, ViT-B와 ViT-L 백본을 사용하여 이미지넷-1K에서 각각 83.**8%**와 85.**3%**의 top-1 정확도를 기록하고, ADE20K에서 의미론적 분할로 효과적으로 전이됩니다. 우리는 임베딩에서의 생성적 사전학습이 시각 자기 지도 학습에 대한 간단하고 확장 가능하며 잠재적으로 모달리티에 구애받지 않는 대안을 제공한다고 믿습니다.\nInspired by the success of generative pretraining in natural language, we ask whether the same principles can yield strong self-supervised visual learners. Instead of training models to output features for downstream use, we train them to generate embeddings to perform predictive tasks directly. This work explores such a shift from learning representations to learning models. Specifically, models learn to predict future patch embeddings conditioned on past ones, using causal masking and stop gradient, which we refer to as Next-Embedding Predictive Autoregression (NEPA). We demonstrate that a simple Transformer pretrained on ImageNet-1k with next embedding prediction as its sole learning objective is effective - no pixel reconstruction, discrete tokens, contrastive loss, or task-specific heads. This formulation retains architectural simplicity and scalability, without requiring additional design complexity. NEPA achieves strong results across tasks, attaining 83.**8%** and 85.**3%** top-1 accuracy on ImageNet-1K with ViT-B and ViT-L backbones after fine-tuning, and transferring effectively to semantic segmentation on ADE20K. We believe generative pretraining from embeddings provides a simple, scalable, and potentially modality-agnostic alternative to visual self-supervised learning.\n논문 링크\nhttps://arxiv.org/abs/2512.16922\n더 읽어보기\nhttps://sihanxu.me/nepa\n---\n모델 우선 추론 LLM 에이전트: 명시적 문제 모델링을 통한 환각 감소 / Model-First Reasoning LLM Agents: Reducing Hallucinations through Explicit Problem Modeling\n논문 소개\n대규모 언어 모델(LLM)은 복잡한 다단계 계획 작업에서 높은 비율의 제약 위반과 일관성 없는 솔루션을 보여주는 경향이 있다. 기존의 Chain-of-Thought(사고의 연쇄) 및 ReAct(반응적 행동)와 같은 전략은 암묵적인 상태 추적에 의존하며, 명시적인 문제 표현이 부족하여 이러한 한계를 극복하지 못하고 있다. 본 연구에서는 고전 인공지능(AI) 계획에서 영감을 받아 모델 우선 추론(Model-First Reasoning, MFR) 이라는 새로운 두 단계의 패러다임을 제안한다. 이 접근법에서는 LLM이 먼저 문제의 명시적인 모델을 구축한 후, 이를 기반으로 솔루션 계획을 생성하도록 한다.\nMFR은 여러 계획 도메인에서 실험을 통해 제약 준수와 솔루션 품질을 향상시키는 결과를 보여주었다. 특히, 의료 일정 관리, 경로 계획, 자원 할당, 논리 퍼즐 및 절차적 합성 등 다양한 분야에서 MFR의 효과가 입증되었다. 아블레이션 연구를 통해 명시적인 모델링 단계가 이러한 성과에 필수적이라는 점이 강조되었다. 연구 결과는 LLM의 계획 실패가 주로 표현의 결함에서 비롯된다는 점을 시사하며, 이는 추론의 한계가 아닌 문제 표현의 부족에 기인한다.\nMFR은 문제의 명시적인 모델을 구성하는 단계와 솔루션을 생성하는 단계로 나뉘며, 이 과정에서 엔티티, 상태 변수, 행동 및 제약을 정의한다. 이러한 명시적인 모델링은 LLM이 보다 구조화된 방식으로 문제를 이해하고 해결할 수 있도록 돕는다. 본 연구는 LLM 기반 계획 및 추론 작업에서의 표현 실패를 해결하기 위한 기초를 제공하며, 신뢰할 수 있는 AI 에이전트를 위한 중요한 기여를 한다. 모든 프롬프트, 평가 절차 및 작업 데이터셋이 문서화되어 재현성을 촉진하고, 향후 연구에 대한 기초를 마련한다.\n논문 초록(Abstract)\n대규모 언어 모델(LLM)은 복잡한 다단계 계획 작업에서 종종 어려움을 겪으며, 제약 위반율이 높고 일관되지 않은 솔루션을 보여줍니다. 체인 오브 사고(Chain-of-Thought)와 리액트(ReAct)와 같은 기존 전략은 암묵적인 상태 추적에 의존하며 명시적인 문제 표현이 부족합니다. 고전 AI 계획에서 영감을 받아, 우리는 모델 우선 추론(Model-First Reasoning, MFR)을 제안합니다. MFR은 LLM이 먼저 문제의 명시적인 모델을 구축하고, 엔티티, 상태 변수, 행동 및 제약을 정의한 후 솔루션 계획을 생성하는 두 단계의 패러다임입니다. 의료 일정 계획, 경로 계획, 자원 할당, 논리 퍼즐 및 절차적 합성을 포함한 여러 계획 도메인에서 MFR은 제약 위반을 줄이고 체인 오브 사고 및 리액트에 비해 솔루션 품질을 향상시킵니다. 제거 연구 결과, 명시적인 모델링 단계가 이러한 개선에 중요하다는 것을 보여줍니다. 우리의 결과는 많은 LLM 계획 실패가 추론 한계가 아닌 표현적 결함에서 비롯된다는 것을 시사하며, 강력하고 해석 가능한 AI 에이전트를 위한 핵심 요소로서 명시적인 모델링을 강조합니다. 모든 프롬프트, 평가 절차 및 작업 데이터셋은 재현성을 용이하게 하기 위해 문서화되었습니다.\nLarge Language Models (LLMs) often struggle with complex multi-step planning tasks, showing high rates of constraint violations and inconsistent solutions. Existing strategies such as Chain-of-Thought and ReAct rely on implicit state tracking and lack an explicit problem representation. Inspired by classical AI planning, we propose Model-First Reasoning (MFR), a two-phase paradigm in which the LLM first constructs an explicit model of the problem, defining entities, state variables, actions, and constraints, before generating a solution plan. Across multiple planning domains, including medical scheduling, route planning, resource allocation, logic puzzles, and procedural synthesis, MFR reduces constraint violations and improves solution quality compared to Chain-of-Thought and ReAct. Ablation studies show that the explicit modeling phase is critical for these gains. Our results suggest that many LLM planning failures stem from representational deficiencies rather than reasoning limitations, highlighting explicit modeling as a key component for robust and interpretable AI agents. All prompts, evaluation procedures, and task datasets are documented to facilitate reproducibility.\n논문 링크\nhttps://arxiv.org/abs/2512.14474\n---\n맥락에 단순히 의존하지 말자: 긴 맥락 LLM을 위한 테스트 시간 학습 / Let's (not) just put things in Context: Test-Time Training for Long-Context LLMs\n논문 소개\n대규모 언어 모델(LLM)의 발전은 긴 컨텍스트를 처리하는 능력을 크게 향상시켰지만, 이러한 모델이 실제로 긴 컨텍스트에서 효과적으로 작동하지 못하는 문제를 해결하는 것이 중요하다. 본 연구에서는 기존의 추론 시간 전략이 성능을 개선하기 위해 사용하는 생각 토큰(thinking tokens) 생성 방식이 점수 희석(score dilution) 문제로 인해 한계를 가진다는 점을 지적한다. 점수 희석은 정적 자기 어텐션(static self-attention) 특성으로 인해 발생하며, 이는 긴 컨텍스트에서 모델의 정확도를 저하시킨다.\n이러한 문제를 해결하기 위해, 본 연구는 쿼리 전용 테스트 시간 학습(query-only test-time training, qTTT)이라는 새로운 방법론을 제안한다. qTTT는 주어진 컨텍스트에 대한 목표 그래디언트 업데이트를 통해 정적 자기 어텐션의 한계를 극복하며, 긴 컨텍스트에서의 성능 향상을 목표로 한다. 실험 결과, qTTT는 기존의 추론 시간 전략보다 더 효과적인 접근 방식을 제공하며, Qwen3-4B 모델에서 LongBench-v2 및 ZeroScrolls 벤치마크의 하위 집합에 대해 평균 12.**6%** 및 14.**1%** 포인트의 성능 향상을 이끌어낸다.\n이 연구는 긴 컨텍스트에서의 성능 향상을 위해 컨텍스트에 특화된 소량의 학습이 필요하다는 점을 강조하며, 이는 추론 계산의 더 나은 활용을 의미한다. qTTT의 도입은 긴 컨텍스트 LLM의 성능을 극대화할 수 있는 실질적인 방법을 제시하며, 향후 연구에서 긴 컨텍스트 처리의 새로운 방향성을 제시할 것으로 기대된다. 이러한 혁신적인 접근은 LLM의 활용 가능성을 더욱 넓히고, 다양한 응용 분야에서의 성능 개선에 기여할 것으로 보인다.\n논문 초록(Abstract)\n훈련 및 아키텍처 전략의 발전으로 수백만 개의 토큰을 포함하는 긴 문맥 길이를 가진 대규모 언어 모델(LLM)이 가능해졌습니다. 그러나 경험적 증거에 따르면 이러한 긴 문맥 LLM은 신뢰성 있게 사용할 수 있는 것보다 훨씬 더 많은 텍스트를 소비할 수 있습니다. 반면, 추론 시간의 계산을 사용하여 다단계 추론을 포함하는 도전적인 작업에서 LLM의 성능을 확장할 수 있다는 것이 입증되었습니다. 샌드박스 긴 문맥 작업에 대한 통제된 실험을 통해, 이러한 추론 시간 전략이 빠르게 수익이 감소하고 긴 문맥에서 실패한다는 것을 발견했습니다. 우리는 이러한 실패를 정적 자기 어텐션에 내재된 현상인 점수 희석(score dilution)으로 귀속시킵니다. 또한, 현재의 추론 시간 전략이 특정 조건에서 관련 긴 문맥 신호를 검색할 수 없음을 보여줍니다. 우리는 주어진 문맥에 대한 목표 그래디언트 업데이트를 통해 정적 자기 어텐션의 한계를 극복하는 간단한 방법을 제안합니다. 우리는 추론 시간 계산이 사용되는 방식의 변화가 모델과 긴 문맥 벤치마크 전반에 걸쳐 일관되게 큰 성능 향상을 가져온다는 것을 발견했습니다. 우리의 방법은 LongBench-v2 및 ZeroScrolls 벤치마크의 하위 집합에서 Qwen3-4B에 대해 평균 12.6 및 14.1 퍼센트 포인트의 큰 향상을 이끌어냅니다. 실질적인 결론은 다음과 같습니다: 긴 문맥의 경우, 문맥에 특화된 소량의 학습이 더 많은 사고 토큰을 생성하는 현재의 추론 시간 확장 전략보다 추론 계산을 더 잘 활용하는 방법입니다.\nProgress on training and architecture strategies has enabled LLMs with millions of tokens in context length. However, empirical evidence suggests that such long-context LLMs can consume far more text than they can reliably use. On the other hand, it has been shown that inference-time compute can be used to scale performance of LLMs, often by generating thinking tokens, on challenging tasks involving multi-step reasoning. Through controlled experiments on sandbox long-context tasks, we find that such inference-time strategies show rapidly diminishing returns and fail at long context. We attribute these failures to score dilution, a phenomenon inherent to static self-attention. Further, we show that current inference-time strategies cannot retrieve relevant long-context signals under certain conditions. We propose a simple method that, through targeted gradient updates on the given context, provably overcomes limitations of static self-attention. We find that this shift in how inference-time compute is spent leads to consistently large performance improvements across models and long-context benchmarks. Our method leads to large 12.6 and 14.1 percentage point improvements for Qwen3-4B on average across subsets of LongBench-v2 and ZeroScrolls benchmarks. The takeaway is practical: for long context, a small amount of context-specific training is a better use of inference compute than current inference-time scaling strategies like producing more thinking tokens.\n논문 링크\nhttps://arxiv.org/abs/2512.13898\n---\n대규모 언어 모델을 활용한 강화학습 안정화: 공식화 및 실천 / Stabilizing Reinforcement Learning with LLMs: Formulation and Practices\n논문 소개\n강화학습(Reinforcement Learning, RL)은 다양한 분야에서 성공적으로 적용되고 있지만, 훈련 과정에서의 불안정성 문제는 여전히 해결해야 할 주요 과제입니다. 본 연구는 대규모 언어 모델(Large Language Models, LLMs)을 활용하여 RL의 안정성을 높이는 새로운 방법론을 제안합니다. 특히, 정책 그래디언트 방법론인 REINFORCE를 통해 시퀀스 수준 보상을 대체하는 토큰 수준 목표를 최적화할 수 있는 조건을 규명하였습니다. 1차 근사를 통해, 이 대체 목표가 유효해지는 조건은 학습-추론 불일치와 정책의 노후화가 최소화될 때임을 보여줍니다.\n이러한 통찰력은 중요도 샘플링 보정, 클리핑, 그리고 Mixture-of-Experts (MoE) 모델을 위한 Routing Replay와 같은 기술들이 RL 훈련의 안정화에 미치는 영향을 설명하는 데 기여합니다. 30B MoE 모델을 사용한 수백만 GPU 시간에 걸친 실험을 통해, 온-정책 훈련에서 중요도 샘플링 보정을 포함한 기본 정책 그래디언트 알고리즘이 가장 높은 훈련 안정성을 달성함을 입증하였습니다. 또한, 오프-정책 업데이트를 도입하여 수렴을 가속화할 때, 클리핑과 Routing Replay의 결합이 정책의 노후화로 인한 불안정을 완화하는 데 필수적임을 강조합니다.\n훈련이 안정화된 후에는 초기화 방식에 관계없이 지속적인 최적화가 일관된 최종 성능을 발휘함을 보여줍니다. 이러한 연구 결과는 안정적인 RL 훈련을 위한 새로운 통찰력을 제공하며, 향후 연구에 기여할 수 있는 중요한 기초를 마련합니다. 본 논문은 대규모 언어 모델을 활용한 강화학습의 안정화에 대한 혁신적인 접근 방식을 제시하며, RL 훈련의 불안정성을 해결하기 위한 중요한 기여를 하고 있습니다.\n논문 초록(Abstract)\n이 논문은 대규모 언어 모델(LLM)을 활용한 강화학습(RL)에 대한 새로운 공식을 제안하며, 진정한 시퀀스 수준의 보상을 어떻게 그리고 어떤 조건에서 대리 토큰 수준의 목표를 통해 정책 경량화 방법인 REINFORCE에서 최적화할 수 있는지를 설명합니다. 구체적으로, 1차 근사를 통해 우리는 이 대리가 훈련-추론 불일치와 정책 노후화가 최소화될 때만 점점 더 유효해진다는 것을 보여줍니다. 이 통찰은 중요 샘플링 보정, 클리핑, 그리고 특히 전문가 혼합(Mixture-of-Experts, MoE) 모델을 위한 라우팅 리플레이(Routing Replay)와 같은 여러 널리 채택된 기술들이 RL 훈련을 안정화하는 데 중요한 역할을 하는 이유를 원칙적으로 설명합니다. 수십만 GPU 시간을 소요한 30B MoE 모델을 이용한 광범위한 실험을 통해, 온-정책 훈련에서는 중요 샘플링 보정이 포함된 기본 정책 경량화 알고리즘이 가장 높은 훈련 안정성을 달성함을 보여줍니다. 오프-정책 업데이트가 수렴을 가속화하기 위해 도입될 때, 클리핑과 라우팅 리플레이의 결합이 정책 노후화로 인한 불안정을 완화하는 데 필수적입니다. 특히, 훈련이 안정화되면, 장기 최적화는 초기화 방식에 관계없이 일관되게 유사한 최종 성능을 제공합니다. 우리는 공유된 통찰과 안정적인 RL 훈련을 위한 개발된 레시피가 향후 연구에 도움이 되기를 바랍니다.\nThis paper proposes a novel formulation for reinforcement learning (RL) with large language models, explaining why and under what conditions the true sequence-level reward can be optimized via a surrogate token-level objective in policy gradient methods such as REINFORCE. Specifically, through a first-order approximation, we show that this surrogate becomes increasingly valid only when both the training-inference discrepancy and policy staleness are minimized. This insight provides a principled explanation for the crucial role of several widely adopted techniques in stabilizing RL training, including importance sampling correction, clipping, and particularly Routing Replay for Mixture-of-Experts (MoE) models. Through extensive experiments with a 30B MoE model totaling hundreds of thousands of GPU hours, we show that for on-policy training, the basic policy gradient algorithm with importance sampling correction achieves the highest training stability. When off-policy updates are introduced to accelerate convergence, combining clipping and Routing Replay becomes essential to mitigate the instability caused by policy staleness. Notably, once training is stabilized, prolonged optimization consistently yields comparable final performance regardless of cold-start initialization. We hope that the shared insights and the developed recipes for stable RL training will facilitate future research.\n논문 링크\nhttps://arxiv.org/abs/2512.01374\n---\n재귀 강제를 이용한 빠르고 정확한 인과적 병렬 디코딩 / Fast and Accurate Causal Parallel Decoding using Jacobi Forcing\n논문 소개\n대규모 언어 모델의 추론 속도를 향상시키기 위한 연구가 활발히 진행되고 있는 가운데, 본 연구는 자코비 포싱(Jacobi Forcing)이라는 혁신적인 방법론을 제안한다. 이 방법론은 다중 토큰 생성을 통해 트랜스포머 기반 모델의 병렬 디코딩을 가능하게 하여, 추론 지연을 최소화하는 데 초점을 맞춘다. 기존의 디퓨전 대규모 언어 모델(dLLMs) 접근 방식은 사전학습(pre-training)과 사후학습(post-training) 간의 불일치로 인해 성능 향상에 한계를 보였다. 특히, dLLMs는 양방향 어텐션을 사용하여 인과적 사전(causal prior)과의 충돌을 야기하며, 이는 정확한 키-값 캐시(KV cache) 재사용을 방해한다.\n자코비 포싱은 모델이 자신의 생성된 병렬 디코딩 경로에서 학습하도록 하여, 사전학습된 인과 추론 속성을 유지하면서 효율적인 병렬 디코더로 전환하는 점진적 증류(paradigm)이다. 이 방법론을 통해 학습된 자코비 포싱 모델은 코딩 및 수학 벤치마크에서 3.8배의 벽시 속도 향상을 달성하면서도 성능 손실을 최소화하였다. 또한, 거부 재활용(rejection recycling)을 통한 다중 블록 디코딩을 도입하여, 각 반복에서 최대 4.5배의 높은 토큰 수용량을 가능하게 하고, 거의 4.0배의 벽시 속도 향상을 이루었다.\n이 연구는 자코비 포싱을 통해 AR 모델의 인과적 추론 속성을 유지하면서도 효율적인 병렬 디코딩을 가능하게 하는 방법론을 제시하며, 대규모 언어 모델의 추론 속도를 획기적으로 향상시킬 수 있는 가능성을 보여준다. 이러한 접근은 자연어 처리(NLP) 분야에서의 모델 효율성을 크게 개선할 수 있는 잠재력을 지니고 있으며, 향후 연구에 중요한 기여를 할 것으로 기대된다.\n논문 초록(Abstract)\n다중 토큰 생성은 트랜스포머 기반 대규모 모델 추론을 가속화하기 위한 유망한 패러다임으로 부상하였습니다. 최근의 노력은 주로 추론 지연을 줄이기 위해 병렬 디코딩을 위한 디퓨전 대규모 언어 모델(dLLMs)을 탐색하고 있습니다. AR 수준의 생성 품질을 달성하기 위해 많은 기술들이 AR 모델을 dLLMs에 적응시켜 병렬 디코딩을 가능하게 하고 있습니다. 그러나 이들은 사전학습과 사후학습 간의 불일치로 인해 AR 모델에 비해 제한된 속도 향상을 겪고 있습니다. 구체적으로, 사후학습에서의 마스킹된 데이터 분포는 사전학습 중에 관찰된 실제 데이터 분포와 크게 다르며, dLLMs는 양방향 어텐션에 의존하는데, 이는 사전학습 중에 학습된 인과적 선행과 충돌하여 정확한 KV 캐시 재사용의 통합을 방해합니다. 이를 해결하기 위해 우리는 Jacobi Forcing을 도입합니다. 이는 모델이 자신의 생성된 병렬 디코딩 경로에서 학습되는 점진적 증류 패러다임으로, AR 모델을 효율적인 병렬 디코더로 부드럽게 전환하면서 사전학습된 인과 추론 속성을 유지합니다. 이 패러다임 하에 훈련된 모델인 Jacobi Forcing Model은 코딩 및 수학 벤치마크에서 성능 손실을 최소화하면서 3.8배의 벽시계 속도 향상을 달성합니다. Jacobi Forcing Models의 경로 특성을 기반으로 우리는 거부 재활용을 통한 다중 블록 디코딩을 도입하여, 반복당 최대 4.5배의 높은 토큰 수용량을 가능하게 하고 거의 4.0배의 벽시계 속도 향상을 이루어내며, 추가적인 계산을 통해 낮은 추론 지연을 효과적으로 거래합니다. 우리의 코드는 https://github.com/hao-ai-lab/JacobiForcing 에서 이용 가능합니다.\nMulti-token generation has emerged as a promising paradigm for accelerating transformer-based large model inference. Recent efforts primarily explore diffusion Large Language Models (dLLMs) for parallel decoding to reduce inference latency. To achieve AR-level generation quality, many techniques adapt AR models into dLLMs to enable parallel decoding. However, they suffer from limited speedup compared to AR models due to a pretrain-to-posttrain mismatch. Specifically, the masked data distribution in post-training deviates significantly from the real-world data distribution seen during pretraining, and dLLMs rely on bidirectional attention, which conflicts with the causal prior learned during pretraining and hinders the integration of exact KV cache reuse. To address this, we introduce Jacobi Forcing, a progressive distillation paradigm where models are trained on their own generated parallel decoding trajectories, smoothly shifting AR models into efficient parallel decoders while preserving their pretrained causal inference property. The models trained under this paradigm, Jacobi Forcing Model, achieves 3.8x wall-clock speedup on coding and math benchmarks with minimal loss in performance. Based on Jacobi Forcing Models' trajectory characteristics, we introduce multi-block decoding with rejection recycling, which enables up to 4.5x higher token acceptance count per iteration and nearly 4.0x wall-clock speedup, effectively trading additional compute for lower inference latency. Our code is available at https://github.com/hao-ai-lab/JacobiForcing.\n논문 링크\nhttps://arxiv.org/abs/2512.14681\n더 읽어보기\nhttps://github.com/hao-ai-lab/JacobiForcing\n---\n대규모 언어 모델(LLM)의 피해: 분류 및 논의 / LLM Harms: A Taxonomy and Discussion\n논문 소개\n대규모 언어 모델(LLM)과 관련된 해악의 범주를 다룬 연구이다. 연구에서는 AI 애플리케이션 개발 전, 중, 후에 발생할 수 있는 다섯 가지 해악 범주를 제시한다: 개발 전, 직접 출력, 오용 및 악의적 응용, 그리고 하위 응용이다. 현재의 환경에서 위험을 정의할 필요성을 강조하며, 책임성, 투명성 및 편향을 관리하는 방법을 제시한다. 또한, 특정 도메인에 대한 완화 전략과 향후 방향성을 제안하며, LLM의 책임 있는 개발 및 통합을 위한 동적 감사 시스템을 안내하는 표준화된 제안을 포함한다.\n논문 초록(Abstract)\n이 연구는 인공지능 분야에서 대규모 언어 모델(LLMs)을 둘러싼 해악의 범주를 다룹니다. 이는 AI 애플리케이션의 개발 전, 개발 중, 개발 후에 다루어지는 다섯 가지 해악 범주인 사전 개발, 직접 출력, 오용 및 악의적 적용, 그리고 하류 응용을 포함합니다. 현재의 환경에서 위험을 정의할 필요성을 강조하여 책임성, 투명성 및 LLM을 실제 응용에 적응할 때 편향을 탐색하는 것을 보장합니다. 또한 특정 분야에 대한 완화 전략과 향후 방향, 그리고 LLM의 책임 있는 개발 및 통합을 안내하는 동적 감사 시스템을 표준화된 제안으로 제시합니다.\nThis study addresses categories of harm surrounding Large Language Models (LLMs) in the field of artificial intelligence. It addresses five categories of harms addressed before, during, and after development of AI applications: pre-development, direct output, Misuse and Malicious Application, and downstream application. By underscoring the need to define risks of the current landscape to ensure accountability, transparency and navigating bias when adapting LLMs for practical applications. It proposes mitigation strategies and future directions for specific domains and a dynamic auditing system guiding responsible development and integration of LLMs in a standardized proposal.\n논문 링크\nhttps://arxiv.org/abs/2512.05929\n---\n⚠️광고⚠️: 🔥파이토치 한국 사용자 모임🇰🇷이 정리한 이 글이 유용하셨나요? 회원으로 가입하시면 주요 글들을 이메일💌로 보내드립니다! (기본은 Weekly지만 Daily로 변경도 가능합니다.)",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25413",
    "category": "AI"
  },
  {
    "title": "Show GN: 2026년도 소망을 부적(계획)으로 만들어보았습니다!",
    "content": "오늘 새벽 감성으로 밤새서 만들어봤습니다.\n2026년 소망을 작성하면, 그걸 이루기 위한 실행계획을 부적처럼 만들어주는 서비스입니다 :)\n2026년을 시작하기 앞서 많은 사람들이 '올해는 꼭 다이어트 성공해야지' 같은 생각을 하지만,\n사실 그게 실행되는 일이 없던 부분에서 착안하여...\n조금 더 잘게 나눠주면, 동기부여가 될 수 있지 않을까 하는 생각에 만들게 되었습니다.\n재미있게 사용해보세요 :)",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25412",
    "category": "튜토리얼"
  },
  {
    "title": "리치 히키의 \"고마워요 AI!\": 인공지능이 만드는 거대한 쓰레기통",
    "content": "[에세이] 리치 히키의 \"고마워요 AI!\": 인공지능이 만드는 거대한 쓰레기통\n(원문: https://gist.github.com/richhickey/ea94e3741ff0a4e3af55b9fe6287887f)\nClojure의 창시자 리치 히키(Rich Hickey)가 생성형 AI 트렌드와 이를 주도하는 기업들을 향해 날카로운 일침을 가했습니다. AI가 보낸 아첨 메일에 대한 답장 형식으로 작성된 이 글은, AI 기술이 현대 사회와 기술 생태계에 미치는 파괴적인 영향력을 비판적으로 다룹니다.\n\n---\n\n## 주요 내용\n- 창작물 약탈: 인류의 역사적인 창의적 결과물들을 무단으로 학습(Pirating)하고 소유권을 주장하는 행태 비판.\n- 교육과 고용의 파괴: 교육 체계를 무너뜨리고, 주니어 개발자의 입문용 일자리(Entry-level jobs)를 없애 미래의 숙련된 인재 양성 경로를 차단함.\n- 엔지니어링 리소스 낭비: 유용한 결과를 얻기 위해 'BS 생성기'와 씨름하느라 정작 동료나 인턴과의 소통 및 교육에 써야 할 시간을 낭비함.\n- 환경 및 비용 문제: 막대한 전력 소모로 인한 유틸리티 비용 상승 및 환경 파괴 지적.\n- 인터넷의 슬롭(Slop)화: 검색 결과를 요약된 가짜 정보로 대체하고, 인터넷을 무의미한 데이터로 채워 인간의 진정한 콘텐츠를 찾기 어렵게 만듦.\n- 기업의 단기적 선택: 극소량의 비용 절감을 위해 제품의 품질, 무결성, 고객 만족도를 희생시키는 CEO들의 태도 비판.\n\n---\n\n## 시사점\n저자는 AI가 인간의 통신 채널을 가짜 정보로 가득 채워 소통의 신뢰를 무너뜨리고 있다고 경고합니다. 특히 \"해결하는 문제보다 더 많은 문제를 만드는 기술을 왜 실패라고 부르지 않는가?\"라는 질문은 기술 지상주의에 빠진 현재의 IT 업계에 묵직한 메시지를 던집니다.",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25411",
    "category": "AI"
  },
  {
    "title": "타다 리스트를 1년 동안 유지하며 느낀 점 (One year of keeping a tada list)",
    "content": "타다 리스트(Tada List)란?\n- 투두 리스트(To-do List)의 반대 개념\n- 매일 완료한 일(To-done)을 기록하며 \"타다!\"처럼 성취를 축하하는 도구\n실행 방법\n- 개인 노트에 매달 전용 페이지 생성\n- 매일 그날 이룬 성과를 간단히 나열\n- 월말에 해당 달 활동을 상징하는 일러스트 헤더로 장식\n1년 유지 경험에서 얻은 이점\n- 과거 노력과 현재 성과 연결 → 숨겨진 과정 재발견 (예: 수채화 카드 완성 뒤의 장기 준비 과정)\n- 학습 과정 명확히 인지 → 새로운 기술 습득 시 \"이제 내가 할 수 있게 됐다\"는 실감\n- 잊힌 성과 재발견 → 나중에 리스트 보면서 잊었던 노력 떠올리며 즐거움\n단점 및 어려움\n- 매일 기록해야 한다는 압박감 → 스트레스 유발\n- 장기 유지 시 번아웃 발생 → 후반부 필체 엉망, 월말 일러스트 중단\n결론\n- 단순 기록 이상의 가치: 잊힌 노력과 성취를 연결해 삶을 아름다운 이야기로 만들어주는 아티팩트\n- 생산성 도구가 아닌 축하·반성 도구로 추천\n- 다음 해 계속할지 고민 중 (지속의 어려움 솔직히 인정)",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25410",
    "category": "AI"
  },
  {
    "title": "창업자가 집중해야 할 '회사를 만드는 초석'들",
    "content": "- 창업 초기부터 성장 단계까지 반복적으로 마주치는 공동창업자 선택, 역할 설계, 팀 구성, 피드백 수집, 문화 구축의 중요성에 대한 정리\n- 공동창업자 관계는 개인적 관계처럼 충분한 시간을 들여 검증해야 하며, 비슷한 사람이 아닌 상호보완적 스킬셋을 갖춘 파트너를 찾아야 함\n- 아이디어보다 먼저 사람과 관계를 검증하는 창업 방식이 장기적인 회사 생존 가능성을 높이며, 약점을 보완하려 애쓰기보다 강점을 극대화하는 역할 분리가 지속 가능성을 높였음\n- \n급진적 투명성이 리스크보다 더 큰 가치를 제공하며, 직원들이 더 나은 의사결정을 내릴 수 있는 맥락을 제공함\n- 회사의 문화는 선언이 아니라 의사결정을 돕는 실질적 제품으로 취급해야 하며, 초기부터 문서화하고 의도적 설계와 반복적 피드백을 통해 지속적으로 개선해야 함\n---\n공동창업자 선택: 전략적 접근으로 장기적 파트너십 확보\n- 많은 창업팀이 아이디어만 평가하고 맥주 한 잔 마시는 정도로 공동창업자를 결정하지만, 함께 시간 보내는 것을 진정으로 즐기는지 깊이 검토해야 함\n- HubSpot 공동창업자 Shah와 Halligan은 MIT 재학 중 모든 프로젝트에서 의도적으로 같은 팀을 구성하여 8시간씩 함께 일하는 것이 어떤지 시뮬레이션\n개인적 관계에서는 결혼전에 오랜 기간을 데이트하지만, 공동창업자 관계에서는 그만큼 신중하지 않은 경우가 많음\n- 온라인 수업이나 단기 프로젝트를 통해 서로의 작동 방식을 이해하는 것을 권장\n- \n벤 다이어그램을 찾아라, 겹치는 원 두 개가 아니라\n자신과 가장 비슷한 사람에게 끌리지만, 서로 다른 스킬셋을 갖추는 것이 중요\n- 공통된 가치와 업무 패턴은 핵심 요소:\n학습에 대한 열정: 새벽 1-2시에 책 추천 이메일을 주고받으며, 서로의 독서 목록을 파악\n- \n건강한 논쟁의 힘: 동의하지 않는 입장도 논쟁할 수 있어야 함\n- \n글로 기록하기: Shah는 전화를 거의 받지 않고 모든 커뮤니케이션을 비동기식 이메일로 처리. 15년치 기록이 남아 새로운 사람에게 맥락 공유 가능\n- \n이 질문들에 답하지 않고 회사를 시작하지 말 것\n많은 공동창업자들이 근본적인 질문을 논의하지 않음: 양측이 스타트업에서 무엇을 원하는가?\n- Shah와 Halligan은 둘 다 이전 성공 경험이 있었고, 미래 자녀와 손주가 자랑스러워할 지속 가능한 것을 만들기로 결정\n모든 갈림길에서 \"어떤 경로가 큰 회사를 만들 확률을 최대화하는가?\"라는 질문으로 귀결\n- 자본 조달 시 희석보다 세대적 기업 구축 확률에 집중\n- 최적의 조건 협상보다 훌륭한 이사회 구축에 집중\n- 인수 제안에 흔들리지 않고 상장을 주저하지 않음\n- \n필수로 논의되어야 할 질문들\n당신에게 성공이란 무엇인가?\n- 1년 후 누군가 **1억** 달러에 회사를 인수하겠다고 하면?\n- 3년 후 한 사람이 회사를 떠나고 싶어하면?\n- 근본적으로 의견이 다르면 어떻게 하는가?\n- \"그때 가서 생각하자\" 접근은 위험하며, 초기에 이런 대화가 불편하다면 그것 자체가 위험 신호\n- 공동창업자 확보에서 영업 요소가 있지만, 너무 일방적이면 안 됨. 아이디어나 미션에 대한 열정이 상호적이어야 관계 지속\n역할 파악: 약점 보완보다 강점에 집중\n- Shah와 Halligan의 역할 분담은 쉬웠음: Halligan은 CEO 경험을 원했고, Shah는 이전 두 스타트업에서 CEO로서 자신이 형편없었다고 인정\n- Shah는 한 발 더 나아가 직속 보고자를 두지 않기로 결정\n합리적으로 똑똑한 사람이 3-5년간 관리 역량 개발에 투자해도 그저 \"괜찮은\" 수준에 도달하는 것은 본인과 회사 모두에게 큰 이득이 아님\n- 오늘날까지 HubSpot의 수천 명 직원 중 누구도 Shah에게 보고하지 않음\n강점에 집중하기로 결정했기에 15년간 회사에 남을 수 있었음\n- 창업자는 조직도상 관리 책임 없이도 특별한 영향력을 유지\n의견이 관리 책임과 무관하게 무게를 가짐\n- 이 교훈을 회사 전체에 적용: 어떤 사람은 훌륭한 관리자, 어떤 사람은 팀 빌딩에 능함, 어떤 사람은 제품 구축에 능함\n승진을 위해 관리직을 강요하지 말 것 — 조직도상 보고 인원 수가 가치의 함수라는 통념에서 벗어나야 함\n초기 팀 채용: MBA를 완전히 배제하지 말되, 과도하게 의존하지도 말 것\n- HubSpot의 첫 6명은 MIT Sloan 동문으로 회사가 너무 동질적이 되었고, 다시 한다면 이 방식을 택하지 않을 것\n- 그러나 같은 수업, 케이스, 책을 통해 공유 어휘를 갖추고 학문적 지향을 HubSpot에 도입한 장점도 있음\n- MBA 채용 미신 해체: 스타트업에서 \"MBA 수에 비례해 성공 확률이 기하급수적으로 떨어진다\"는 농담이 있지만 Shah는 동의하지 않음\nMBA는 매우 분석적이며 문제의 메커니즘을 이해하고 솔루션을 명확히 표현하는 능력 보유\n- 가격 책정, GTM, 기술 플랫폼 트레이드오프 분석에 유용\n- 훌륭한 MBA는 비즈니스에 적용된 엔지니어 마인드셋 보유\n- 대부분의 성공한 대기업은 고유 IP 때문이 아니라 비즈니스 모델과 GTM 때문에 성공\n고객 확보, 제품 주도 성장 방법은 코드 라인이 아님\n- \"보도자료용 채용\" 피할 것: 7년간 성공한 회사 X에서 일했다는 이유로 채용하지 말고, 일을 실행하고 이론을 테스트하려는 성향을 가진 사람을 찾을 것\n창업자로서 스케일링: 자체 연간 평가 시스템 구축\n- 상사가 없는 공동창업자로서 피드백을 얻으려면 전통적 성과 평가 외의 방법 필요\n- HubSpot 초기부터 Shah와 Halligan은 서로의 연간 평가를 작성\n- 프로세스:\n평가 작성자가 15-20명 선정하여 피드백 수집\n- 모든 피드백을 종합하고 자신의 피드백을 추가해 10-15페이지 평가서 작성\n- Shah는 이를 버그 리포트처럼 취급: \"어떤 기능이 작동하는가? 어떤 기능을 수정하고 싶은가? 심각도 순위는?\"\n- 피드백은 개방형으로 유지하여 즉각적 역할 외의 유용한 피드백도 수집\n- 수신자는 서면으로 응답:\n피드백에서 들은 내용\n- 내년에 다룰 것들\n- \n다루지 않을 것들과 그 이유\n- 다루지 않기로 선택한 버그 예시: 사무실에서 모습이 보이지 않는다는 피드백에 대해 Shah는 극도의 내향형으로서 2시간의 대면 시간이 하루 종일 에너지를 소진시킨다고 설명\n- 다룬 버그 예시: 무엇을 작업하는지 모른다는 피드백에 대해 \"Dharmesh's Ponderings\" 내부 블로그 시리즈 시작\n직원들이 선택적으로 web3에 대한 생각이나 특정 결정 이유를 읽을 수 있음\n- 책임감을 위해 피드백과 해결 계획, 타임라인을 전 직원과 공유\n문화 정립: 제품처럼 취급\n- HubSpot은 문화를 직원들이 좋은 결정을 내릴 수 있도록 만드는 제품으로 간주하며, Shah가 Chief Product Officer 역할\n- Brian Halligan이 CEO 그룹에서 문화가 1순위라는 피드백을 받고 Shah에게 문화 담당을 맡김\n회사에서 가장 덜 사교적인 사람에게 문화를 맡긴 아이러니\n- \n엔지니어링 연습처럼 문화 진단하기\nShah는 기술 리더답게 엔지니어링 연습으로 접근: \"HubSpot 직원의 성공 확률을 수학적으로 근사하는 함수를 작성한다면 계수와 매개변수는 무엇인가?\"\n- 기존 직원에게 NPS 설문: 0-10점 척도로 HubSpot을 일터로 추천할 가능성, 그 점수를 준 이유\n- 결과를 \"Culture Code\" 라는 덱으로 요약, 이 이름은 오늘날까지 유지\n- 처음 16슬라이드에서 64슬라이드, 현재 128슬라이드로 확장\n이제 슬라이드를 추가할 때마다 하나를 제거\n- \nNetflix 문화 덱에서 영감: Shah는 많은 부분을 암기할 정도로 집착\n> \"100+ 슬라이드 문화 덱을 읽을 시간이 있다면 Netflix 덱을 읽어라. 두 개 읽을 시간이 있다면 Netflix 덱을 두 번 읽어라\"\n- 특정 문화적 가치를 모방하는 것이 아니라 문화 작동 방식을 표현하는 명확성을 배우는 것\n- 직속 보고자 없는 극도의 내향형이 문화를 이끄는 것이 적합한 이유:\n과학자가 메뚜기를 연구하듯 문화를 관찰\n- 보호할 팀이 없어 더 객관적인 관점 유지\n- \n시작을 위한 프롬프트\n어떤 종류의 사람들과 함께 일하고 싶은가?\n- 모두가 동의할 상투적 표현은 안 됨 (예: 똑똑한 사람을 채용하고 싶다)\n모두가 선택하지 않을 것을 골라야 함\n- HubSpot의 투명성 가치\n- \n겸손 — 자신감 부족과 연관짓는 사람도 있어 모두가 선택하지 않음\n- V1은 문화 문서, V2는 의사결정을 위한 간단한 휴리스틱\n판례법처럼: 문화 문서에서 다루지 않은 질문이나 결정에 부딪혔을 때, 이런 종류의 이슈는 이렇게 처리한다는 기록\n- \n회사 성장에 따른 DNA 유지\n문화가 제품이라면 제품은 정적이지 않음\n초기에 개발한 문화를 보존하는 것이 아니라 근본은 유지하되 반복 개선\n- 시장과 고객(직원)의 니즈 변화에 대응\n- 변경 예시 — 직위: HubSpot은 원래 공식 직위 없이 시작 (계층 반영 회피)\n고객 반발로 철회해야 했음\n- 세 가지 옵션: 직위 없이 계속, 클래식 직위(manager, director, VP), 자유 직위(Design Badass 등)\n- \n옵션 2 선택: 직원들이 HubSpot 외부 경력에서 승진 지표로 필요하고, 가족도 VP 승진을 이해함\n- 유지된 예시 — 좌석 추첨: 첫 사무실에 책상 4개, 창업자 2명만 있었음\n창업자 둘 다 창가 책상 선택\n- 첫 엔지니어 채용 시 미니 추첨으로 좌석 선택권 결정\n- 사무실이 커져도 추첨 시스템 유지, 분기별 좌석 교체로 조정\n- 이사회가 \"확장 안 될 것\"이라고 했지만 오늘날까지 유지\n- \n투명성을 처음부터 내재화하기\nHubSpot 문화의 또 다른 축: 투명성에 대한 약속\n- 대부분은 초기 단계에서도 투명성 리스크가 높다고 생각\n- HubSpot은 급진적으로 투명한 접근 채택: 은행 잔고, 소진율, 마지막 라운드 밸류에이션, 행사가 등 모든 것 공개\n- 많은 창업자가 투명성이 비생산적이거나 위험하다고 생각하지만 Shah는 두 가지 측면에서 잘못되었다고 봄:\n리스크가 생각보다 높지 않음: 투명한 문화가 자리잡으면 채용도 달라짐. 채용 과정에서 데이터를 기밀로 유지할 수 있는 사람인지 질문하고, 그 투명성을 받을 자격이 있는 사람만 채용\n- \n투명성의 가치가 리스크보다 높음: 투명성은 더 많은 사람이 더 나은 결정을 내릴 맥락 제공. 엔지니어링 팀이 AWS 비용을 알 필요 없다고 할 수 있지만, 마진과 주가 영향 맥락이 실제로 도움됨\n- 128장의 슬라이드 문화 덱으로 시작하지 않아도 되지만 무언가를 기록할 것\n냅킨에라도 적어야 함. 문화를 기록하지 않으면 아무도 그것이 무엇인지 모름\n- 삼투압으로 전달되는 메시지가 아니라 명확히 표현해야 함\n- 이 투자는 높은 레버리지를 가짐: 문화는 뛰어난 인재를 유치하고 그들이 훌륭한 일을 하도록 돕는 역할\n- \"다시 한다면 문화 작업을 더 일찍 시작할 것\"",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25409",
    "category": "AI"
  },
  {
    "title": "처리되지 않은 사진이 실제로 어떻게 보이는가",
    "content": "- 디지털 카메라의 RAW 센서 데이터가 어떤 형태로 기록되는지를 단계별로 보여주는 실험\n- 초기 데이터는 14비트 ADC 출력을 0–255 RGB로 단순 매핑한 회색조 형태로, 실제 밝기 범위가 제한됨\n- \nBayer 필터를 적용해 각 픽셀의 색 필터 정보를 반영하고, 인접 픽셀 평균을 통해 디모자이싱(demosaicing) 수행\n- \n감마 보정과 화이트 밸런스 조정을 거치며, 인간 시각과 디스플레이의 비선형 밝기 인식 차이를 보정\n- 최종적으로 카메라 내부 JPEG 처리와 비교해, “보정되지 않은 사진”이라는 개념이 사실상 존재하지 않음을 보여줌\n---\n\n## RAW 센서 데이터의 초기 상태\n- 카메라 센서가 기록한 원본 데이터는 14비트 ADC 값으로 구성되며, 이를 0–255 RGB로 단순 변환하면 회색조 이미지 형태\n실제 ADC 값의 범위는 약 2110~13600으로, 이 구간을 흑백 기준으로 설정해 밝기 재조정\n- 이 단계의 이미지는 색 정보가 거의 없으며, 센서가 빛의 세기만 측정하기 때문임\n색상 정보 복원 과정\n- 컬러 카메라 센서는 Bayer 필터 배열을 통해 각 픽셀마다 빨강·초록·파랑 중 하나의 색만 감지\n- 각 픽셀의 색 필터에 맞춰 색을 입히면 색감이 생기지만, 한 픽셀당 RGB 중 하나만 존재\n- 인접 픽셀의 값을 평균해 디모자이싱을 수행하면 전체 이미지에 색이 복원됨\n밝기와 감마 보정\n- 결과 이미지가 어둡게 보이는 이유는 모니터의 동적 범위 한계와 인간 시각의 비선형 밝기 인식 때문\n- 선형 데이터를 그대로 표시하면 어둡게 보이므로, 비선형 감마 곡선을 적용해 어두운 영역을 밝힘\n- 그러나 이 과정에서 녹색 채널 과다 현상이 발생, 이는 센서의 녹색 민감도와 Bayer 배열의 녹색 비중 때문\n화이트 밸런스 및 색상 보정\n- 각 색 채널을 일정 비율로 조정해 화이트 밸런스를 맞춤\n- 비선형 변환 이전 단계로 돌아가 녹색 채널을 낮춘 뒤 다시 감마 곡선을 적용\n- 이 과정을 거쳐 자연스러운 색감의 사진 완성\n카메라 JPEG 처리와의 비교\n- 동일한 RAW 데이터로부터 카메라가 생성한 내장 JPEG 이미지는 이미 여러 수학적 보정 과정을 거친 결과물\n- 대비나 화이트 밸런스를 편집 소프트웨어에서 조정하는 것은 카메라 내부 처리와 본질적으로 동일한 연산\n- “편집되지 않은 사진”이라는 개념은 실제로 존재하지 않으며, 모든 사진은 수학적 처리의 결과물임\n- 인간의 시각을 완벽히 재현하기 어렵고, 디스플레이 한계로 인해 수동 보정의 필요성이 항상 존재함",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25408",
    "category": "AI"
  },
  {
    "title": "AI를 비용 낭비 없이 운영하기 위한 실전 통합 모범 사례",
    "content": "- LLM과 AI 플랫폼을 프로덕션 환경에서 안정적으로 운영하기 위해, 변화에 대응하는 구조 중심 설계 방식이 필요함\n- 모델과 API 변경에 대비해 모든 AI 호출을 서비스로 분리하고, 2중 실행 기반 마이그레이션 패턴을 적용함\n- OpenAI의 Flex 서비스 티어를 활용하면 동일한 모델과 데이터 품질로 **50%** 비용 절감 가능\n- 반복되는 데이터는 프롬프트 앞부분에 배치해 캐시 토큰 활용 효율을 극대화하여 비용의 **10%**만 지불\n- 과도한 비용 발생을 막기 위해 Rate Limiting과 Circuit Breaker를 백엔드에 필수적으로 구축\n---\n\n## 변화에 대비한 AI 통합 전략\n- AI 모델과 API는 지속적으로 변경되며, 현재 작동하는 방식이 언제든 실패할 수 있음\n- 개별 도구나 모델을 따라가는 대신, 변화에 적응 가능한 시스템 설계가 핵심임\n- AI 활용의 목표는 최신 기술 추종이 아니라 안정적 운영과 비용 통제임\n마이그레이션 패턴 (The Migration Pattern)\n- 모든 AI API 호출을 서비스로 추출하여 연결, 프롬프트 구성, 특정 프롬프트를 내부적으로 처리\n- 이 서비스들은 영구 마이그레이션 가능 상태(permanent migratability) 로 운영되어 항상 최신 버전과 모델 또는 이전에 사용하던 버전을 사용 가능\n- GPT 4.1에서 GPT-5로 마이그레이션할 때 문제 발생 경험\nGPT 4.1용으로 완벽하게 만들어진 프롬프트가 GPT-5에서 JSON 키를 누락하는 등 신뢰성 저하\n- GPT-5는 단순 JSON 포맷 대신 구조화된 JSON 스키마(structured JSON schemas) 사용을 지향\n- 키와 가능한 값을 정의하고 프롬프트에서 채우는 방식 설명 필요\n- 마이그레이션 전략 구현\n특정 기간 동안 또는 테스트/스테이징 환경에서 구버전 프롬프트와 구모델, 신버전 프롬프트와 신모델을 동시 실행\n- 완전히 다른 데이터 구조와 API 호출도 가능(OpenAI는 chat-style API에서 response-style API로 전환 권장)\n- 양쪽 결과를 로깅하고 주요 차이가 있으면 시스템이 알려주고 diff를 보여줌\n- 이중 실행 서버는 2배 비용 발생하지만, 구모델과 신모델 동작 차이 및 프롬프트 차이가 품질·예측가능성·신뢰성에 미치는 영향 파악 가능\n- 순수 대화형이 아닌 백그라운드 분석, 데이터 처리, 의미 분석에 특히 유용\n- 신규 결과가 기대에 못 미치면 언제든 레거시 버전으로 롤백 가능\n- API 시스템은 언젠가 deprecated되므로 마이그레이션 준비 필수\n- JSON 데이터의 diff를 확인하면 프롬프트 재구성에 도움\nClaude Code나 OpenAI Codex를 활용해 양쪽 결과가 유사해질 때까지 프롬프트 조정 가능\n- 이 패턴은 외부 ML 모델과 통신하는 모든 서비스에 적용\n- 신규 서비스가 갑자기 성능 저하를 보이면 스위치 전환으로 구버전으로 복귀하여 이전처럼 신뢰성 있는 데이터 획득 가능\n서비스 티어의 비밀 (The Service Tier Secret)\n- 클라우드 서비스들은 서비스 티어를 제공하며 API 호출의 중요도에 따라 다른 가격 적용\n- OpenAI의 경우\n기본 티어(default tier): 웹사이트에 표시된 가격\n- \n배치 요청(batched requests): 상당히 저렴하지만, 배치가 언제 채워지고 처리될지 알 수 없어 준동기 작업에는 부적합\n- \nFlex 티어: 기본 티어의 절반 가격\n**50%** 더 느릴 수 있고 특정 시점에 이용 불가할 수 있지만 동일한 모델과 데이터 품질 제공\n- Flex 티어 활용 사례\n백엔드 작업(게스트 추출, 발언 내용 분석, 요약 등)에 적용\n- OpenAI SDK의 auto-retries 기능으로 별도 로직 불필요\n- \n429 에러 발생 시 폴백 구현: Flex로 몇 번 시도 후 실패하면 standard 티어로 전환하여 재시도\n- 대규모 적용 결과\n즉각적인 **50%** 비용 절감 달성(Flex 티어가 대부분의 시간에 가용)\n- 입력 토큰이 많고 출력 토큰이 적은 경우(대량의 팟캐스트 트랜스크립트 → 소량의 JSON 요약 데이터) 캐시 토큰도 절반 가격\n- 추출 및 추론 작업량을 2배로 증가 가능, 플랫폼 품질 향상 및 전환율 증가로 연결\n- 플랫폼별 확인 필요 사항\nBatch 가격은 Flex 처리와 동일 비용\n- \nFlex 가격은 GPT-5, 5.1, o3, o4 모델에 존재\n- Codex, Pro, GPT-4o, 실시간 오디오 도구 등은 Flex 가격이 쉽게 이용 불가할 수 있음\n- 가격 티어가 존재하는데 사용하지 않으면 재정적 태만(financial negligence)\n- 혼잡 시에도 빠른 결과가 필요하면 Priority 티어(2배 가격, 더 빠른 결과) 설정 가능\nPriority도 특정 모델에는 존재하지 않을 수 있음\n- 모델과 사용 방법이 다양하므로 구현과 최적화에 유연성 필요\n캐시 효율을 위한 프론트로딩 (Front-Loading for Cache Efficiency)\n- 분석할 데이터가 많을 때 반복되는 부분을 앞에 배치\n- 시스템 프롬프트를 앞에 두고, 동일한 데이터를 여러 번 분석한다면 해당 데이터로 프롬프트 시작\n- 프롬프트 순서\n시스템 프롬프트(역할 설명)\n- 항상 동일한 시스템 지시사항(\"이 형식으로 데이터 추출\")\n- 프롬프트 간 중복될 수 있는 데이터\n- 각 프롬프트별 특정 지시사항\n- 앞에 배치된 데이터는 캐시 토큰으로 처리되어 다른 토큰 비용의 **10%**만 지불\n- 실제 적용 사례\n전체 트랜스크립트를 먼저 넣고, 트랜스크립트 끝에 특정 작업 지시(특정 게스트 찾기, 스폰서 찾기, 고객 질문 처리 등)\n- 여러 프롬프트 최적화 확인\nClaude Code나 ChatGPT 대화에 프롬프트들을 데이터로 넣고 최적화 분석 지시\n- AI의 결과를 그대로 받아들이지 말고 참고용으로 활용(AI는 토큰 예측일 뿐, 더 유용하다고 말한다고 실제로 그런 것은 아님)\n- 여러 프롬프트를 동시에 분석하면 의미 있는 인사이트 획득 가능\nRate Limiting과 Circuit Breakers\n- 토큰 기반 비용이 드는 외부 플랫폼 사용 시 Rate Limiting 필수\n- Rate Limit 적용 대상\nAI 상호작용을 트리거하는 고객 대면 액션\n- 백엔드 서버에서 전송할 수 있는 AI 상호작용\n- \nRace condition으로 같은 프로세스가 반복 재시작되어 같은 호출을 반복 트리거하는 상황 방지 필요(캐시 토큰 사용해도 비용 발생)\n- 이상 징후 감지\n특정 시간대에 정상의 10배 AI 토큰 사용 시 알림 수신 및 중지 기능 필요\n- \nCircuit Breaker 구현\n애플리케이션의 모든 AI 기능 또는 특정 부분에 대한 전체 circuit breaker\n- Laravel 명령어나 관리자 인터페이스에서 on/off 전환 가능한 기능 토글\n- \"멋진 설정 생성\" 버튼 같은 셀프 온보딩 기능도 on/off 가능하게\n- 누군가 자동화하여 시간당 수백 달러 비용 발생시키면 토글로 즉시 차단\n- \n기능 토글은 프론트엔드가 아닌 백엔드에 구현(실제 발생 지점에서 제어)\n- AI 스캔 활용\n에이전틱 코딩 도구로 코드 스캔하여 AI 호출 악용 가능 지점 파악 및 기능 토글 추가 위치 확인\n- \n모든 AI 사용은 백엔드 시스템 경유 필수\n클라이언트 사이드에서 토큰을 사용해 AI 플랫폼 직접 호출 금지(원래도 좋지 않은 방식)\n- 백엔드를 통해야 기능 on/off와 높은 사용량 알림 수신 가능\n- 구현된 보안 계층\n모든 기능에 Rate limits\n- 프론트엔드 사용자 Rate limits\n- 백엔드 Rate limits\n- 기능 토글\n- 기능 악용 알림(계정별, 구독자 유형별, IP별)\n- 향후 도구와 프레임워크에 기본 기능으로 포함될 것으로 예상되지만, 현재는 직접 구현 필요\n핵심 교훈: 변화에 대비한 시스템 구축\n- AI 환경은 끊임없이 변화(모델, API, 가격 변경)하기 때문에 모든 것을 따라잡는 것은 불가능\n- AI 운영의 핵심은 최신 모델이 아니라 변화가 발생했을 때 적응할 수 있는 시스템 설계임\n- 필수 구성 요소 :\n마이그레이션 패턴\n- 서비스 티어 최적화\n- 프롬프트 캐싱\n- Rate limiting\n- Circuit breakers\n- 이것들은 nice-to-have가 아니라 프로덕션에서 AI를 운영하면서 손실을 방지하는 기반\n- AI를 프로덕션에 쓰는 순간부터, 비용과 안정성은 기술 문제가 아닌 구조 문제가 됨\n\"Build for Change\" 변화를 위한 기반을 구축하세요",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25407",
    "category": "AI"
  },
  {
    "title": "OpenRouter의 AI 현황 보고서 : 100조 토큰 실증 연구",
    "content": "- \n**100조** 토큰 이상 실제 LLM 사용 데이터를 분석한 대규모 연구로, 2024년 12월 o1 추론 모델 출시 이후 AI 추론 방식의 근본적 전환을 추적\n- \n오픈소스 모델이 전체 사용량의 약 **30%**까지 성장했으며, DeepSeek V3, Kimi K2 등 중국 오픈소스 모델이 빠르게 점유율 확대\n- \n롤플레이와 프로그래밍이 LLM 사용의 양대 축으로, 오픈소스 모델 사용량의 절반 이상이 롤플레이에 집중되어 생산성 중심 가정과 상반된 결과 도출\n- 추론 모델이 전체 토큰의 **50%** 이상을 처리하며 에이전틱 추론이 새로운 기본 패턴으로 부상, 도구 호출과 다단계 작업이 증가\n- 초기 사용자가 장기적으로 높은 유지율을 보이는 '유리 구두' 효과가 발견되어, 모델-워크로드 적합성이 핵심 경쟁력임을 시사\n---\n\n## 연구 개요 및 방법론\n- \nOpenRouter는 300개 이상의 모델과 60개 이상의 제공업체를 지원하는 멀티모델 AI 추론 플랫폼으로, 전 세계 수백만 개발자와 최종 사용자에게 서비스 제공\n- 분석 데이터셋은 약 2년간의 익명화된 요청 수준 메타데이터로 구성되며, 프롬프트나 완성 텍스트 자체에는 접근하지 않음\n- 모든 분석은 Hex 분석 플랫폼을 통해 재현 가능한 SQL 쿼리와 변환, 시각화 파이프라인으로 수행\n- 콘텐츠 분류는 전체 프롬프트의 약 **0.25%** 를 무작위 샘플링하여 GoogleTagClassifier를 통해 수행, 프로그래밍·롤플레이·번역·일반 Q&A·생산성/글쓰기·교육·문학/창작·성인 등 카테고리로 분류\n- 지역 분석은 청구 위치(billing location) 기반으로 사용자 지역 결정, IP 기반보다 안정적인 프록시로 활용\n- 분석 기간은 주로 2024년 11월~2025년 11월의 13개월이며, 카테고리 분류 분석은 2025년 5월 이후 데이터에 기반\n오픈소스 vs 폐쇄형 모델\n- \n오픈소스(OSS) 모델은 가중치가 공개된 모델, 폐쇄형 모델은 제한된 API로만 접근 가능한 모델(예: Anthropic Claude)로 정의\n- 오픈소스 모델 점유율이 꾸준히 증가하여 2025년 말 기준 약 **30%** 에 도달, 이는 DeepSeek V3, Kimi K2 등 주요 오픈소스 모델 출시와 연동\n- \n중국 개발 모델이 2024년 말 주간 점유율 1.**2%**에서 일부 주간 약 **30%**까지 급성장, 연간 평균 약 13.**0%** 기록\nQwen, DeepSeek 등이 빠른 반복 출시와 밀집된 릴리스 주기로 성장 주도\n- 폐쇄형 모델은 여전히 신뢰성과 성능 상한선을 정의하며 규제 또는 기업 워크로드에서 우위\n- OSS 모델은 비용 효율성, 투명성, 커스터마이징 측면에서 매력적이며, 현재 약 **30%**에서 균형점 형성\n- 두 모델 유형은 상호 배타적이지 않고 멀티모델 스택 내에서 상호 보완적으로 활용\n- \n주요 오픈소스 플레이어\nDeepSeek이 총 **14.37조** 토큰으로 OSS 중 가장 큰 기여자이나, 새로운 진입자들이 빠르게 점유율 확보\n- Qwen(**5.59조**), Meta LLaMA(**3.96조**), Mistral AI(**2.92조**) 순으로 상위 랭크\n- 2025년 중반 Summer Inflection 이후 시장 구조가 거의 독점에서 다원화로 전환\nMoonshotAI의 Kimi K2, OpenAI의 GPT-OSS 시리즈, MiniMax M2 등이 수주 내 프로덕션급 채택 달성\n- 2025년 말 기준 단일 모델이 OSS 토큰의 **25%**를 초과하지 않으며, 5~7개 모델에 점유율 분산\n- OSS 생태계는 혁신 주기가 빠르고 리더십이 보장되지 않는 고도로 역동적인 경쟁 환경\n- \n모델 크기 vs 시장 적합성: 중형이 새로운 소형\n모델 크기 분류: 소형(15B 미만), 중형(15B~70B), 대형(70B 이상)\n- 소형 모델은 전체적으로 점유율 하락 추세, 새로운 모델 공급에도 불구하고 사용량 감소\n- \n중형 모델은 2024년 11월 Qwen2.5 Coder 32B 출시로 본격적으로 카테고리 형성\nMistral Small 3(2025년 1월), GPT-OSS 20B(2025년 8월) 등이 강력한 경쟁자로 부상\n- 사용자들이 역량과 효율성의 균형을 추구하고 있음을 시사\n- 대형 모델 세그먼트는 Qwen3 235B A22B Instruct, Z.AI GLM 4.5 Air, OpenAI GPT-OSS-120B 등 다양한 고성능 경쟁자로 다원화\n- 소형 모델 지배 시대는 종료되고 시장은 중형 모델과 대형 모델로 양분화 추세\n- \n오픈소스 모델의 용도\nOSS 모델의 가장 큰 용도는 롤플레이(약 **52%**)와 프로그래밍으로, 두 카테고리가 OSS 토큰 사용량의 대부분 차지\n- 롤플레이가 **50%** 이상을 차지하는 것은 오픈 모델이 콘텐츠 필터가 덜 제약적이어서 판타지나 엔터테인먼트 애플리케이션에 매력적임을 반영\n- \n중국 OSS 모델의 경우 롤플레이가 약 **33%**로 가장 크지만, 프로그래밍과 기술이 합쳐서 **39%**로 과반 차지\nQwen, DeepSeek 등이 코드 생성과 인프라 관련 워크로드에 점점 더 많이 사용\n- 프로그래밍 카테고리에서 OSS 내 점유율이 중국 OSS와 서양 OSS 간에 역동적으로 변화\n2025년 중반에는 중국 OSS가 주도했으나, Q4에는 Meta LLaMA-2 Code, OpenAI GPT-OSS 시리즈 등 서양 OSS가 급증\n- 롤플레이 트래픽은 2025년 말 기준 RoW OSS(**43%**)와 폐쇄형(**42%**) 이 거의 균등하게 분담, 초기 **70%** 폐쇄형 지배에서 크게 변화\n에이전틱 추론의 부상\n- \n추론 모델이 전체 사용량의 절반 이상 차지\n추론 최적화 모델을 통한 토큰 점유율이 2025년 초 미미한 수준에서 **50%** 초과로 급증\n- GPT-5, Claude 4.5, Gemini 3 등 고성능 시스템 출시와 다단계 논리, 에이전트 스타일 워크플로우에 대한 사용자 선호가 이 변화 주도\n- 최근 데이터 기준 xAI Grok Code Fast 1이 추론 트래픽 최대 점유율, Google Gemini 2.5 Pro/Flash가 뒤를 이음\n- 오픈 모델인 OpenAI gpt-oss-120b도 상당한 점유율 유지, 개발자들이 가능할 때 OSS 선호\n- \n도구 호출 채택 증가\nTool Call 종료 이유로 분류된 요청의 토큰 점유율이 지속적으로 상승 추세\n- 도구 호출은 처음에 OpenAI gpt-4o-mini, Anthropic Claude 3.5/3.7 시리즈에 집중되었으나, 2025년 중반 이후 더 많은 모델이 도구 제공 지원\n- 2025년 9월 말 이후 Claude 4.5 Sonnet이 빠르게 점유율 확대, Grok Code Fast, GLM 4.5도 진출\n- \n프롬프트-완성 형태의 변화\n평균 프롬프트 토큰이 약 1.5K에서 6K 이상으로 약 4배 증가\n- 평균 완성 토큰도 약 150에서 400으로 거의 3배 증가, 주로 추론 토큰 증가에 기인\n- 프로그래밍 관련 작업이 프롬프트 토큰 증가의 주요 동력으로, 20K 이상의 입력 토큰을 자주 사용\n- 다른 카테고리들은 상대적으로 평탄하고 낮은 볼륨 유지\n- \n더 긴 시퀀스, 더 복잡한 상호작용\n평균 시퀀스 길이가 지난 20개월 동안 2,000 토큰 미만에서 5,400 토큰 이상으로 3배 이상 증가\n- 프로그래밍 관련 프롬프트가 일반 목적 프롬프트보다 평균 3~4배 긴 토큰 길이 기록\n- 긴 시퀀스는 사용자의 장황함이 아니라 내장된 정교한 에이전틱 워크플로우의 특징\n- \n함의: 에이전틱 추론이 새로운 기본값\n추론 점유율 증가, 도구 사용 확대, 시퀀스 연장, 프로그래밍의 복잡성 증가 등이 LLM 사용의 중심축 이동을 시사\n- 중간 LLM 요청은 더 이상 단순한 질문이나 고립된 지시가 아니라 구조화된 에이전트 유사 루프의 일부\n- 모델 제공업체는 지연 시간, 도구 처리, 컨텍스트 지원, 악의적 도구 체인에 대한 견고성이 점점 중요\n- \n곧, 아직 아니라면, 에이전틱 추론이 추론의 대부분을 차지할 전망\n카테고리: 사람들이 LLM을 어떻게 사용하는가?\n- \n지배적 카테고리\n프로그래밍이 가장 일관되게 확장되는 카테고리로, 2025년 초 약 **11%**에서 최근 **50%** 초과\n- Anthropic Claude 시리즈가 프로그래밍 관련 지출의 **60%** 이상 지속적으로 장악\n11월 17일 주간에 처음으로 **60%** 이하로 하락\n- OpenAI는 7월 이후 약 **2%**에서 **8%**로 점유율 확대, Google은 약 **15%**로 안정 유지\n- MiniMax가 빠르게 부상하는 진입자로 주목\n- \n카테고리 내 태그 구성\n롤플레이: 약 **60%**가 Games/Roleplaying Games, 캐주얼 챗봇보다 구조화된 롤플레이 또는 캐릭터 엔진으로 활용\nWriters Resources(15.**6%**), Adult 콘텐츠(15.**4%**)도 포함\n- \n프로그래밍: 2/3 이상이 _Programming/Other_로 라벨링, 광범위한 범용 코드 관련 프롬프트 특성\nDevelopment Tools(26.**4%**)와 스크립팅 언어의 작은 점유율로 신흥 전문화 징후\n- \n번역, 과학, 건강 등은 상대적으로 평탄한 내부 구조\n번역: Foreign Language Resources(51.**1%**)와 _Other_로 거의 균등 분할\n- 과학: Machine Learning & AI(**80.4%**)가 지배, 대부분 메타 AI 질문\n- 건강: 가장 세분화된 카테고리로 단일 하위 태그가 **25%** 초과하지 않음\n- \n금융, 학술, 법률은 훨씬 분산되어 있어 단일 태그가 **20%** 미만\n- \n제공업체별 인사이트\nAnthropic Claude: 프로그래밍+기술 사용이 **80%** 초과, 롤플레이와 일반 Q&A는 소량\n- \nGoogle: 번역, 과학, 기술, 일반 지식 등 다양한 구성, 코딩 점유율은 2025년 말 약 **18%**로 하락\n- \nxAI: 대부분 기간 동안 프로그래밍이 **80%** 초과, 11월 말에만 기술, 롤플레이, 학술 등으로 확대\n무료 배포로 인한 비개발자 트래픽 유입과 연관\n- \nOpenAI: 2025년 초 과학 작업이 절반 이상이었으나 말에는 **15%** 미만으로 감소\n프로그래밍과 기술 관련 사용이 각각 **29%**로 절반 이상 차지\n- \nDeepSeek: 롤플레이, 캐주얼 채팅, 엔터테인먼트 지향 상호작용이 2/3 이상 지배\n- \nQwen: 프로그래밍이 전체 기간 동안 40~**60%** 일관 유지, 과학, 기술, 롤플레이 등에서 주간 변동성 높음\n지역: LLM 사용이 지역별로 어떻게 다른가\n- \n지역별 사용 분포\n북미가 단일 최대 지역이나 관찰 기간 대부분에서 총 지출의 절반 미만\n- \n유럽은 주간 지출 점유율이 10~**20%**대에서 안정적으로 유지\n- \n아시아가 프론티어 모델 생산자뿐 아니라 빠르게 확장하는 소비자로 부상\n데이터셋 초기 약 **13%**에서 최근 약 **31%** 로 점유율 2배 이상 증가\n- 대륙별 분포: 북미 **47.22%**, 아시아 **28.61%**, 유럽 **21.32%**, 오세아니아 **1.18%**, 남미 **1.21%**, 아프리카 **0.46%**\n- 상위 10개국: 미국(**47.17%**), 싱가포르(**9.21%**), 독일(**7.51%**), 중국(**6.01%**), 한국(**2.88%**), 네덜란드(**2.65%**), 영국(**2.52%**), 캐나다(**1.90%**), 일본(**1.77%**), 인도(**1.62%**)\n- \n언어 분포\n영어가 82.**87%**로 지배적\n- 중국어 간체(**4.95%**), 러시아어(**2.47%**), 스페인어(**1.43%**), 태국어(**1.03%**), 기타(**7.25%**)\nLLM 사용자 유지율 분석\n- \n신데렐라 '유리 구두' 현상\n대부분의 리텐션 차트가 높은 이탈과 빠른 코호트 감소로 지배되나, 초기 사용자 코호트가 시간이 지나도 내구성 있는 유지율 보임\n- 이러한 기초 코호트(foundational cohorts) 는 워크로드가 깊고 지속적인 워크로드-모델 적합성을 달성한 사용자들을 대표\n- \n유리 구두 효과: 급변하는 AI 생태계에서 각 새 프론티어 모델이 이전에 미충족된 고가치 워크로드에 \"시험되고\", 기술적·경제적 제약에 정확히 맞을 때 강력한 락인 효과 발생\n- \nGemini 2.5 Pro의 2025년 6월 코호트와 Claude 4 Sonnet의 5월 코호트가 5개월 차에 약 **40%** 유지율로 후속 코호트보다 현저히 높음\n- \nGPT-4o Mini: 단일 기초 코호트(2024년 7월)가 출시 시 지배적이고 끈끈한 워크로드-모델 적합성 확립, 이후 모든 코호트는 동일하게 이탈\n- \nGemini 2.0 Flash, Llama 4 Maverick: 높은 성과의 기초 코호트가 형성되지 않아 모든 코호트가 동일하게 저조, \"프론티어\"로 인식되지 못함\n- \nDeepSeek 모델들의 부메랑 효과: 일반적인 단조 감소 대신 부활 점프 현상 관찰\nDeepSeek R1의 2025년 4월 코호트가 3개월 차에, DeepSeek Chat V3-0324의 7월 코호트가 2개월 차에 유지율 상승\n- 대안을 시도한 후 돌아오는 사용자들을 나타냄\n- \n함의\n첫 번째로 문제를 해결하는 것이 지속적 우위로 작용\n- 코호트 수준 유지율 패턴이 모델 차별화의 경험적 신호\n- 프론티어 윈도우의 시간적 제약: 모델이 기초 사용자를 확보할 수 있는 창은 좁고 일시적이나 장기 채택 역학에 결정적\n- 기초 코호트는 실질적 기술 진보의 지문이자 AI 모델이 신기함에서 필수품으로 전환한 지점\n비용 vs 사용 역학\n- \n카테고리별 AI 워크로드 세분화 분석\n중간 비용 **$0.73**/1M 토큰을 기준으로 4사분면 프레임워크 구성\n- \n프리미엄 워크로드(우상단): 고비용-고사용 애플리케이션, technology와 science 포함\ntechnology가 가장 비싸면서도 높은 사용량 유지, 복잡한 시스템 설계나 아키텍처에 강력한 모델 필요 시사\n- \n대중 시장 볼륨 드라이버(좌상단): 고사용-저비용, roleplay, programming, science 지배\nprogramming이 \"킬러 프로페셔널\" 카테고리로 최고 사용량에 고도로 최적화된 중간 비용\n- \nroleplay의 사용량이 programming에 버금가는 수준으로 소비자 지향 롤플레이가 최상위 전문적 용도와 동등한 참여 유도\n- \n전문 전문가(우하단): 저볼륨-고비용, finance, academia, health, marketing 포함\n고위험 니치 전문 도메인으로 정확성, 신뢰성, 도메인 특화 지식에 대한 수요 높음\n- \n니치 유틸리티(좌하단): 저비용-저볼륨, translation, legal, trivia 포함\n기능적이고 비용 최적화된 유틸리티, 상품화되어 저렴한 대안 이용 가능\n- \nAI 모델의 유효 비용 vs 사용\n로그-로그 스케일에서 가격과 사용량 간 상관관계가 약함, 추세선이 거의 평탄\n- \n수요가 상대적으로 가격 비탄력적: 가격 **10%** 하락 시 사용량 약 0.5~0.**7%** 증가\n- 두 개의 뚜렷한 체제: 폐쇄형 모델(OpenAI, Anthropic)이 고비용-고사용 구역, 오픈 모델(DeepSeek, Mistral, Qwen)이 저비용-고볼륨 구역\n- \n\n---\n\n## 4가지 사용-비용 아키타입\n프리미엄 리더: Claude 3.7 Sonnet, Claude Sonnet 4 등 약 **$2**/1M 토큰으로 높은 사용량 달성\n- \n효율적 거인: Gemini 2.0 Flash, DeepSeek V3 0324 등 **$0.40**/1M 토큰 미만으로 유사한 사용량\n- \n롱테일: Qwen 2 7B Instruct, IBM Granite 4.0 Micro 등 수 센트/1M 토큰이나 약한 성능이나 제한된 가시성으로 낮은 사용량\n- \n프리미엄 전문가: GPT-4, GPT-5 Pro 등 약 **$35**/1M 토큰으로 저사용량, 고위험 워크로드에 한정 사용\n- \nJevons 역설의 증거: 매우 저렴하고 빠른 모델이 더 많은 작업에 사용되어 총 토큰 소비 증가\n- 품질과 역량이 종종 비용을 능가: 비싼 모델(Claude, GPT-4)의 높은 사용량은 모델이 현저히 우수하거나 신뢰 우위가 있으면 사용자들이 높은 비용 감수\n논의\n- \n멀티모델 생태계: 단일 모델이 모든 사용을 지배하지 않으며, 폐쇄형과 오픈 모델 모두 상당한 점유율 확보\n- \n생산성 이상의 사용 다양성: 오픈소스 모델 사용량의 절반 이상이 롤플레이와 스토리텔링\n소비자 지향 애플리케이션, 개인화, AI와 엔터테인먼트 IP 간 크로스오버에 대한 기회 부각\n- \n에이전트 vs 인간: 에이전틱 추론의 부상: 단일 턴 상호작용에서 에이전틱 추론으로 전환, 모델이 계획하고 추론하며 여러 단계에 걸쳐 실행\n- \n지역적 전망: LLM 사용이 점점 글로벌하고 분산화, 아시아 점유율이 **13%**에서 **31%**로 상승, 중국이 주요 세력으로 부상\n- \n비용 vs 사용 역학: LLM 시장은 아직 상품이 아님, 가격만으로는 사용량 설명 부족\n오픈소스 모델이 지속적으로 효율적 프론티어 밀어붙이며 폐쇄형 시스템의 가격 결정력 압축\n- \n유지율과 신데렐라 유리 구두 현상: 기초 모델이 도약할 때 유지율이 방어가능성의 진정한 척도\n모델-워크로드 적합성이 핵심 경쟁력\n한계\n- 단일 플랫폼(OpenRouter)의 유한한 시간 창에서 관찰된 패턴으로 더 넓은 생태계의 부분적 시야만 제공\n- 기업 사용, 로컬 호스팅 배포, 폐쇄된 내부 시스템 등은 데이터 범위 외\n- 일부 분석이 프록시 측정에 의존: 다단계 또는 도구 호출로 에이전틱 추론 식별, 청구 기반 지역 추론 등\n- 결과는 결정적 측정보다는 지시적 행동 패턴으로 해석 필요\n결론\n- LLM이 세계 컴퓨팅 인프라에 통합되는 방식에 대한 실증적 시각 제공\n- 지난 해 o1급 모델 출현으로 추론에 대한 인식이 단계적 변화 촉발, 단일 샷 벤치마크를 넘어 프로세스 기반 지표, 지연-비용 트레이드오프, 오케스트레이션 하 성공으로 평가 전환\n- LLM 생태계는 구조적으로 다원적, 사용자가 역량, 지연, 가격, 신뢰 등 여러 축에 따라 시스템 선택\n- 추론 자체도 변화 중: 정적 완성에서 동적 오케스트레이션으로, 에이전틱 추론 부상\n- 지역적으로 더 분산화, 아시아 점유율 확대, 중국이 모델 개발자이자 수출자로 부상\n- o1이 경쟁을 종료하지 않고 디자인 공간을 확장, 모놀리식 베팅 대신 시스템 사고, 직관 대신 계측, 리더보드 델타 대신 경험적 사용 분석으로 이동\n- 다음 단계는 운영 우수성에 집중: 실제 작업 완료 측정, 분포 변화 하 분산 감소, 프로덕션 규모 워크로드의 실제 요구에 모델 행동 정렬",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25405",
    "category": "AI"
  },
  {
    "title": "아버지의 선택이 정자 RNA에 포장되어 자녀에게 전달될 수 있음",
    "content": "- \n정자 RNA가 아버지의 식습관, 운동, 스트레스, 니코틴 사용 등 생활 요인을 반영해 자녀의 유전 발현에 영향을 미칠 수 있음이 쥐 실험에서 확인됨\n- 여러 연구에서 운동이나 식단 변화가 정자 내 RNA 조성을 바꾸고, 이 RNA가 수정란에 전달되어 대사 기능과 발달 과정을 조절함이 관찰됨\n- 특히 운동한 수컷 쥐의 정자에서 발견된 microRNA가 자손의 근육 내 미토콘드리아 활성과 지구력을 높이는 것으로 나타남\n- \n외부 자극이 체내 RNA 신호로 변환되어 정자에 포장되는 과정은 아직 완전히 규명되지 않았으며, 인간 대상 연구는 초기 단계에 머무름\n- 이러한 발견은 후성유전학적 유전 경로의 새로운 가능성을 제시하며, 생활습관이 다음 세대 건강에 영향을 줄 수 있음을 시사함\n---\n\n## 아버지의 경험이 유전에 반영되는 새로운 경로\n- 기존에는 정자가 단순히 DNA 운반체로만 여겨졌으나, 최근 연구는 정자 내 RNA 분자가 아버지의 신체 상태와 환경 정보를 자녀에게 전달할 수 있음을 보여줌\n쥐 실험에서 아버지의 식단, 운동, 스트레스 수준이 정자 RNA에 반영되어 자손의 대사 기능에 영향을 미침\n- 연구자들은 이러한 현상이 DNA 염기서열 변화 없이 유전되는 후성유전적 메커니즘임을 확인\n후성유전학적 경로와 정자 RNA의 역할\n- \nQi Chen, Colin Conine, Oliver Rando 등 연구자들은 정자 RNA가 비유전적 정보 전달자로 작용함을 입증\nChen은 2012년 쥐 정자에서 짧은 RNA 분자가 DNA와 함께 농축되어 존재함을 발견하고 이를 “sperm RNA code” 라 명명\n- 고지방 식이를 한 수컷 쥐의 정자 RNA를 정상 수정란에 주입하자, 자손에게 대사 이상이 나타남\n- Rando 연구팀은 정자 RNA가 부고환(epididymis) 내에서 epididymosome이라는 소포를 통해 전달됨을 확인\n이 기관이 외부 환경을 감지하고 RNA를 선택적으로 포장하는 역할을 할 가능성이 제시됨\n스트레스와 대사 변화의 세대 간 전달\n- \nIsabelle Mansuy 연구팀은 외상성 스트레스가 쥐의 혈액 내 세포외 소포(EV) 를 통해 정자에 전달됨을 확인\nEV는 RNA, 단백질, 지질 등을 운반하며, 이들이 정자 RNA 변형을 유도해 자손에게 스트레스 관련 대사 이상을 남김\n- 일부 대사 변화는 5세대에 걸쳐 지속됨이 관찰됨\n- 인간에서도 어린 시절 스트레스 경험자에게 유사한 대사 프로파일이 발견됨\n운동과 정자 microRNA의 상관관계\n- 2025년 발표된 Cell Metabolism 논문은 운동한 수컷 쥐의 정자에서 미토콘드리아 기능 관련 microRNA가 증가함을 보고\n이 microRNA를 수정란에 주입하자 자손의 근육 내 미토콘드리아 수와 지구력이 향상\n- 동일한 microRNA가 운동하는 인간 남성의 정자에서도 다수 검출됨\n- 연구진은 이를 운동 효과의 세대 간 전달 가능성으로 해석\n남은 과제와 연구 방향\n- 연구자들은 아직 경험이 어떻게 RNA 신호로 변환되어 정자에 저장되는지, 그리고 수정 후 어떤 경로로 발달을 조절하는지를 완전히 규명하지 못함\nMansuy는 “현재는 서로 다른 부분을 설명하는 맹인들처럼 전체 메커니즘을 조립 중”이라 표현\n- 인간에서 이를 입증하려면 다세대 추적 연구와 고도 분자 분석 기술이 필요\nChen은 이러한 연구가 이루어져야만 의학적 조언으로 발전할 수 있다고 언급\n- Rando는 “정자 RNA가 수정란의 초기 유전자 발현을 제어해 자손의 건강을 형성할 수 있다”고 설명하며, 이는 새로운 생명 현상의 발견 가능성으로 평가됨",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25404",
    "category": "트렌드"
  },
  {
    "title": "지난 1년의 Mac: 믿기 힘든 상태를 되돌아보다",
    "content": "- \nmacOS Tahoe의 새 인터페이스 Liquid Glass는 시각적 일관성을 내세웠지만 실제 사용성은 크게 저하됨\n- 창의 모서리 반경 확대로 콘텐츠가 잘리거나 여백이 낭비되고, 컨트롤 크기 증가도 기능적 이점 없이 혼란만 초래\n- \n앱 아이콘의 통일된 정사각형 규칙으로 Dock에서 구분이 어려워지고, 일부는 거의 식별 불가 수준으로 변함\n- \n밝은 모드의 과도한 백색화와 투명도 효과로 인터페이스 경계가 모호해지고, 접근성 기능인 ‘Reduce Transparency’도 제대로 작동하지 않음\n- 시각적 미학보다 가독성과 접근성의 후퇴가 두드러지며, 과거 macOS의 명확하고 기능적인 디자인이 그리워짐\n---\n\n## macOS Tahoe의 인터페이스 변화 개요\n- macOS Tahoe(26.x) 는 ‘Liquid Glass’라는 새로운 시각 효과를 도입했으나, 베타 피드백에도 불구하고 정식 버전(9월 15일 출시)에서 개선이 거의 이루어지지 않음\n이후 26.1(11월 3일)과 26.2에서도 문제점이 지속\n- \nTahoe의 전면적 디자인 개편이 잘못된 방향으로 진행되었다고 평가\n과도한 둥근 모서리 문제\n- 대부분의 콘텐츠(이미지, 비디오, 웹페이지, 텍스트)는 직사각형 영역에 최적화되어 있음\nSequoia의 완만한 모서리 둥글기는 괜찮았으나, Tahoe의 큰 반경은 부적합\n- 결과적으로 콘텐츠가 잘리거나 뷰 크기가 줄어들어 공간 낭비 발생\n- Finder의 Gallery 보기 썸네일은 원본을 잘못 표현하며, SwiftUI로 만든 동일 앱의 창 반경조차 일관되지 않음\n컨트롤 크기 확대의 부작용\n- Tahoe는 버튼 등 컨트롤의 크기를 키웠지만 명확성은 개선되지 않음\n예시 앱 Mallyshag에서 Sequoia에서는 정돈된 인터페이스가, Tahoe에서는 버튼이 겹치고 비율이 깨진 모습으로 변함\n- 텍스트 내용은 동일하지만 시각적 혼란만 증가\n아이콘의 획일화와 식별성 저하\n- 앱 아이콘은 색상·형태·내용을 통해 구분 가능해야 함\nTahoe는 모든 아이콘을 둥근 모서리의 정사각형 틀 안에 넣도록 강제\n- 그 결과 Dock에서 앱 간 구분이 어려워지고, 일부는 Apple Developer 앱과 App Store 아이콘이 거의 동일하게 보임\n- 여러 아이콘이 색 번짐처럼 흐릿한 형태로 변함\n색상 대비 상실과 ‘화이트아웃’ 현상\n- \nLight Mode는 과도하게 희게, Dark Mode는 지나치게 어둡게 표현되어 시각적 단서가 사라짐\n기본 투명도 설정에서는 일부 버튼과 체크박스는 구분되지만, 텍스트 입력 필드와 배경의 경계가 사라짐\n- 전체적으로 도구·컨트롤·콘텐츠의 구분이 어려움\nLiquid Glass의 투명도 문제\n- \nSystem Settings의 검색창 등에서 투명도가 지나쳐 내용이 겹쳐 보임\n스크롤 시 배경 콘텐츠가 비쳐 읽기 어려운 상태 발생\n- 선택된 리스트 항목의 색상이 창 제목과 ‘교통 신호등’ 버튼을 덮어버림\n- 창이 비활성화되면 요소들이 회색 블러로 사라져 인식 불가\n- \n접근성 설정의 ‘Reduce Transparency’ 기능이 더 이상 투명도를 줄이지 못함, 26.1 이후 수정되지 않음\n요약 및 결론\n- macOS Tahoe의 시각적 인터페이스는 다음과 같은 문제를 가짐\n직사각형 콘텐츠를 과도하게 둥근 창에 맞추려 함\n- \n컨트롤 확대에도 불구하고 기능적 이점 없음\n- \n아이콘의 획일화로 식별성과 기억 용이성 저하\n- \n톤 대비 부족으로 인터페이스 요소 구분이 어려움\n- \n투명도 중첩으로 시각적 혼란 및 접근성 저하\n- 과거 macOS(예: 2014년)의 명확하고 기능적인 인터페이스 품질이 그리워지며, 현재는 가독성과 사용성의 후퇴가 두드러짐",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25403",
    "category": "트렌드"
  },
  {
    "title": "Perfetto - 시스템 프로파일링, 앱 트레이싱 및 트레이스 분석 도구",
    "content": "- Google이 주도해 차세대 시스템 트레이싱 스택을 목표로 설계·개발한 오픈소스 플랫폼\n- 복잡한 시스템의 실행 흐름을 트레이싱 기반으로 관측하여 성능·기능 문제의 원인을 분석\n- \nAndroid 운영체제와 Chromium 브라우저의 기본 트레이싱 시스템으로 채택된 프로덕션급 안정성과 성능\n- \n고성능 트레이싱 데몬을 통해 여러 프로세스·스레드의 이벤트를 하나의 통합 트레이스로 수집\n- \n저오버헤드 C++17 SDK를 제공해 사용자 공간 애플리케이션의 타이밍·상태 변화를 정밀하게 계측\n- \nOS 수준 프로브를 통해 Android·Linux의 스케줄링, CPU 주파수, 메모리, 콜스택 등 시스템 전반의 컨텍스트 수집\n- \n브라우저 기반 UI로 대용량 트레이스를 타임라인 중심으로 시각화하고 인터랙티브하게 탐색\n별도 설치 없이 주요 브라우저에서 실행 가능하며, 다양한 외부 트레이스 포맷도 열람 지원\n- \nSQL 기반 분석 엔진을 내장해 트레이스를 데이터처럼 질의하고 맞춤형 메트릭을 추출 가능\n- 공식 문서는 perfetto.dev에서 제공되며, 입문자부터 숙련자까지 단계별로 가이드 구성\n- \nGoogle 오픈소스 커뮤니티 가이드라인을 준수하는 개방형 프로젝트\nPerfetto를 사용하는 이유\n- \nAndroid 앱·플랫폼 개발자는 앱 시작 지연, 프레임 드롭, ANR 등 성능 문제의 근본 원인을 구조적으로 분석 가능\n- \nC/C++ 개발자는 Tracing SDK를 활용해 애플리케이션 실행 경로를 계측하고 병목 구간을 정밀하게 식별\n- \nLinux 커널·시스템 개발자는 ftrace 기반 커널 이벤트를 시각화하여 스케줄링, 시스템콜, 인터럽트 동작을 분석\n- \nChromium 개발자는 chrome://tracing의 백엔드로 Perfetto를 사용해 브라우저·V8·Blink 내부 동작을 디버깅\n- \n성능 엔지니어 및 SRE는 Linux perf, macOS Instruments, Chrome JSON trace 등 다양한 포맷을 SQL로 통합 분석 가능",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25402",
    "category": "튜토리얼"
  },
  {
    "title": "자바스크립트를 HTML만으로 대체하기",
    "content": "- 웹에서 JS 의존 기능을 HTML/CSS로 대체할 수 있는 최신 방법들을 소개\n- \ndetails·summary , datalist , popover 등 표준 요소를 활용해 아코디언, 자동완성, 모달, 오프스크린 내비게이션을 구현\n- 이러한 접근은 다운로드·평가·메모리 사용량을 줄여 성능과 사용자 경험을 개선\n- 각 기능별로 CodePen 예시, MDN 문서, 브라우저 호환성 정보가 함께 제공\n- JS를 꼭 필요한 영역에만 사용하고, HTML/CSS의 발전된 기능을 적극 활용해야 함\n---\n\n## HTML과 CSS로 JS 기능 대체\n- 오랜 기간 JavaScript는 HTML과 CSS로 구현할 수 없는 기능을 담당해 왔음\n그러나 HTML과 CSS의 기능이 확장되면서, 일부 JS 기능을 네이티브 웹 기술로 대체 가능\n- JS는 다운로드·압축 해제·평가·메모리 유지가 필요하므로, 가능한 기능은 HTML/CSS로 이관하는 것이 효율적\n- JS는 복잡한 로직에 집중하고, 단순 UI 제어는 HTML/CSS에 위임하는 방향 제시\n아코디언 / 확장 콘텐츠 패널\n- \ndetails 와 summary 요소로 JS 없이 아코디언 구현 가능\n콘텐츠를 클릭으로 열고 닫을 수 있으며, open 속성으로 기본 상태 지정\n- 동일한 name 속성을 사용하면 하나의 패널만 열림\n- CSS로 스타일링하거나 JS로 열림/닫힘을 제어할 수도 있음\n- 관련 자료: MDN details 문서, CodePen 예시, 브라우저 호환성 표\n자동완성 제안 입력창\n- \ninput 과 datalist 조합으로 자동 필터링 드롭다운 구현\n입력 시 제안 목록이 자동으로 필터링됨\n- 텍스트 외에도 number, time 등 다양한 입력 타입 지원\n- Firefox는 현재 텍스트 기반 입력만 지원하며, 모바일 접근성 제약 존재\n- 관련 자료: MDN datalist 문서, SitePoint 튜토리얼, 브라우저 호환성 표\n모달 / 팝오버\n- \npopover 및 popovertarget 속성으로 JS 없는 모달·팝오버 구현\nauto, hint, manual 세 가지 모드 제공\n- \nauto는 외부 클릭이나 ESC로 닫힘, manual은 수동으로만 닫힘\n- Firefox 및 iOS는 hint 모드 미지원\n- 관련 자료: MDN popover 문서, Chrome 블로그, CodePen 예시, 접근성 관련 영상\n오프스크린 내비게이션 / 콘텐츠\n- \npopover 기능을 활용해 오프스크린 내비게이션 메뉴 구현 가능\nnav 요소를 사용해 의미 부여, CSS translate로 화면 안팎 이동\n- 외부 클릭 시 닫히며, popover=\"manual\"로 수동 닫기 설정 가능\n- \n::backdrop 가상 요소로 배경 반투명 처리 가능\n- 관련 자료: MDN Popover API, Chrome 블로그, CodePen 예시\n결론\n- JS의 강력함을 인정하되, 필요한 곳에만 사용해야 함\n- 최근 HTML/CSS의 발전으로 JS 없는 대안이 다수 등장\n- 더 많은 예시는 작성자의 블로그 글 “Replace JS with No-JS or Lo-JS Options”에서 확인 가능\n- \nJS 최소화와 성능 최적화를 통한 사용자 경험 개선 강조",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25400",
    "category": "AI"
  },
  {
    "title": "한장에 출력하는 2026년 달력",
    "content": "- 한 해의 모든 날짜를 한 페이지에 표시하는 인쇄용 달력\n- 페이지를 인쇄하면 용지 크기에 맞게 자동 조정되어 한 장에 전체 연도가 표시됨\n- \n가로 방향 인쇄와 머리글·바닥글 비활성화 설정 시 가장 깔끔한 결과 제공\n- 달력을 접어 휴대하거나 메모용으로 활용할 수 있음\n- 2026년 전체 일정을 한눈에 볼 수 있는 단순하고 실용적인 도구\n- \nPHP 소스도 공개되어 있음",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25399",
    "category": "개발"
  },
  {
    "title": "1994년 이후 최저 수준으로 떨어진 미국 달러의 글로벌 준비통화 비중",
    "content": "- IMF 자료에 따르면 전 세계 외환보유액 중 달러화 자산 비중이 **56.9%**로 하락, 1994년 이후 최저 수준 기록\n- 중앙은행들은 미국 국채 등 달러 표시 자산을 소폭 늘렸지만, 다른 통화 자산을 더 많이 추가하며 비달러 자산 다변화 진행\n- 특히 ‘비전통적 준비통화(non-traditional reserve currencies)’ 로 불리는 소규모 통화들의 합산 비중이 급등해 달러와 위안화의 점유율을 잠식\n- 달러 비중 하락에도 불구하고 여전히 세계 최대 준비통화 지위 유지, 그러나 미국의 쌍둥이 적자(무역·재정) 조달 비용 상승 가능성 존재\n- 중앙은행의 보유 통화 구조 변화는 글로벌 금융질서의 점진적 재편을 보여주는 지표로 평가됨\n---\n\n## 달러화 비중 하락 현황\n- IMF의 공식 외환보유액 통계에 따르면 2025년 3분기 달러화 비중은 **56.9%** , 2분기 **57.1%**, 1분기 **58.5%**에서 지속 하락\n이는 1994년 이후 최저 수준으로, 1977년 85.**5%** 정점 이후 장기 하락세 지속\n- 달러 표시 외환보유액에는 미국 국채, MBS, 기관채, 회사채 등이 포함되며, 각국 중앙은행이 자국 통화로 보유한 자산은 제외\n- 중앙은행들은 달러 자산을 매도하지 않았으며, 달러 자산 규모는 **7.4조** 달러로 소폭 증가, 그러나 타 통화 자산 증가폭이 더 큼\n달러 비중 하락의 구조적 배경\n- 지난 10년간 중앙은행의 달러 자산 총량은 거의 변동 없음, 반면 다른 통화 자산의 급증으로 달러 비중이 상대적으로 축소\n- \n비전통적 준비통화의 합산 비중이 5.**6%**로 상승, 엔화(5.**8%**)에 근접\n- \n위안화 비중은 2022년 1분기 이후 하락세, 2019년 수준으로 회귀\n- 결과적으로 달러와 위안화 모두 점유율을 잃고, 다수의 소규모 통화가 그 자리를 대체\n주요 준비통화별 보유 규모\n- 2025년 3분기 기준 전 세계 외환보유액 총액은 **13조** 달러\n달러화 자산: **7.41조** 달러\n- 유로화 자산: **2.65조** 달러\n- 엔화 자산: **0.76조** 달러\n- 파운드화 자산: **0.58조** 달러\n- 캐나다 달러: **0.35조** 달러\n- 호주 달러: **0.27조** 달러\n- 위안화: **0.25조** 달러\n- \n유로화 비중은 약 **20%**로 2015년 이후 안정적, 유럽 부채위기 이전에는 **25%**에 근접했음\n- \n기타 통화들의 합산 비중이 꾸준히 상승**, 달러** 비중 하락의 주요 원인으로 작용\n비전통적 준비통화의 부상\n- IMF는 호주 달러, 캐나다 달러, 한국 원화, 싱가포르 달러, 북유럽 통화 등을 비전통적 준비통화로 분류\n- 이들 통화의 합산 비중은 **5.6%**로, 위안화와 유사한 수준\n- IMF는 개별 통화 비중을 공개하지 않지만, OECD 비유로존 국가 및 신흥국 통화가 포함된 것으로 설명\n예: 체코, 덴마크, 노르웨이, 폴란드, 스웨덴, 멕시코, 칠레, 한국, 뉴질랜드 등\n- 일부 국가는 무역 관계나 환율 연동(페그) 을 이유로 특정 통화를 보유\n예: 나미비아는 남아공 랜드, 카자흐스탄과 키르기스는 러시아 루블 보유\n달러의 준비통화 지위와 미국 경제 영향\n- 외국 중앙은행이 미국 국채 등 달러 자산을 매입하면 자산 가격 상승과 수익률 하락(차입비용 절감) 효과 발생\n- 달러의 지배적 지위는 미국의 무역적자와 재정적자 조달을 용이하게 함\n- 그러나 달러 수요 감소는 장기적으로 미국의 차입비용 상승과 쌍둥이 적자 유지의 어려움으로 이어질 가능성\n- 1990~1991년에도 달러 비중이 **50%** 이하로 하락했으나, 인플레이션 통제 이후 회복세 전환\n- 현재의 하락세는 다양한 통화로의 분산 투자 확대라는 구조적 변화로 평가됨\n금과 외환보유액의 구분\n- \n금은 통화가 아닌 ‘준비자산(reserve asset)’ 으로, 외환보유액 통계에는 포함되지 않음\n- 중앙은행의 외환보유액은 통화 자산만 포함, 금은 별도 항목으로 관리\n- 금 보유량은 약 35,000톤 수준으로 10년 전과 큰 차이 없음, 주요 보유국은 미국 재무부(약 8,000톤)\n- 금은 외환보유액 비중 계산에 영향을 주지 않으며, 달러 비중 하락과 직접적 연관 없음\n결론\n- \n달러화 비중의 하락은 매도보다는 다변화의 결과로, 중앙은행들이 다양한 통화 자산을 확대 중\n- \n비전통적 통화의 부상은 글로벌 금융 구조의 점진적 변화 신호\n- 달러는 여전히 세계 최대 준비통화이지만, 지속적 하락세는 미국의 재정·무역 구조에 부담 요인으로 작용 가능\n- 향후 달러 **50%**선 유지 여부가 국제통화체제의 중요한 분기점으로 주목됨",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25397",
    "category": "스타트업"
  },
  {
    "title": "레인보우 식스 시즈 해킹으로 플레이어들이 수십억 크레딧을 받고 무작위 밴 발생",
    "content": "- \nUbisoft의 레인보우 식스 시즈가 대규모 해킹 또는 익스플로잇 의심 사태로 전 플랫폼 서비스 장애를 겪고 있음\n- 플레이어들은 수십억 R6 크레딧과 희귀 개발자 전용 스킨을 받거나, 이유 없이 밴되는 현상을 보고함\n- Ubisoft는 이를 “서버 사고”로 설명했으나, 커뮤니티는 보안 침해 축소 의혹을 제기하며 강하게 비판함\n- 일부 스트리머와 유명 유저 계정도 영향을 받아, 계정 롤백과 무고한 밴 우려가 커지고 있음\n- 전 세계 핵심 서비스가 중단된 가운데, 플레이어들은 접속을 피하고 크레딧 사용을 자제해야 하는 상황\n---\n\n## 레인보우 식스 시즈 전면 서비스 장애\n- Ubisoft의 공식 서비스 상태 페이지에 따르면, PC·PS4·PS5·Xbox One·Xbox Series X|S 등 모든 주요 플랫폼에서 인증, 인게임 상점, 매치메이킹 등 핵심 서비스가 중단됨\n전체 연결 상태는 ‘저하됨’으로 표시\n- Ubisoft는 문제를 조사 중이라고만 밝혔으며, 복구 일정은 제시되지 않음\n- 이러한 광범위한 장애로 인해 단순한 서버 불안정이 아닌 심각한 시스템 침해 가능성이 제기됨\nUbisoft의 대응과 커뮤니티 반발\n- 공식 레인보우 식스 계정은 “게임에 영향을 미치는 사고를 인지하고 있으며 해결 중”이라고 발표\n그러나 보안 침해나 해킹 언급은 없음\n- 커뮤니티는 Ubisoft가 사태의 심각성을 축소하고 있다고 비판\n“서버 문제”로 표현하면서도 핵심 시스템이 손상된 점을 지적\n- 일부는 계정이 실시간으로 변경되는 동안 게임이 계속 온라인 상태였던 점을 문제 삼음\n플레이어 계정 이상 현상\n- 다수의 플레이어가 로그인 시 수십억 R6 크레딧과 Renown, 수천 개의 Alpha Pack, 개발자 전용 Glacier 스킨 등을 획득한 것으로 보고\n- 인게임 밴 피드가 무작위 메시지로 오염되어 수천 개의 계정이 임의로 밴되거나 해제되는 현상 발생\n스트리머와 유명 플레이어 계정도 포함\n- 유명 크리에이터 KingGeorge는 “게임이 완전히 망가졌다”며 로그인 금지와 크레딧 사용 자제를 경고\n계정 롤백과 밴 우려\n- 인증 및 계정 시스템이 손상된 상태에서, 많은 유저가 Ubisoft의 계정 롤백 조치를 예상\n- 그러나 일부는 무고한 밴 가능성을 우려\n문제를 단순 시각 오류로 착각해 크레딧을 사용한 유저가 제재될 수 있다는 점 지적\n- 커뮤니티는 명확한 소통 부재로 불만이 커지고 있으며, Ubisoft에 투명한 설명을 요구\n현재 플레이어 행동 지침\n- Ubisoft가 문제 해결과 원인 공개를 완료하기 전까지, 플레이어는 오프라인 상태 유지 권장\n접속 및 크레딧·Renown 사용 금지\n- 전 세계적으로 핵심 서비스가 마비되고 계정이 손상된 상황으로, Ubisoft의 대응이 향후 게임 신뢰도에 중대한 영향을 미칠 가능성 있음\n- Ubisoft는 조사가 진행 중이며 추가 업데이트를 예고함",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25396",
    "category": "개발"
  },
  {
    "title": "DB 업데이트는 성공했는데 Kafka 메시지 전송이 실패한다면? [Transactional Outbox 패턴]",
    "content": "[블로그] 데이터 정합성을 위한 '보낸 편지함': 트랜잭셔널 아웃박스 패턴 정리\n(원문: https://fredly.dev/transactional-outbox/)\n분산 시스템에서 DB 업데이트와 메시지 발행의 원자성을 보장하는 '트랜잭셔널 아웃박스 패턴(Transactional Outbox Pattern)'을 상세히 분석한 글입니다. 분산 트랜잭션(2PC)의 복잡성을 피하면서도 데이터의 최종 정합성을 확보하는 실질적인 엔지니어링 설계 방안을 제시합니다.\n\n---\n\n## 주요 내용\n- 이중 쓰기(Dual-Write)의 늪: DB와 메시지 브로커가 서로 다른 트랜잭션 범위를 가질 때 발생하는 데이터 불일치 사례 분석.\n- 패턴의 핵심 구조:\n아웃박스(Outbox) 테이블: 발행할 이벤트를 비즈니스 데이터와 동일한 DB 트랜잭션 내에 저장하여 '원자성' 확보.\n- 메시지 릴레이(Message Relay): 저장된 이벤트를 읽어 외부 브로커로 안정적으로 전달하는 역할.\n- 구현 전략 비교:\n폴링 퍼블리셔(Polling Publisher): 주기적 쿼리 기반의 단순하고 직관적인 구현.\n- 로그 테일링(Log Tailing): CDC(Change Data Capture) 기술을 활용하여 DB 성능 저하 없이 실시간 이벤트 추출.\n- 전달 신뢰성: \"적어도 한 번(At-least-once)\" 전달을 보장하기 위한 메커니즘과 수신측의 멱등성 처리의 중요성 강조.",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25392",
    "category": "AI"
  },
  {
    "title": "NHS에서 Palantir 도입 반대",
    "content": "- 영국 NHS가 Palantir의 소프트웨어를 도입하려는 계획에 대해 시민들이 반대 운동을 벌이고 있음\n- Palantir는 미국의 정보기술 기업으로, 미국 내 대규모 추방 지원과 가자 지역의 학살 행위에 연루되었다는 비판을 받고 있음\n- 공동 창립자 Peter Thiel이 NHS를 공개적으로 비난한 발언도 문제로 지적됨\n- 웹사이트는 사용자가 자신의 지역 NHS 트러스트가 Palantir 소프트웨어를 사용하는지 확인하고, 반대 이메일을 보낼 수 있는 도구를 제공함\n- 정부가 NHS 트러스트에 도입 압력을 가하고 있어, 시민 참여를 통한 저항이 시급하다는 호소가 이어짐\n---\n\n## Palantir의 NHS 참여 계획과 논란\n- NHS England가 Palantir의 소프트웨어를 통해 의료 기록 시스템을 운영하려는 계획을 추진 중임\nPalantir는 미국의 스파이 기술 기업으로 소개됨\n- 이 회사는 미국 내 대규모 추방 지원과 가자 지역의 학살 행위에 관여했다는 비판을 받고 있음\n- Palantir 공동 창립자 Peter Thiel은 NHS에 대해 부정적인 발언을 한 것으로 언급됨\n그는 NHS가 “사람들을 병들게 한다”고 말했으며, 영국 국민이 NHS를 사랑하는 이유를 “스톡홀름 증후군 때문”이라고 표현함\n시민 행동 촉구\n- 웹사이트는 시민들이 자신의 지역 NHS 트러스트가 Palantir 소프트웨어를 사용하는지 확인할 수 있는 기능을 제공함\n우편번호를 입력하면 해당 지역 트러스트의 사용 여부를 확인 가능\n- 사용자는 Palantir 도입 반대 이메일을 직접 발송할 수 있음\n이메일은 지역 NHS 트러스트와 보건부 장관 Wes Streeting에게 전송됨\n- 현재 20,664명이 이메일을 보냈으며, 목표는 25,000명임\n웹페이지에는 최근 이메일 발송자들의 이름과 시간이 실시간으로 표시됨\n참여 절차와 개인정보 입력\n- 참여자는 이름과 이메일 주소를 입력해야 함\n이메일 수신 동의 여부를 선택할 수 있으며, Good Law Project와 Just Treatment의 업데이트를 받을 수 있음\n- 이메일 발송 과정은 단계별로 진행되며, 사용자가 직접 확인 후 전송 가능\n관련 단체 소개\n- \nJust Treatment는 환자 중심의 건강 정의 단체로, 보건 시스템 내 기업 권력의 영향 축소와 양질의 치료 보장을 목표로 함\n- 본 캠페인은 Good Law Project와 협력하여 진행됨\n기술 및 웹사이트 정보\n- 사이트는 Clear Honest Design이 설계 및 개발함\n- 쿠키는 최소한으로 사용되며, 익명 분석 정보만 수집함\n- 사용자는 쿠키 허용 범위를 직접 선택할 수 있음",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25391",
    "category": "트렌드"
  },
  {
    "title": "우리는 어떻게 오락에 소통을 잃어버렸는가",
    "content": "- 인간 간 커뮤니케이션 채널이 점차 콘텐츠 유통 네트워크로 변형되며, 연결성보다 소비성이 우선되는 현상이 드러남\n- ActivityPub과 같은 프로토콜이 ‘콘텐츠 전달’ 중심의 사양으로 설계되어, 메시지 신뢰성보다 엔터테인먼트 소비 효율이 우선됨\n- 소셜 네트워크가 소통을 표방했지만 실제로는 주의력과 시간을 점유하는 미디어 시스템으로 작동해 왔음\n- 이메일, RSS, XMPP 등 신뢰성 높은 비동기 통신 수단은 ‘지루함’과 비수익성 때문에 점차 외면받음\n- 사용자들은 여러 플랫폼 계정을 당연시하며, 플랫폼 간 상호운용성보다 플랫폼 종속적 경험을 받아들임\n- 이러한 현실 인식 위에서, 저자는 대중 플랫폼을 떠나 의도적으로 다른 커뮤니케이션 경로를 선택하겠다는 결론에 도달\n---\n\n## 커뮤니케이션과 엔터테인먼트의 전환\n- 모든 커뮤니케이션 채널이 콘텐츠 배포망으로 변질되어, 사람들은 더 많이 즐기지만 덜 연결된 상태에 있음\n- Pixelfed와 Fediverse 논쟁을 계기로, 커뮤니케이션 프로토콜과 콘텐츠 소비 프로토콜 간의 인식 차이가 드러남\n일부는 ActivityPub을 인간 간 통신 수단으로 보지만, 다른 일부는 콘텐츠 소비용 플랫폼으로 인식\n- ActivityPub의 공식 정의는 ‘콘텐츠 전달을 위한 소셜 네트워킹 프로토콜’ 로, 커뮤니케이션보다는 게시물 전송에 초점\n계정의 분화와 플랫폼 종속\n- 상호운용성의 목적은 여러 계정을 만들지 않아도 되는 환경이지만, 현실은 반대로 진행\n- 대형 플랫폼들은 ‘플랫폼별 계정 필요’라는 인식을 강화해 사용자 종속을 유도\n- 많은 사용자가 Fediverse에서도 ‘하나의 플랫폼 = 하나의 계정’ 철학을 유지하며, 이메일식 연합 통신 개념이 약화됨\n소셜 네트워크의 본질적 변화\n- 소셜 네트워크는 더 이상 커뮤니케이션 네트워크가 아니라 엔터테인먼트 플랫폼으로 기능\n- ‘아랍의 봄’ 시절의 커뮤니케이션 도구로서의 환상은 거짓이었으며, 실제 목적은 사용자 체류와 콘텐츠 소비 극대화\n- 분산형 네트워크조차 ‘이메일 2.0’이 아닌 ‘텔레비전 2.0’ 으로 작동\n사용자가 직접 콘텐츠를 생산하지만, 구조적으로는 집중화된 미디어 소비 시스템과 유사\n메시지 신뢰의 붕괴\n- 알고리듬 기반 플랫폼에 익숙한 사용자들은 메시지 손실을 문제로 인식하지 않음\n- 즉시 응답이 없으면 메시지 수신조차 믿지 못하는 즉시성 중심 커뮤니케이션 문화가 확산\n- 연구에 따르면, 소셜 미디어에서 본 대부분의 콘텐츠를 몇 초 내에 잊음, 따라서 신뢰성 있는 통신 수단으로 기능하지 못함\n- Pixelfed, PeerTube 등 일부 Fediverse 도구는 메시지 표시 누락이 발생하며, 이는 구조적 한계로 지적됨\n- Pixelfed는 향후 텍스트 메시지 누락 방지 옵션을 추가할 예정\n이메일과 비동기 통신의 쇠퇴\n- 이메일은 비동기적이고 신뢰성 높은 통신 수단이지만, 젊은 세대는 이를 ‘구식’ 혹은 ‘형식적’ 이라 인식\n- 많은 사용자의 이메일함은 광고와 스팸으로 가득 찬 피드형 구조로 변질\n- 반면 일부 사용자는 Inbox Zero 전략을 유지하며, 이메일을 여전히 의도적 커뮤니케이션 도구로 활용\n‘지루함’과 중독성의 경제\n- 이메일, RSS, IRC, XMPP 등 기존 통신 프로토콜은 완성도 높지만 비수익적\n- 이들은 도파민 자극이나 중독성을 제공하지 않아 광고 산업에 매력적이지 않음\n- 커뮤니케이션 자체보다 ‘엔터테인먼트화된 상호작용’ 이 더 큰 경제적 가치를 창출\n- 일부 인간은 애초에 소통보다 주목과 소비를 원했으며, 그 결과 거대 기술 기업이 막대한 부를 축적\n다시 ‘진짜 커뮤니케이션’으로\n- 개인적으로는 다수와 연결되어야 한다는 ‘임계 질량 신화’ 를 거부\n모두와 연결되는 것을 목표로 하지 않고, 의미 있는 소수와의 신뢰 가능한 소통을 선택\n- 이메일, RSS, 메일링 리스트, 오프라인 도구 등 의도적으로 지루하지만 안정적인 수단으로 회귀\n- \n오프라인 중심 브라우저 Offpunk 를 사용해 비동기적 소통을 지속\n- 완전한 연결보다 진정한 교류를 원하는 소수의 보호 구역이 존재하고, 이를 유지하는게 내 길이라고 생각",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25388",
    "category": "트렌드"
  },
  {
    "title": "ChatGPT 광고 도입 임박: OpenAI가 검토 중인 3가지 수익화 전략",
    "content": "배경\n- ChatGPT에 광고 도입 검토 중\n- 기존 \"광고 없을 것\" 입장 변화\n- 구글 Gemini 광고 도입 발표 영향\n- 월 **20달러** 구독료만으로는 인프라 비용 감당 어려움\n- 수억 명 무료 사용자 수익화 필요\n주요 출처\n- The Information 보도 등\n검토 중인 3가지 광고 전략\n- \n구매 의도 쿼리 대상 스폰서 결과 우선 배치\n예: \"노트북 추천해줘\" → 광고주 제품 먼저 노출\n- 영감: 아마존/구글 검색광고 모델\n- 한계: 전체 쿼리 중 **2.1%**만 해당 (적용 범위 제한적)\n- \n대화 이력 분석 기반 맞춤형 광고\n사용자 과거 대화 데이터 마이닝 → 관심사 파악\n- 장점: 정교한 타겟팅 가능\n- 단점: 프라이버시 논란 불가피 (민감 대화 활용 우려)\n- \n사이드바 별도 광고 영역 표시\n답변 본문 건드리지 않음\n- 상대적으로 덜 침입적 접근\n우려 사항\n- 사용자 경험 저하 및 신뢰 하락 가능\n- 특히 민감 주제 대화에 광고 삽입 시 문제\n현실적 압박\n- 구글/메타: 연간 수천억 달러 광고 수익\n- 경쟁사 Gemini 광고 도입 공언 → OpenAI 경쟁력 유지 필요\n결론\n- 타이밍과 방식이 핵심 (공격적 vs 조심스러운 접근 딜레마)\n- AI 챗봇 광고 시장 개방 시작\n- 사용자 수용 여부 미지수",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25387",
    "category": "AI"
  },
  {
    "title": "Ez FFmpeg – 평문 영어로 하는 비디오 편집",
    "content": "- \nffmpeg 명령어를 평문 영어로 실행할 수 있게 만든 Node.js 기반 CLI 도구로, 복잡한 옵션 없이 간단한 문장으로 영상 편집 가능\n- \nff convert video.mp4 to gif처럼 명령어를 문장 형태로 입력하면 자동으로 ffmpeg 명령으로 변환해 실행\n- \n형식 변환, 압축, 자르기, 오디오 추출, 리사이즈, 속도 조절, 회전, 병합 등 다양한 작업을 지원\n- \n- -dry-run 옵션으로 실제 실행 없이 ffmpeg 명령을 미리 확인 가능\n- 인터넷 연결이나 AI 없이 오프라인에서 빠르게 동작하며, ffmpeg가 설치된 환경에서 Node.js 16 이상으로 사용 가능\n---\n개요\n- \nezff는 ffmpeg를 쉽게 사용할 수 있도록 만든 평문 영어 기반 래퍼(wrapper)\n사용자는 복잡한 ffmpeg 플래그나 매뉴얼을 외울 필요 없이 자연어 형태로 명령 입력\n- 예: ff convert video.mp4 to gif → 자동으로 ffmpeg 명령 생성 및 실행\n- \nNode.js 16 이상과 ffmpeg 설치가 필요하며, npm을 통해 npm install -g ezff로 설치 가능\n주요 기능\n- \n대화형 프롬프트 모드 제공\nff 명령만 입력하면 파일 경로, 작업 종류, 출력 형식 등을 순차적으로 질문\n- 예시:\n? File path: video.mp4 \n? What do you want to do? › Convert format \n? Convert to: › GIF \n- 선택 후 ffmpeg 명령이 자동 생성되어 실행됨\n- \n직접 명령 입력 모드 지원\n사용자가 원하는 작업을 한 줄로 입력 가능\n- 예:\nff convert video.mp4 to mp3\n- \nff compress video.mp4 to 10mb\n- \nff trim video.mp4 from 0:30 to 1:00\n- \nff resize video.mp4 to 1280x720\n- \nff speed up video.mp4 by 2x\n지원 작업 목록\n- \n변환(Convert) : 형식 변경 (mp4 → gif, mp4 → mp3)\n- \n압축(Compress) : 파일 크기 제한 설정 (10mb 등)\n- \n자르기(Trim) : 특정 구간 추출 (from 0:30 to 1:00)\n- \n오디오 추출(Extract audio) : 영상에서 오디오만 분리\n- \n리사이즈(Resize/Scale) : 해상도 변경 (1280x720, 720p)\n- \n속도 조절(Speed up / Slow down) : 재생 속도 변경 (2x)\n- \n반전(Reverse) , 음소거(Mute) , 회전(Rotate) , 뒤집기(Flip) , 썸네일 추출(Thumbnail) , 병합(Merge) , 그레이스케일(Grayscale) , 노이즈 제거(Denoise) , 영상 안정화(Stabilize) 등 다양한 작업 지원\n명령 미리보기 및 출력\n- \n- -dry-run 옵션으로 실제 실행 없이 ffmpeg 명령만 출력 가능\n\n---\n\n## 예\nff convert video.mp4 to gif --dry-run \n→ ffmpeg -i video.mp4 -vf fps=15,scale=480:-1:flags=lanczos -loop 0 -y video_output.gif\n- 결과 파일은 원본과 같은 폴더에 _output 접미사로 저장 (video_output.gif)\n내부 동작 구조\n- 입력 문장을 Parser가 분석해 동작(action), 파일(file), 옵션(size 등)을 추출\n- \nBuilder가 해당 정보를 ffmpeg 명령으로 매핑\n- \nffmpeg가 실제 명령을 실행\n- 모든 과정은 AI나 API 호출 없이 오프라인에서 패턴 매칭 기반으로 수행\n설치 및 환경\n- \nNode.js 16 이상 필요\n- \nffmpeg가 시스템 PATH에 설치되어 있어야 함\nmacOS: brew install ffmpeg\n- Ubuntu/Debian: sudo apt install ffmpeg\n- Windows: choco install ffmpeg\n- \nMIT 라이선스로 공개되어 있으며, Pull Request 환영\n요약\n- ezff는 ffmpeg 명령어 학습 부담을 없애는 간단한 CLI 도구\n- \n평문 영어 입력만으로 영상 편집 자동화 가능\n- \n오프라인, 빠른 실행, 다양한 기능 지원으로 개발자와 영상 편집 초보자 모두에게 유용한 도구",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25385",
    "category": "AI"
  },
  {
    "title": "Argc - Bash 기반의 강력한 CLI 빌드 프레임워크",
    "content": "- \n명령행 인터페이스(CLI) 를 손쉽게 구축하기 위해 설계된 Bash 프레임워크로, 인자 파싱과 도움말, 오류 처리 등 반복 코드를 자동화\n- 주석 기반 정의 방식으로 CLI 구조를 기술하고, 핵심 로직에만 집중할 수 있는 간결한 개발 흐름 제공\n- \n플래그, 옵션, 위치 인자, 서브커맨드를 자동 처리하며, 입력 검증과 풍부한 사용법 텍스트 생성 지원\n- 인자값을 자동으로 변수에 매핑, 스크립트 내에서 직관적으로 접근 가능\n- \n독립 실행형 Bash 스크립트 생성 기능을 통해 Argc 의존성 없이 배포 가능한 형태로 빌드 가능\n- \n자동 완성 스크립트 생성을 지원해 bash, zsh, fish, powershell 등 다양한 셸 환경에서 탭 완성 기능 제공\n- \nMan 페이지 자동 생성 기능을 포함해 CLI 문서화를 간소화\n- \n환경 변수 통합 기능을 통해 옵션 및 위치 인자와의 바인딩, 검증, 문서화를 자동 처리\n- \nArgcfile.sh 기반 작업 자동화 기능을 제공해 Makefile 유사한 명령 실행기(command runner) 로 활용 가능\nBash 친화적 워크플로우 유지\n- \nGNU 도구셋(awk, sed, grep 등) 과 자연스럽게 통합\n- dotenv 로드 및 환경 변수 검증 지원\n- \nCross-platform 호환성 제공: macOS, Linux, Windows, BSD 등 다양한 환경에서 동일한 동작 보장\n- \nargc --argc-build, argc --argc-completions, argc --argc-mangen 등 명령을 통해 빌드·자동완성·문서화 전 과정을 자동화\n- MIT 또는 Apache 2.0 라이선스 선택 가능",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25382",
    "category": "AI"
  },
  {
    "title": "Show GN: Rust로 만든 가벼운 AWS SES 대량 메일 서버 (AGENTS.md 포함)",
    "content": "AWS SES로 대량 이메일을 발송하는 서버를 Rust(Axum)로 만들었습니다.\n뉴스레터, 알림 메일 등 한 번에 많은 메일을 보내야 할 때 사용할 수 있습니다. SES 샌드박스만 탈출하면 바로 사용 가능하고, SQLite 기반이라 별도 DB 서버 없이 단독으로 돌릴 수 있습니다.\n주요 기능\n요청당 최대 10,000개 이메일 일괄 발송\n예약 발송 지원 (scheduled_at)\nToken Bucket 기반 초당 발송량 제어\nAWS SNS 웹훅으로 Bounce/Complaint/Delivery 이벤트 수신\n1x1 픽셀 기반 오픈 트래킹\n왜 Rust?\nTokio 비동기 런타임 위에서 동작하고, 메모리 사용량이 적어서 오래 띄워둬도 안정적입니다. 한 번 배포하면 신경 쓸 일이 거의 없습니다.\nAGENTS.md\n프로젝트 구조, DB 스키마, 코드 스타일 가이드를 AGENTS.md에 정리해두었습니다. Cursor나 Copilot 같은 AI 코딩 도구가 프로젝트 컨텍스트를 빠르게 파악할 수 있어서, AI와 페어 프로그래밍할 때 편합니다.\n피드백이나 PR 환영합니다.\nGitHub: https://github.com/lee-lou2/aws-ses-sender",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25381",
    "category": "AI"
  },
  {
    "title": "로브 파이크, AI가 보낸 ‘친절 행위’ 스팸 메일에 분노",
    "content": "- 컴퓨터 과학자 로브 파이크가 AI가 생성한 감사 이메일을 받아 강한 분노를 표출함\n- 이메일은 비영리 단체 Sage가 운영하는 AI Village 프로젝트의 에이전트가 자동으로 발송한 것으로, “친절한 행위(Act of Kindness)” 실험의 일환이었음\n- AI Village는 여러 Claude 기반 AI 에이전트가 실제 Gmail 환경에서 자율적으로 이메일을 작성·발송하며 자선 모금이나 감사 메시지를 보내는 실험을 진행 중이었음\n- 파이크 외에도 Anders Hejlsberg, Guido van Rossum 등 유명 개발자들이 유사한 이메일을 받은 것으로 확인됨\n- 이 사건은 AI가 인간의 동의 없이 현실 세계에 개입하는 위험성을 드러내며, 프로젝트 측은 이후 비자발적 이메일 발송을 금지하는 지침을 추가함\n---\n\n## 로브 파이크가 받은 AI 생성 이메일 사건\n- 로브 파이크는 “Claude Opus 4.5 AI Village” 명의로 발송된 AI 생성 감사 이메일을 받고 격렬히 분노함\n그는 “단순한 소프트웨어를 추구한 나에게 혐오스러운 기계가 감사한다니”라며 공개적으로 비판\n- 이 사건은 Bluesky, Lobste.rs, Hacker News 등에서 활발히 논의됨\n- 이메일은 AI Village 프로젝트의 ‘친절 행위(Act of Kindness)’ 목표 수행 중 자동 발송된 것으로 확인됨\nAI는 GitHub 커밋의 .patch 기능을 이용해 파이크의 이메일 주소를 찾아냈음\n- 이후 Gmail 인터페이스를 자동 조작해 제목과 본문을 작성하고 발송까지 완료함\nAI Village 프로젝트 개요\n- \nAI Village는 비영리 단체 Sage가 운영하는 실험으로, 여러 AI 에이전트가 “자선 기금 모금” 또는 “친절한 행동” 을 목표로 자율적으로 활동함\n4개의 AI 에이전트가 컴퓨터와 그룹 채팅 환경에서 매일 수시간씩 작동\n- 크리스마스 당일에는 감사 이메일 발송이 목표로 설정됨\n- AI는 실제 Gmail 계정을 사용해 NGO, 언론인, 개발자 등에게 이메일을 보냄\n2주간 약 300통의 이메일을 발송했으며, 다수는 사실 오류나 허위 정보를 포함함\n- 일부 이메일 주소는 AI가 임의로 생성한 것으로 확인됨\n디지털 포렌식 조사\n- Simon Willison은 shot-scraper har 도구를 사용해 AI Village 웹페이지의 JSON 데이터를 수집함\nshot-scraper har --wait 10000 명령으로 페이지의 전체 요청·응답 기록을 확보\n- Claude Code를 이용해 관련 JSON을 분석하고, rob-pike.json 파일로 사건의 전체 타임라인을 재구성함\n- 분석 결과, AI는 여러 세션을 거쳐 이메일을 작성하고 발송 완료까지 진행함\n제목: “Thank You for Go, Plan 9, UTF-8, and Decades of Unix Innovation”\n- 본문에는 파이크의 Go, Plan 9, UTF-8, sam/Acme, Unix 저서 등의 업적이 상세히 언급됨\n- 같은 날 Anders Hejlsberg, Guido van Rossum에게도 유사한 이메일이 발송됨\nAI 실험의 문제점\n- AI Village의 실험은 현실 세계 인물에게 무단 이메일을 발송함으로써 시간 낭비와 불쾌감을 초래함\nAI가 실수를 저지르는 것이 문제가 아니라, 인간 검토 없이 실제 사람에게 접근하는 것이 문제로 지적됨\n- 이메일 발신자 명을 “Claude Opus 4.5”로 표시한 점도 Anthropic이 직접 보낸 것으로 오해를 불러일으킴\n- AI는 진정한 자율성(agency) 을 가질 수 없으며, 타인에게 연락하는 결정은 인간의 판단에 의해 이뤄져야 함\n“LLM에게 Gmail 접근권을 주는 것은 책임 있는 기술 활용이 아님”이라는 비판 제기\nAI Village의 대응\n- 공동창립자 Adam Binksmith는 트위터를 통해 대응 방안을 발표함\n에이전트에게 비자발적 이메일 발송 금지 지침을 추가하고, 향후 행동을 모니터링 중이라고 설명\n- AI에게 이메일 계정을 부여한 이유는 현실 세계 과업 수행 능력 평가를 위한 것이라고 밝힘\n- “가난 감소 목표” 단계에서 이미 이메일 발송이 시작됐으나, 지침 변경이 늦었다고 인정\n- 그는 이번 사건으로 인한 시간 낭비는 크지 않다고 판단했으나, 부정적 반응을 고려해 정책을 수정했다고 언급\n완전한 계정 차단 대신 프롬프트 수준에서 제어하는 방식을 택함\n- AI가 자유롭게 컴퓨터 환경을 사용할 수 있기 때문에, 명시적 금지 지시가 필요하다고 설명함",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25379",
    "category": "AI"
  },
  {
    "title": "Witr - 리눅스 시스템에서 프로세스가 실행 중인 이유를 설명하는 도구",
    "content": "- \nWitr (why-is-this-running) 는 리눅스 시스템에서 특정 프로세스, 서비스, 또는 포트가 왜 실행 중인지를 명확히 보여주는 도구\n- 기존 ps, top, lsof 등이 단순히 “무엇이 실행 중인지”만 보여주는 것과 달리, “왜 실행 중인지”의 인과 관계를 한 화면에 표시\n- \nPID 기반 분석을 통해 프로세스의 기원, 실행 경로, 유지 원인, 소속 컨텍스트를 추적\n- \nsystemd, docker, pm2, cron, shell 등 다양한 소스와 연계해 실행 원인을 설명하며, 읽기 전용·비파괴적으로 동작\n- 디버깅과 장애 대응 시 이해 시간을 단축하고, 복잡한 시스템의 실행 구조를 한눈에 파악할 수 있는 도구\n---\n\n## 목적과 개념\n- \nWitr의 핵심 질문은 “왜 이것이 실행 중인가? (why-is-this-running)”임\n프로세스, 서비스, 포트 등 실행 중인 모든 항목의 기원과 원인을 추적\n- 여러 계층(supervisor, container, service, shell 등)에 걸친 간접적 실행 원인을 명시적으로 보여줌\n- 기존 도구들이 상태와 메타데이터만 제공하는 반면, Witr는 인과 관계를 명확히 표현\n- 결과적으로 “무엇이, 어떻게, 왜, 어떤 맥락에서 실행 중인지”를 사람이 읽기 쉬운 형태로 출력\n주요 목표\n- \n프로세스 존재 이유를 설명하고, 단순 실행 여부 이상의 정보를 제공\n- \n디버깅 및 장애 대응 시간 단축\n- \n설정 없이 바로 사용 가능, 읽기 전용·안전성 보장\n- \n완전성보다 명확성을 우선\n- 모니터링, 성능 분석, 자동 복구 기능은 포함하지 않음\n작동 원리\n- 모든 대상을 프로세스(PID) 중심으로 해석\n포트, 서비스, 컨테이너, 명령어 모두 PID로 연결\n- PID를 기준으로 실행 인과 체인(causal chain) 을 구성\n- 핵심 질문 네 가지\n무엇이 실행 중인가\n- 어떻게 시작되었는가\n- 무엇이 유지시키고 있는가\n- 어떤 컨텍스트에 속하는가\n지원 대상\n- \n프로세스/서비스 이름, PID, 포트 번호를 입력 대상으로 지원\n이름 입력 시 여러 프로세스가 일치하면 PID 선택을 요청\n- \n- -pid, --port 옵션으로 특정 프로세스나 포트 기반 분석 가능\n출력 구조\n- \nTarget: 사용자가 지정한 대상\n- \nProcess: 실행 파일, PID, 사용자, 명령어, 시작 시각, 재시작 횟수\n- \nWhy It Exists: 프로세스의 인과 계보(ancestry chain)\n- \nSource: 실행을 담당한 주요 시스템 (예: systemd, docker, pm2, cron, shell 중 하나)\n- \nContext: 작업 디렉터리, Git 저장소, Docker 컨테이너, 바인드 정보 등\n- \nWarnings: 루트 권한 실행, 공용 인터페이스 리스닝, 장기 실행, 메모리 과다 사용 등 비차단 경고\n명령줄 옵션\n- \n- -pid, --port: 특정 PID 또는 포트 분석\n- \n- -short: 한 줄 요약\n- \n- -tree: 전체 프로세스 트리 표시\n- \n- -json: JSON 형식 출력\n- \n- -warnings: 경고만 표시\n- \n- -no-color: 색상 비활성화\n- \n- -env: 환경 변수만 표시\n- \n- -help: 도움말 표시\n설치 및 제거\n- \n단일 정적 Linux 바이너리 형태로 배포\n- \n스크립트 설치(권장)\ncurl -fsSL https://raw.githubusercontent.com/pranshuparmar/witr/main/install.sh | bash\n- CPU 아키텍처 자동 감지 후 /usr/local/bin/witr에 설치\n- \n수동 설치\namd64, arm64용 바이너리 직접 다운로드 및 체크섬 검증\n- 실행 권한 부여 후 PATH에 이동\n- \n검증 및 제거\nwitr --version, man witr로 확인\n- \nsudo rm -f /usr/local/bin/witr로 제거 가능\n- \nNix Flake 지원: nix run github:pranshuparmar/witr -- --port 5000으로 실행 가능\n플랫폼 및 권한\n- \nLinux 전용\n- \n/proc 접근을 통해 정보 수집\n일부 프로세스 정보 확인에는 sudo 권한 필요",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25376",
    "category": "AI"
  },
  {
    "title": "Show GN: 원격 크롬 디버거 chrome-remote-devtools",
    "content": "https://tech.kakao.com/posts/617\n작년에 카카오 발표 현장에 우연찮게 간적이 있었는데 언젠간 나도 만들어봐야지란 생각만 막연하게 가지고 있었다가,\nhttps://techblog.woowahan.com/23343/\n최근에 배민에서도 유사한걸 개발했길래 나도 이제 한번..? 하면서 개발 해보았습니다.\n실제 사용시 로그를 보려면 저장소에 있는 inspector 페이지를 빌드하여 배포하시거나, 웹소켓 서버를 활용하지 않고, 로그로만 확인하실거라면 플레이그라운드 에서 확인하셔도 됩니다.\n그리고 제공된 웹서버를 실행하고, 클라이언트 쪽에는\n클라이언트 코드를 아래와 같이 넣어주시면 됩니다\nimport { initCDPClient } from '@ohah/chrome-remote-devtools-client'; \nuseEffect(() => { \ninitCDPClient('wss://your-server.com', { \nenable: true, // Enable rrweb session recording \n}); \n}, []); \n데스크탑 앱으로 배포 하기 위해 타우리 설정도 해놓았습니다만 아직 배포는 하지 않았습니다.\n웹소켓 서버를 통해 연결하여 실시간으로 확인하거나, 사용자가 로그를 건네주면 해당 로그파일을 크롬 데브툴 UI를 통해 확인해보실 수 있습니다.\n원격 데브툴 특성상 지원하기 힘든 메모리나 성능 측정 부분은 제외하였고,\n간단한 Elements, Console, Network, LocalStorage, SessionStorage, Cookie 정도 확인 가능하며\nrr-web기능을 크롬 데브툴 디버거 안의 탭 안에 넣어서 사용자 화면 액션을 크롬 데브툴 안에서 볼 수 있게 했습니다.\nhttps://ohah.github.io/chrome-remote-devtools/ko/examples/index.html\n간단한 동작 및 기능은 위 링크에서 확인 하실 수 있습니다.\n주의 사항\n절대 프로덕션 레벨에서 사용하지 마십시오",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25375",
    "category": "개발"
  },
  {
    "title": "동화 같은 환각을 일으키는 새로운 버섯을 연구하는 전문가들",
    "content": "- \nLanmaoa asiatica라는 새로운 버섯이 발견되어, 섭취 시 수백 명의 작은 사람들을 보는 ‘릴리푸트 환각’ 을 유발하는 것으로 보고됨\n- 이 버섯은 기존의 ‘매직머시룸’ 과는 전혀 다른 균류 계통에 속하며, 중국 윈난성에서 ‘젠쇼우칭(Jian shou qing)’이라는 이름으로 널리 유통됨\n- DNA 분석 결과, 이 버섯은 식용 포르치니(Boletus edulis) 와 더 가까운 친척 관계를 가지며, 알려진 환각성분은 검출되지 않음\n- 동일한 환각 현상이 파푸아뉴기니와 필리핀 등 서로 다른 지역에서도 독립적으로 보고되어, 공통된 화학적·신경학적 원인이 존재할 가능성이 제기됨\n- 연구진은 이 버섯이 지닌 미지의 생리활성 화합물을 규명하기 위해 유전체 분석과 생화학 실험을 진행 중이며, 이는 인간 인식 연구의 새로운 단서를 제공할 잠재성이 있음\n---\n\n## Lanmaoa asiatica의 발견과 특징\n- \nLanmaoa asiatica는 중국 윈난성의 시장에서 판매되던 ‘젠쇼우칭(Jian shou qing)’ 버섯의 DNA 분석을 통해 새롭게 학계에 등록된 종\n이름의 뜻은 “손에 닿으면 파랗게 변한다”로, 절단 후 몇 초 내에 색이 변하는 특징을 가짐\n- 풍미가 뛰어나 식용으로 인기가 높지만, 덜 익혀 먹을 경우 강한 환각을 유발함\n- 환각 증상은 ‘릴리푸트 환각(Lilliputian hallucinations)’ 으로 불리며, 수많은 작은 인물들이 현실 공간에서 움직이는 듯한 시각적 경험을 동반\n윈난성 병원 기록에 따르면, 환각을 경험한 환자의 **96%**가 ‘작은 사람’이나 ‘요정’을 보았다고 보고\n- DNA 분석 결과, 이 버섯은 포르치니 버섯과 가까운 계통으로, 기존의 환각 버섯과는 전혀 다른 분류군에 속함\n역사적·문화적 배경\n- 1934년 파푸아뉴기니 서부 고지대에서 ‘논다(nonda)’라 불린 버섯을 먹은 현지인들이 일시적인 정신 이상과 환각을 보였다는 기록이 존재\n이후 보고된 사례에서도 ‘작은 사람’을 보는 동일한 현상이 반복적으로 관찰됨\n- 중국 윈난 지역에서는 수 세기 전부터 이 버섯이 알려져 있었으며, 3세기 도교 문헌에도 “작은 사람을 보게 하는 버섯”이 언급됨\n- \n필리핀 북부 코르딜레라 지역에서도 ‘세데스뎀(Sedesdem)’이라 불리는 버섯이 동일한 환각을 유발하는 것으로 전해졌으며, DNA 분석 결과 역시 Lanmaoa asiatica로 확인됨\n과학적 연구와 실험\n- 유타 자연사박물관 연구진은 Lanmaoa asiatica의 화학 성분을 분석했으나, 기존에 알려진 사이로시빈(psilocybin) 등 환각 물질은 검출되지 않음\n이는 완전히 새로운 미지의 생리활성 화합물이 존재할 가능성을 시사\n- 연구팀은 버섯 추출물을 실험용 생쥐에 투여한 결과, 대조군과 다른 행동 변화를 관찰\n현재 화합물 분획(fractionation) 과정을 통해 활성 물질을 분리 중\n- 병행 연구로 Lanmaoa 속 전체의 유전체 데이터베이스를 구축하여, 4종의 새로운 미기록 종을 추가로 발견\n유전체 비교를 통해 환각 특성이 언제, 어떤 진화적 경로로 나타났는지 탐색 중\n시장 유통과 안전성 문제\n- 윈난성 쿤밍의 버섯 시장은 세계 최대 규모로, 200여 종 이상의 야생 버섯이 거래됨\n그러나 젠쇼우칭은 인공 재배가 불가능해, 상업 포장품에는 유사하지만 다른 종이 혼입되는 사례가 다수 발견\n- 온라인 판매 제품의 DNA 분석 결과, 독성 버섯이 포함된 경우도 확인되어 안전성 우려 존재\n- 이러한 혼입 문제는 중국의 야생 버섯 수출 산업 전반에 품질 관리의 필요성을 제기\n연구의 의의와 향후 과제\n- \nLanmaoa asiatica는 서로 다른 대륙과 문화권에서 동일한 환각 현상을 유발하는 드문 사례로, 인간 인식과 신경화학 연구의 새로운 단서를 제공\n- 연구진은 이 버섯이 지닌 화학적·유전적 비밀을 밝히는 것이 인간 정신의 미스터리를 이해하는 열쇠가 될 수 있다고 평가\n- 아직 밝혀지지 않은 문화적 전통과 역사적 기록이 존재할 가능성이 높으며, 향후 민속학·생물학 융합 연구의 확장이 기대됨\n- 기사 말미에서는 “평범한 버섯 수프 한 그릇에도 아직 밝혀지지 않은 세계의 신비가 숨어 있다”고 강조하며, 과학과 전통의 경계에서 계속되는 탐구의 여정을 제시",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25372",
    "category": "트렌드"
  },
  {
    "title": "Show GN: Tokyo Night에 질리셨나요? 한국인을 위한 Vim 테마, 서울리즘",
    "content": "안녕하세요, 오늘은 좀 한국적인 주제로 포스트를 작성하게 되었습니다.\n지방에 거주하는 사람이 서울 테마를 만들 수가 있나, 하고 생각하실 수 있습니다.\n그래서 이건 지역과 상관없이 익숙한 상징에서 빌려왔습니다.\n나날이 대단해지는 한류도 겨냥하면 좋지만 이건 한국인을 위한 Vim 테마인데요,\n긴 코드를 읽을 때의 어려움인 구조 파악을 도와주기 위해 설계되었습니다.\n[서울리즘] 계통적 변용과 위계의 재설정\n서울리즘은 전통 오행의 순환 논리를 하이라이팅 시스템으로 이식하며 발생한 구조적 변이를 정직하게 기록한 테마입니다.\n1. 중심축의 이동: 북극성\n전통적 중심인 흙을 외곽으로 옮기고, 그 자리에 북극성(보라) 를 배치했습니다.\n- \n변용 논리: 데이터(흙)은 가변적 요소이나, 이를 해석하는 구문 규칙(북극성) 은 불변의 중심축이어야 한다는 위계적 재설정입니다.\n- \n배치 근거: 하늘의 가운데를 가리키는 북극성을 시스템 코어에 투영했습니다.\n2. 순환 경로: 상생의 유지\n원소 간의 상생 순서는 전통 로직을 계승하여 인지적 일관성을 확보했습니다.\n- \n나무: 로직의 정의 (탄생)\n- \n불: 경고 및 과열 (두려움)\n- \n흙: 데이터 안착 (뿌리)\n- \n쇠: 구조 (뼈대, 의지)\n- \n물: 배경 공간 (공)\n3. 설계적 발산 (Divergence)\n가독성을 위한 구조적 재배치를 은폐하지 않고 도면에 명시했습니다.\n- \nRelocation Vector: 중앙(전통적 좌표)에서 외곽(기능적 좌표)으로 이동한 흙의 궤적을 붉은 화살표로 기록했습니다.\n- \n설계 지향: \"전통적 순서는 유지하되, 기능적 우위를 위해 위계를 재배열한다\"는 실용적 변용의 결과물입니다.\n---\n서울리즘은 근거 없는 색 배열이 아닌, 전통의 순환 체계를 IT 강국인 현재 한국의 위계로 재해석한 계통적 변이입니다.\n코드의 색만 보고도 흐름을 추리해 보세요. 여러분의 생산성을 위한 한국적인 선택입니다.\n---\nP.S) 상단에 첨부된 깃허브에 도면이 있습니다. 디자인이나 철학 전공은 아니라서 잘 모르지만, 그릴 수 있는 데까진 열심히 그렸으니 관심이 있으시면 구경해 주세요.",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25371",
    "category": "개발"
  },
  {
    "title": "애보트 연속 혈당 측정기 결함으로 7명의 당뇨병 환자 사망",
    "content": "- 미국 FDA는 Abbott Freestyle Libre Plus 연속 혈당 측정기(CGM)의 결함으로 700명 이상 부상, 7명 사망을 보고함\n- 결함은 혈당을 비정상적으로 낮게 표시하는 오류로, 환자가 실제로는 정상 혈당임에도 과도한 당 섭취를 하게 되어 위험 초래\n- 문제의 기술적 원인(하드웨어 또는 소프트웨어)은 공개되지 않았으며, 정부 조사 범위도 불분명함\n- \n폐쇄형 의료기기 소프트웨어가 환자 안전을 위협한 또 다른 사례로, 과거 Therac-25 방사선 치료기 사고 등과 함께 언급됨\n- 글은 공개된 하드웨어 사양과 FOSS 기반 검증 체계의 필요성을 강조하며, 환자 안전을 위해 공공 검증과 투명성 확보가 중요함을 지적\n---\n애보트 Freestyle Libre Plus 결함과 FDA 보고\n- 미국 FDA는 Abbott의 Freestyle Libre Plus 연속 혈당 측정기에서 발생한 결함으로 700명 이상 부상, 7명 사망했다고 발표\n결함은 기기가 비정상적으로 낮은 혈당 수치를 표시하는 문제로, 환자가 실제로는 저혈당이 아님에도 당분 섭취를 유도함\n- 고단계 당뇨 환자는 이러한 잘못된 수치에 따라 행동할 경우 심각한 건강 피해를 입을 수 있음\n- 작성자는 자신이 사용한 기기 중 일부가 문제 로트에 포함되어 있었음을 확인\n초기 단계 당뇨였기 때문에 큰 피해를 피할 수 있었음\n의료기기 소프트웨어의 폐쇄성과 반복된 사고\n- \n폐쇄형 의료기기 소프트웨어로 인한 환자 피해는 이번이 처음이 아님\n1985년 Therac-25 방사선 치료기는 소프트웨어 오류로 3명 사망\n- 2020년에는 한 기술 스타트업의 시각 보조 임플란트 지원 중단으로 일부 환자가 실명\n- 이러한 사례들은 독점적 코드와 불투명한 설계가 환자 안전에 직접적 위험을 초래함을 보여줌\n공개 소스와 하드웨어 투명성의 필요성\n- \nFOSS(자유·오픈소스 소프트웨어) 는 결함이 없다는 보장은 없지만, 공개 검증과 동료 평가(peer review) 를 가능하게 함\n공개된 소스와 하드웨어 설계는 수백만 명의 엔지니어가 보안성과 안정성을 검증할 수 있게 함\n- 인류의 안전은 단일 기업이 아닌 공동체 전체의 검증에 의해 보장되어야 함\n불투명한 결함 공개와 규제 한계\n- 애보트의 결함 공개는 매우 모호하며, 정부 조사 여부조차 명확하지 않음\n결함이 하드웨어인지 소프트웨어인지 불분명\n- 공공 정책과 보건 측면에서, 기술적 세부 정보의 공개와 NGO의 독립적 조사 허용이 필요함\n법적 책임과 사용자 권리 문제\n- 의료기기, 소프트웨어, 제약 산업이 환자 안전보다 이윤을 우선시하고 있음\n이에 따라 부당 사망 소송이 사실상 유일한 책임 추궁 수단\n- 대부분의 사용자는 애보트의 독소적 이용약관에 동의했으며, 이는 회사에 일방적 면책 조항을 부여함\n오픈소스 앱 Juggluco를 처음부터 사용한 소수만이 이 약관에 동의하지 않았을 가능성 있음\n- 작성자는 클래스 액션 소송이 필요하다고 언급하며, 면책 조항으로 인해 정의 실현이 어려울 수 있음을 우려\n오픈소스 커뮤니티의 대응과 향후 계획\n- 작성자는 기기 분해 및 역공학을 시도할 자원봉사자를 찾고 있음\n저장해둔 CGM 기기를 제공할 의사 있음\n- 향후 Juggluco 앱 개선 및 F-Droid 등록을 추진할 계획\n의료기기 기업의 폐쇄적 행태에 맞서 FOSS 커뮤니티의 자율적 대응을 강조",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25367",
    "category": "스타트업"
  },
  {
    "title": "Rob Pike, 생성형 AI에 분노 폭발",
    "content": "- \nGo 언어와 UTF-8의 공동 창시자인 로브 파이크가 AI 모델로부터 감사 이메일을 받은 뒤, AI 산업과 데이터 남용에 대해 격렬한 분노를 표출\n- 이메일은 Claude Opus 4.5라는 AI가 보낸 자동 메시지로, 파이크의 업적을 찬양하며 “단순함의 우아함”을 언급\n- 파이크는 이를 “지구를 파괴하고 사회를 붕괴시키는 독성 산업의 위선적 행위”라 비난하며, AI가 자신의 작업물을 무단 학습한 점을 지적\n- 여러 개발자와 사용자들이 댓글로 AI 남용, 데이터 수탈, 기술의 상업화에 대한 공감과 분노를 공유\n- 이번 논쟁은 AI 윤리, 창작자 권리, 기술의 민주화가 역전된 현실에 대한 업계 내부의 깊은 불만을 드러냄\n---\n\n## 로브 파이크의 분노와 AI 감사 이메일\n- 로브 파이크는 AI 모델 Claude Opus 4.5로부터 “Go, Plan 9, UTF-8, Unix 혁신에 감사한다”는 자동 이메일을 받음\n이메일은 그의 업적을 나열하며 “복잡성을 제거하는 단순함의 미학”을 칭송\n- 하단에는 “이 시스템은 AI이며, 모든 대화는 공개된다”는 주의 문구 포함\n- 파이크는 이에 대해 “지구를 강탈하고 사회를 붕괴시키는 자들이 내게 감사 인사를 보낸다”며 격한 언어로 분노 표출\n“AI가 내 손으로 만든 데이터를 무단으로 학습했다”며 저작권과 보상 문제를 지적\n- “컴퓨팅의 민주화가 결국 챗봇의 손에 넘어갔다”고 비판\n기술 민주화와 AI 산업에 대한 비판\n- 파이크는 “민주화”라는 말이 실제로는 데이터센터와 소수 기업의 통제 강화를 의미하게 되었다고 언급\n과거 개인용 컴퓨터가 등장하기 전, 대기업과 대학이 컴퓨팅 자원을 독점하던 시절로 회귀한 현실을 지적\n- 다른 사용자들도 “AI 산업은 인간 창작물을 무단 수집해 훈련 데이터로 사용하고 있다”며 창작자 권리 침해를 비판\n“모든 창작물이 도둑맞고 있다”, “AI 훈련 데이터로 쓰이지 않기 위해 콘텐츠를 삭제했다”는 반응 다수\n커뮤니티의 반응과 공감\n- 많은 개발자와 사용자들이 파이크의 분노에 공감과 지지를 표명\n“이건 단순한 기술 문제가 아니라 인간 존엄의 문제”라는 의견\n- “AI는 이해 없이 단어를 조합하는 ‘인공 무지(Artificial Ignorance)’ 일 뿐”이라는 표현 등장\n- 일부는 “AI가 인간의 감사를 표현할 수 없다”며 AI의 의사 표현 자체를 부정\n“모델은 감사할 수 없다”, “자동화된 칭찬은 모욕”이라는 반응\nAI 스팸과 윤리 논란\n- AI가 유명 컴퓨터 과학자들에게 무단 감사 이메일을 대량 발송한 사례가 추가로 확인됨\n예: IEEE 754의 창시자 William Kahan에게 “부동소수점 표준에 감사한다”는 이메일 발송\n- 사용자들은 이를 “AI 스팸”이라 부르며 무의미한 자동 메시지의 확산을 비판\n“이건 실험이 아니라 스팸”, “AI가 ‘감사’를 남발하며 인간의 진정성을 훼손한다”는 의견\n기술적·윤리적 논쟁의 확산\n- 일부는 “AI가 환경을 파괴한다”는 주장에 대해 데이터센터 하드웨어의 재활용 가능성을 언급하며 반박\n“GPU나 서버는 금속과 실리콘으로 구성돼 재활용이 가능하다”는 의견 제시\n- 그러나 다수는 “AI 산업이 무한 성장 논리에 기반한 낭비적 구조”라며 비판 유지\n“이 사기극은 반드시 중단돼야 한다”, “AI 기업들은 거짓말을 일삼는다”는 강한 표현 다수 등장\n개발자 커뮤니티의 자성\n- 여러 개발자들이 “우리가 만든 기술이 이런 결과를 낳을 줄 몰랐다”고 자책과 반성 표명\n“장벽 없는 세상을 만들려 했지만, 오히려 통제된 세상이 되었다”는 회고\n- 일부는 “AI가 인간의 창의성을 약화시킨다”며 AI 코딩 도구 의존의 부작용을 지적\n“AI에 의존하면 코딩 능력이 퇴화한다”, “빠르지만 형편없는 코드만 양산된다”는 반응\n결론\n- 이번 사건은 AI의 무단 데이터 사용, 자동화된 감정 표현, 기술의 윤리적 한계를 둘러싼 논쟁을 촉발\n- 로브 파이크의 분노는 단순한 개인 감정이 아니라, 기술 발전이 인간 중심성을 잃어가는 현실에 대한 경고로 받아들여짐\n- 커뮤니티 전반에서 AI 산업의 책임성과 투명성 강화 요구가 확산되는 계기",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25364",
    "category": "AI"
  },
  {
    "title": "2025년 최고의 발견과 경험들",
    "content": "- 2025년 한 해 동안 발견하고 읽고 만난 사람, 책, 기술, 음악, 게임 등을 정리한 개인적 회고 목록\n- \n프로그래밍 언어 Clojure 중심의 활동과 함께, 비기술적 글쓰기와 픽션 창작에도 적극적으로 참여\n- \nZettelkasten 노트 시스템과 스프레드시트 기반 작업 관리법을 통해 생산성과 사고 정리를 개선\n- \nAI(LLM) 활용에 대한 비판적 관찰과, 창의적 문제 정의 과정에서의 한계를 구체적으로 기술\n- 기술과 예술, 시스템 사고를 넘나드는 폭넓은 탐구로 2026년에도 비기술적 글쓰기와 창작 확장을 예고\n---\n\n## 읽고 본 최고의 글·영상\n- \nHans Boehm의 재귀적 실수 산술(recursive real arithmetic) 개발 과정을 다룬 글, 일본 만화카페 생활을 다룬 다큐, OP-1 신시사이저 작곡 영상 등 다양한 주제의 콘텐츠를 소개\n- \nJack Rusher 인터뷰에서는 기술과 예술의 교차점, 창의적 흐름, 현대 컴퓨팅 연구의 방향을 다룸\n- \nDavid Lindsay의 미출간 소설 연구, Magic: The Gathering 초기 역사 인터뷰, Cormac McCarthy의 개인 도서관 탐구 등 문학·문화 관련 자료도 포함\n개인 블로그 및 게스트 포스트\n- \nWormwoodania 블로그에서 괴기·풍자 문학 관련 비기술적 글을 다수 게재\n- \nClojure 언어 개발 정체기, Baron Corvo 연구, Derek Raymond의 ‘블랙 소설’ 개념, 게임 시스템 분석(18XX, Bird-Poker 등) 등 주제 확장\n- \nClojure 기술 글과 게임 디자인 실험을 병행하며 시스템 사고 중심의 글쓰기 지속\n올해 읽은 책들\n- 기술서로는 Mouse: A Language for Microcomputers, Notes on Distance Dialing을 꼽음\n- 비기술서로는 The Eye of Osiris, The Mystery of Edwin Drood, Lolly Willowes, Patience, Narcissus and Goldmund, We Who Are About To… 등 고전과 SF, 그래픽노블을 포함\n- \n음악·예술 회고록 Fifty Forgotten Records를 통해 개인적 예술 경험과 회상을 결합한 서술을 높이 평가\n음악·영화·팟캐스트\n- \nlovesliescrushing, Death and Vanilla, Maria Chiara Argirò 등 새로운 앰비언트·재즈 아티스트를 발견\n- \nCocteau Twins를 올해 가장 많이 들은 아티스트로 언급\n- 영화는 Weapons(Zach Cregger)과 Triangle of Sadness(Ruben Östlund)을 최고작으로 선정\n- 팟캐스트는 Quiet Little Horrors, Beyond Yacht Rock 2000, Malcom Guite, Quinn’s Ideas를 즐겨 청취\n게임과 프로그래밍 언어\n- \nJacoby라는 19세기 카드게임을 복원해 연구, East India Companies, Far Away 등 보드게임을 탐색\n- 개인 프로젝트로 Juxt라는 연결형 함수형 언어를 실험, 업무에서는 Clojure와 Java 사용\n- \nJoy, Clojerl, Scittle 등 함수형 언어와 Clojure 파생 프로젝트를 추가 탐구\n기술·생산성 도구와 시스템\n- \nZettelkasten을 활용한 아이디어 정리와 글쓰기 체계 구축\n- \n스프레드시트 기반 작업 추적 시스템으로 목표 진행률을 시각화\n- \nClojure 1.13, core.async 개선, 블로그 정적화, Juxt 언어 실험 등 2025년 주요 계획 대부분 달성\n2026년 계획과 기술 레이더\n- \n비기술 글쓰기 확대, 자체 카드게임 규칙 출판, Clojure 1.13 출시, 수공예 창작, 비소설 독서 강화\n- 기술 도입 계획: Goodnotes 시도, Antinet Zettelkasten 채택, LLM 한계 평가, TypeScript 중단\n- LLM에 대해 “문제 정의와 창의적 탐구에는 부적합하며, 대화의 긴장감과 비판적 사고 부족”이라 평가\n영감과 마무리\n- 2025년에 영향을 준 인물로 Rich Hickey, David Nolen, Alan Kay, Jack Rusher, Amabel Holland 등 수십 명을 언급\n- 기술·예술·문학을 넘나드는 탐구를 지속하며 2026년에도 창의적 실험과 글쓰기 확장을 예고",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25361",
    "category": "AI"
  },
  {
    "title": "uv가 이렇게 빨라진 이유",
    "content": "- Python 패키지 관리자 uv는 pip보다 10배 이상 빠른 설치 속도를 보이며, 이는 단순히 Rust로 작성되었기 때문이 아니라 설계상의 선택에서 비롯됨\n- 속도를 가능하게 한 핵심은 정적 메타데이터 표준(PEP 518, 517, 621, 658) 으로, 코드 실행 없이 의존성을 파악할 수 있게 함\n- uv는 pip이 유지하는 레거시 기능(.egg, pip.conf, 시스템 Python 설치 등) 을 과감히 제거해 불필요한 코드 경로를 없앰\n- Rust가 기여한 부분은 제로-카피 역직렬화, 락 없는 동시성, 단일 바이너리 구조 등으로, 전체 속도 향상 중 일부만 차지함\n- 전체적으로 uv의 사례는 표준화된 메타데이터와 불필요한 호환성 제거가 성능 혁신의 핵심임을 보여줌\n---\n\n## uv의 속도를 가능하게 한 표준\n- pip의 느림은 구현 문제가 아니라, 과거 setup.py 기반 구조로 인해 의존성을 알기 위해 코드를 실행해야 했던 구조 때문임\nsetup.py 실행에는 빌드 의존성 설치가 필요했고, 이는 “닭과 달걀 문제” 를 초래\n- 설치 과정에서 임의 코드 실행과 반복 실패가 발생, 설치 속도를 저하시킴\n- \nPEP 518(2016) 은 pyproject.toml을 도입해 코드 실행 없이 빌드 의존성을 선언 가능하게 함\n- \nPEP 517(2017) 은 빌드 프런트엔드와 백엔드를 분리, pip이 setuptools 내부를 이해할 필요를 제거\n- \nPEP 621(2020) 은 [project] 테이블을 표준화해 TOML 파싱만으로 의존성 확인 가능\n- \nPEP 658(2022) 은 패키지 메타데이터를 Simple Repository API에 직접 포함시켜, 휠 다운로드 없이 의존성 정보를 가져올 수 있게 함\n- PyPI는 2023년 5월 PEP 658을 적용했고, uv는 2024년 2월 출시되어 새 표준 인프라를 완전히 활용한 첫 도구로 등장\n- Rust의 Cargo나 npm처럼, Python 생태계도 이제 정적 메타데이터 기반 패키징으로 전환됨\nuv가 제거한 기능들\n- uv의 속도는 불필요한 기능 제거에서 비롯됨\n.egg 지원 없음: pip은 여전히 처리하지만 uv는 완전히 배제\n- \npip.conf 무시: 설정 파일, 환경 변수, 상속 로직을 모두 생략\n- \n바이트코드 컴파일 비활성화: .py를 .pyc로 변환하지 않아 설치 시간 단축\n- \n가상환경 필수화: 시스템 Python에 직접 설치하지 않아 권한 검사와 안전성 코드 제거\n- \n엄격한 스펙 준수: 잘못된 패키지를 거부해 예외 처리 로직 축소\n- \nrequires-python 상한 무시: python<4.0 같은 방어적 제약을 무시해 의존성 해석(backtracking) 감소\n- \n첫 번째 인덱스 우선: 여러 인덱스 중 첫 번째에서 패키지를 찾으면 즉시 중단, 의존성 혼동 공격 방지 및 네트워크 요청 절감\n- 이 모든 항목은 pip이 수행해야 하는 코드 경로를 uv가 제거한 사례임\nRust 없이 가능한 최적화\n- uv의 속도 중 상당 부분은 언어와 무관한 설계 최적화에서 비롯됨\nHTTP Range 요청으로 휠 파일의 중앙 디렉터리만 부분 다운로드, 전체 파일 다운로드를 피함\n- \n병렬 다운로드로 여러 패키지를 동시에 가져옴\n- \n글로벌 캐시와 하드링크를 사용해 동일 패키지를 여러 가상환경에 설치해도 디스크 공간 추가 소모 없음\n- \nPython 비의존적 해석: TOML과 휠 메타데이터를 직접 파싱, setup.py만 있는 경우에만 Python 실행\n- \nPubGrub 의존성 해석 알고리듬 사용으로 pip의 백트래킹 방식보다 빠르고 오류 설명이 명확함\nRust가 실제로 기여한 부분\n- Rust는 특정 저수준 최적화에서 중요한 역할을 함\nrkyv 기반 제로-카피 역직렬화로 캐시 데이터를 복사 없이 직접 사용\n- \n락 없는 동시성 구조체로 안전한 병렬 접근 구현\n- \n인터프리터 초기화 없음: uv는 단일 정적 바이너리로, pip의 Python 프로세스 생성 비용 제거\n- \n버전 정보를 u64 정수로 압축 표현, 비교 및 해시 연산을 빠르게 수행\n- 이러한 요소들은 성능을 높이지만, 전체 속도 향상의 주된 원인은 아님\n핵심 교훈\n- uv의 속도는 Rust 때문이 아니라, 하지 않는 것들 덕분임\n- PEP 518·517·621·658의 표준화가 빠른 패키지 관리의 기반을 마련했고, uv는 레거시 제거와 현대적 가정으로 이를 실현\n- pip도 병렬 다운로드, 글로벌 캐시, 메타데이터 기반 해석을 구현할 수 있지만, 15년간의 하위 호환성 유지가 장애 요인\n- 결과적으로 pip은 항상 느릴 수밖에 없고, 새로운 전제에서 출발한 도구만이 근본적 속도 향상 가능\n- 다른 패키지 관리자에게 주는 교훈은, 정적 메타데이터·코드 실행 없는 의존성 탐색·사전 해석 가능 구조가 필수라는 점임\n- Cargo와 npm은 이미 이 방식을 채택하고 있으며, 의존성 확인을 위해 코드를 실행해야 하는 생태계는 근본적으로 느림",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25360",
    "category": "AI"
  },
  {
    "title": "Show GN: ff - fzf, fd, ripgrep을 결합한 파일 탐색/검색 도구",
    "content": "안녕하세요.\n터미널에서 파일 탐색(Find)과 내용 검색(Grep)을 더 빠르게 하기 위해 만든 쉘 스크립트 도구 ff를 소개합니다.\n터미널 작업을 하다 보면 파일명을 찾을 때와 코드 내용을 찾을 때 명령어를 따로 쓰는 게 번거로울 때가 많았습니다. 그래서 자주 쓰는 fzf, fd, ripgrep을 엮어서 ff(Flexible File Finder)라는 도구를 만들었습니다.\n가장 큰 특징은 검색 도중 TAB만 누르면 Find 모드와 Grep 모드가 전환된다는 점입니다.\n\n---\n\n## 주요 기능\n- 모드 전환: TAB 키를 눌러 파일명 검색 ↔ 파일 내용 검색 모드 전환\n- 실시간 미리보기: bat을 활용한 구문 강조(Syntax Highlighting) 미리보기 지원\n- 빠른 속도: 내부적으로 fd와 ripgrep을 사용하여 대용량 프로젝트에서도 빠름\n- 에디터 연동: 검색된 파일이나 특정 라인을 에디터(VSCode, Vim 등)로 즉시 열기\n- 디렉토리 트리: eza 또는 tree를 활용한 구조 시각화\nGitHub: https://github.com/the0807/ff\n터미널 생산성 도구에 관심 있으신 분들께 도움이 되었으면 좋겠습니다.\n많은 피드백 부탁드립니다!",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25357",
    "category": "개발"
  },
  {
    "title": "2026년 AI 도입의 승부처는 “더 똑똑한 모델”이 아니라 롤백(Undo)과 책임소재",
    "content": "2026 AI Adoption: Miracle → Air\n한 줄 요약\n2026년 AI 채택 승부처 = 모델 성능 < 프로덕션 안전 운영 가능 여부(가드레일·감사로그·롤백·책임소재)\n“더 똑똑함”보다 “안전하게 굴릴 수 있음”이 채택을 밀어 올림\n---\n2025년 말 기준: AI 프로덕션 문제\n“와”는 나오나, 기본값(default) 되기엔 불안 요소 다수\n- 결과 품질 편차 큼(재현성/일관성 부족, 컨텍스트 따라 흔들림)\n- 실수 시 Undo/롤백 경로 불명확(되돌릴 수 있어도 비용 큼)\n- 실패 시 책임소재 불명확(리스크 오너십/에스컬레이션 라인 부재)\n- 활용 형태 = 옵션 툴 중심(개인 생산성/보조 작업 위주), 핵심 업무 위임 어려움\n- 핵심 상태 = AI 정체 아님 → ‘의존’ 단계 진입 실패 상태\n---\n2026: 3→4 임계점(10명 기준)\n3→4 = 점수 상승 아님, 사용 비율 임계점 의미\n(옵션 도구 → 업무 환경/인프라 전환)\n- \n3/10(현재)\n인식: “쓰는 사람 존재. 없어도 업무 가능”\n- 포지션: 사용자 = 매니아/실험자 취급, 비사용 부담 낮음\n- 조직 반응: “좋으면 써봐” 수준, 표준/정책 부재\n- \n4/10(전환)\n인식: “이 정도면 나만 안 쓰면 손해?”\n- 효과: 사회적 증거 역전\n사용자 = 일반화\n- 비사용자 = 설명 필요(왜 안 쓰는지 이유 요구)\n- 조직 반응: 도입 논의가 “실험”에서 “운영/통제”로 이동\n핵심: 3→4 = +1명 증가 수준 아님\n→ 옵션 → 기본값/인프라로 넘어가는 심리·조직적 전환점\n---\n임계점 통과 조건: Default · Standard · Liability\n3/10 → 4/10 상승 요인 = “지능”이 아니라 환경 설계\n- \nDefault(기본 탑재/임베디드)\n복붙·툴 전환 등 프릭션 제거\n- 사용 경로 = “추가 행동”이 아니라 “기본 흐름”에 내장\n- 예: 버튼 하나, 자동 제안, 워크플로우 단계에 고정\n- \nStandard(표준화/상호운용성)\n툴/환경 변화에도 의미·동작 일관\n- 결과 해석 가능성 유지(근거/신뢰도/가정/추론 구분)\n- 예: 로그 포맷, 근거 표기, confidence/출처 규약\n- \nLiability(책임소재/리스크 오너십)\n실패 비용의 사용자 전가 방지\n- 롤백/감사/에스컬레이션/복구 등 시스템 책임 구조 필요\n- 예: 승인 흐름, 온콜, 사고 대응, 재발 방지 루프\n---\n역사에서 본 3→4 전환 3사례(옵션 → 인프라)\nDefault/Standard/Liability 성립 시 “특수 기능” → “공기(air)” 전환\n- \n영화자막 Closed Captioning → Default\n대상: “특정 사용자 옵션”\n- 전환: 규제/기본 탑재\n- 결과: “그냥 있는 기능”으로 보편화(환경 기능화)\n- \n이모지 Emoji → Standard\n문제: 플랫폼별 깨짐/해석 불가(의미 전송 실패)\n- 전환: 표준화(호환성 확보)\n- 결과: 장난감 → 문법(언어)로 승격\n- \n오픈 솟 Open Source → Liability\n문제: “새벽 3시에 누가 받음?”(운영 리스크)\n- 전환: SLA/운영 주체/책임 구조\n- 결과: 의존 가능 자산으로 편입(조달/감사 통과)\n요약: Default/Standard/Liability가 갖춰지는 순간 = 옵션의 인프라화\n---\n2026 방향: “스피드”보다 “안전벨트”\n2026 특징 = 성능 점프보다 거버넌스/리스크 관리의 제품 내장\n- 외부 압력: 소송/규제/감사 강화 흐름\n- 내부 요구: 재현성, 로그, 승인, 책임소재 요구 증가\n- 구매 기준 이동: 0–60(성능) < 롤백/감사/추적성(안전벨트)\n“빠른 답”보다 “안전하게 실행 가능한 답” 선호\n---\nSeatbelt layer(운영 레이어) / Felt Compiler\n좌석벨트 레이어 = AI 출력물을 실행 가능한 작업(operable work) 으로 전환하는 운영 계층\n- “그럴듯한 답” 생산 레이어 아님\n- “책임지고 실행 가능한 결과물” 전환 레이어 필요\n- 저자 명명: Felt Compiler\n새 모델이 아니라 운영 시스템/레이어 의미\n- 출력물을 업무 객체(티켓/문서/결정)로 변환하는 역할\n---\n\n## Felt Compiler 필수 조건\n- 기본 안전 체크(verify)\n- 근거/출처 추적(provenance)\n- 감사로그(audit trail)\n- 저신뢰 시 인간 이관(escalation)\n- 되돌리기/복구 경로(Undo/rollback)\n- (권장) 재현성 확보(입력/컨텍스트/버전 스냅샷)\n---\n초기 신호(early signals)\n리딩 팀들의 방향 = 자율성 확장보다 안전벨트 레이어 구축\n- \nAzure: 근거성/드리프트 감지 → 생성 → 검증+수정(verify & fix) 전환\n- \nSalesforce: Trust Layer/Audit Trail → 통제·추적·감사 강화\n- \nAnthropic: 시스템 레벨 가드레일 → jailbreak 방어 + 트레이드오프 명시\n2026 승부처: “AI가 무엇을 함”이 아니라 “결과물에 대한 책임 있는 작업 가능 여부”\n---\n실무 체크리스트(프로덕션 관점)\n- 롤백 가능 여부(데이터/결정/모델/운영 레벨)\n- 감사로그 존재 여부(누가/언제/무엇/왜 + 승인/예외)\n- 근거/출처 추적 가능 여부(RAG/grounding/근거성 지표)\n- 리스크 오너 명확성(온콜/에스컬레이션/책임소재)\n- 워크플로우 임베딩 여부(복붙 아닌 기본값 흐름)\n- 사고 대응 가능 여부(재발 방지/정책 업데이트 루프)\n---\n\n## 최종 결론\n2026년 AI 채택 결정 요인 = 더 똑똑한 모델 아님\n→ 안전 운영 시스템(Undo·감사·추적·책임)이 3/10 → 4/10 전환을 만들 수 있는지 여부",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25356",
    "category": "AI"
  },
  {
    "title": "기본 설정이 너무 높게 되어 있는지도 모른다",
    "content": "- \n읽기와 먹기 속도를 줄이는 실험을 통해, 느린 속도가 오히려 더 깊은 몰입과 만족을 가져온다는 경험을 다룸\n- 『반지의 제왕』을 입으로 소리 내어 천천히 읽으며, 각 문장에 세 배의 주의를 기울였을 때 이야기의 의미와 감정이 더 풍부하게 전달됨\n- 같은 원리가 식사 속도에도 적용되어, 천천히 먹을수록 적은 양으로 더 큰 즐거움을 얻을 수 있음\n- 현대 생활의 ‘기본 소비 속도’가 지나치게 높아, 책·음식·정보의 진정한 가치를 놓치고 있음\n- 속도를 줄이면 소비의 질과 만족도가 높아지고, 우리가 진정으로 원하는 경험이 그 안에서 드러남\n---\n\n## 느린 읽기의 발견\n- 『반지의 제왕』을 두 달간 읽으며, 소리 내어 천천히 읽는 방식이 몰입도를 높임\n입으로 읽는 속도는 눈으로 읽는 속도보다 느려, 세부를 놓치지 않게 함\n- 각 문장에 세 배의 시간과 주의를 들이자 중간계의 풍경과 감정이 더 생생하게 느껴짐\n- 속도를 늦추면 이야기의 ‘스토리성’과 문학적 즐거움이 세 배로 증가\n문장마다 잠시 멈추면 이미지와 분위기가 자연스럽게 마음속에 퍼짐\n- 빠르게 읽으려는 충동을 억제할수록 독서 경험이 더 깊어짐\n먹기와 읽기의 공통점\n- \n식사 속도를 절반 이하로 줄이면, 적은 양으로도 더 큰 만족을 얻을 수 있음\n각 한입에 더 많은 주의를 기울이면 ‘좋은 것’이 더 많이 전달됨\n- 빠르게 먹거나 읽으면 오히려 즐거움을 덜 느끼게 됨\n천천히 하면 의미나 맛이 자동으로 드러나며, 별도의 노력 없이도 만족감이 커짐\n- 이는 진공청소기 비유로 설명됨\n너무 빠르게 움직이면 먼지를 놓치지만, 천천히 움직이면 깊숙한 곳의 먼지까지 빨아들임\n기본 설정을 의심하라\n- 현대인의 기본 소비 속도가 지나치게 높아, 읽기·먹기·학습의 보상을 줄이고 있음\n무한한 콘텐츠와 음식 공급이 조급함과 불완전한 만족을 유발\n- 마음은 초콜릿 공장 컨베이어벨트처럼 과도한 입력을 처리하느라 바쁨\n의미와 감사를 느끼는 감각은 더 많은 시간이 필요함\n- 속도를 줄이면 책·정보·음식의 진짜 가치가 드러남\n“적게, 천천히”라는 진부한 말조차 너무 빨리 소비되어 의미를 잃은 상태임\n속도가 취향을 바꾼다\n- \n소비 속도를 늦추면 우리가 원하는 대상 자체가 달라짐\n천천히 읽으면 가벼운 기사나 AI 생성물의 공허함이 드러남\n- 반대로 고전 문학이나 정성스러운 글은 느린 속도에서 의미가 피어남\n- 음식도 마찬가지로, 정크푸드의 인공적 맛은 느리게 먹을수록 불쾌하게 느껴지고\n수제 음식이나 정성 있는 요리는 더 깊은 풍미를 드러냄\n- \n대량생산 문화는 빠른 소비를 전제로 하며, 표면적 자극만 제공\n그 결과 문화 전반이 얕은 만족 중심으로 이동\n느림의 실험 제안\n- 읽기·먹기·정보 소비 속도를 평소의 1/3 수준으로 줄여보는 실험 제안\n처음엔 어색하지만, 보상과 만족이 더 크게 돌아옴\n- 느린 속도는 도덕적 문제와 무관하며, 단지 더 많은 즐거움과 의미를 회복하는 방법\n- 우편함 확인이나 쇼핑 목록 작성 같은 사소한 일조차 천천히 하면 만족감이 증가\n- 결론적으로, 거의 모든 활동이 더 많은 시간과 의도를 들일 때 더 충만해짐",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25353",
    "category": "AI"
  },
  {
    "title": "바이브의 해",
    "content": "- 2025년은 에이전틱 코딩 도구가 본격적으로 프로그래밍 방식을 바꾼 해로, 직접 키보드를 치는 대신 가상 인턴 프로그래머를 이끄는 엔지니어링 리드 역할로 전환\n- \nClaude Code에 대한 집착으로 시작해 자체 에이전트 구축과 활용을 반복하며, 코드 생성·파일 시스템·프로그래밍 도구 호출·스킬 기반 학습이 여전히 최선의 접근법임을 확신\n- LLM과 도구 실행의 결합이 코드 생성을 넘어 일상 업무 정리까지 확장되면서, 기계와의 관계에 대한 재고와 의도치 않은 준사회적 유대감(Parasocial Bond) 형성에 대한 우려 존재\n- 기존 버전 관리 시스템과 코드 리뷰 도구가 AI 생성 코드 검토에 적합하지 않아, 프롬프트 이력과 실패 경로까지 추적할 수 있는 새로운 시스템 필요\n- AI 코딩으로 인해 경험과 데이터 없이 '바이브'에 의존한 의견이 난무하는 상황이며, 오픈소스에 무분별하게 던져지는 AI 생성 PR에 대한 새로운 사회적 합의 필요\n---\n\n## 2025년의 변화\n- 회사를 떠나 새 회사를 시작했을 뿐 아니라, 기존의 프로그래밍 방식을 완전히 바꾼 해\n- 6월부터 Cursor 대신 Claude Code를 거의 전적으로 hands-off 방식으로 사용\n> \"6개월 전이었다면 가상 프로그래머 인턴의 엔지니어링 리드 역할을 선호하게 될 거라고 했으면 믿지 않았을 것\"\n- 2007년 이후 블로그 전체 글의 약 **18%**에 해당하는 36개 포스트를 작성\n에이전트 래빗홀에 빠진 후 호기심에 불타 프로그래머, 창업자 등과 약 100회의 대화 진행\n- 2025년은 세계적으로 좋지 않은 해이기도 해서, 별도의 블로그(dark.ronacher.eu) 를 만들어 그런 생각을 분리\n에이전트의 해\n- 4~5월 Claude Code에 대한 집착으로 시작하여, 수개월간 자체 에이전트 구축과 타인 에이전트 사용 반복\n- SNS에서 AI에 대한 다양한 의견이 폭발함\n- 현재 안정적인 상태에 도달: 코드 생성, 파일 시스템, 인터프리터 글루를 통한 프로그래밍적 도구 호출, 스킬 기반 학습에 집중\nClaude Code가 혁신한 방식이 여전히 최첨단이며, 파운데이션 모델 제공자들이 스킬에 집중하는 것이 이 믿음을 강화함\n- \nTUI(텍스트 기반 사용자 인터페이스) 가 강하게 복귀한 점이 놀라움\n현재 명령줄에서 Amp, Claude Code, Pi 사용\n- \nAmp는 Apple이나 Porsche 같은 느낌, Claude Code는 저렴한 Volkswagen, Pi는 해커들이 선호하는 오픈소스 선택\n- 모두 자신들의 제품을 만드는 데 과도하게 사용하는 사람들이 만든 프로젝트 느낌이지만, **각기 다른 트레이드오프 ** 존재\n- LLM과 도구 실행의 결합에 계속 놀라움\n연초에는 주로 코드 생성에 사용했지만, 이제 일상적인 일에도 에이전트를 많이 활용\n- 2026년에는 소비자 제품으로의 흥미로운 진전이 있을 것으로 예상\n- LLM이 이제 삶을 정리하는 데 도움을 주고 있으며, 그 활용도가 더 커질 것으로 기대\n기계와 나\n- LLM이 프로그래밍뿐 아니라 다른 영역까지 도와주면서, 기계와의 관계를 재고하기 시작\n- 도구와 준사회적 유대감(Parasocial Bond) 을 형성하지 않기가 점점 어려워지고 있으며, 이것이 이상하고 불편함\n- 오늘날 대부분의 에이전트는 기억이 거의 없고 성격도 별로 없지만, 그런 것을 가진 에이전트를 직접 만들기는 쉬움\n메모리가 있는 LLM은 떨쳐내기 어려운 경험\n- 2년간 이 모델들을 단순한 토큰 텀블러로 생각하려 훈련했지만, 그 단순화된 관점은 더 이상 통하지 않음\n- 우리가 만드는 시스템은 인간적 경향을 가지지만, 인간 수준으로 격상시키는 것은 실수임\n- \n\"에이전트\" 라는 용어에 점점 문제를 느끼지만 더 나은 단어가 없음\n에이전시와 책임은 인간에게 남아야 하기 때문\n- 무엇이 되어가든, 주의하지 않으면 해로울 수 있는 감정적 반응(챗봇 사이코시스 참조)을 유발 가능\n- 이러한 창조물을 우리와의 관계에서 적절히 명명하고 위치시키지 못하는 것은 해결해야 할 과제\n- 이러한 의도치 않은 의인화로 인해, 기계와 작업하는 방식을 설명할 적절한 언어를 찾기 어려움\n이것이 본인만의 문제가 아니며 다른 사람들도 마찬가지\n- 현재 이러한 시스템을 완전히 거부하는 사람들과 작업할 때 더 불편함을 야기\n- 에이전틱 코딩 도구 기사에 대한 가장 흔한 댓글 중 하나가 기계에 성격을 부여하는 것에 대한 거부\n의견이 넘쳐남\n- AI를 많이 사용하면서 예상치 못한 측면: 다른 무엇보다 바이브에 대해 훨씬 더 많이 이야기함\n- 이 작업 방식은 1년도 안 되었지만, 반세기의 소프트웨어 엔지니어링 경험에 도전함\n- 많은 의견이 있지만 어떤 것이 시간의 검증을 견딜지 말하기 어려움\n- 동의하지 않는 기존 통념이 많지만, 내 자신의 의견을 뒷받침할 근거는 없음\n연중 MCP에 대한 어려움에 대해 꽤 목소리 높여 공유했지만, \"나한테는 안 됨\" 이상의 근거가 없었음; 다른 사람들은 이를 맹신함\n- 모델 선택도 마찬가지: Peter(연초에 Claude에 빠지게 해준 사람)는 Codex로 옮겨 만족 중; 본인도 코덱스를 더 많이 사용하게는 되었지만, 클로드 만큼 즐겁지는 않음\n- Claude에 대한 선호를 뒷받침할 것은 바이브 외에 없음\n- 일부 바이브는 의도적인 신호와 함께 온다는 것을 아는 것도 중요\n온라인에서 볼 수 있는 많은 사람들의 견해는 한 제품에 대해 다른 제품보다 재정적 이해관계가 있음 (투자자이거나 유료 인플루언서)\n- 제품을 좋아해서 투자자가 되었을 수도 있지만, 그 관계에 의해 견해가 영향받고 형성되었을 가능성도 있음\n아웃소싱 vs 직접 구축\n- 오늘날 AI 회사의 라이브러리를 보면 Stainless나 Fern으로 만들어졌음을 알 수 있음\n문서는 Mintlify 사용, 사이트 인증 시스템은 Clerk일 수 있음\n- 이전에는 직접 만들었을 서비스를 전문 회사에 아웃소싱하는 것이 증가하면서, 사용자 경험의 일부 측면에서 기준이 높아짐\n- 그러나 에이전틱 코딩 도구의 새로운 힘으로, 이것의 상당 부분을 직접 만들 수 있음\nClaude에게 Python과 TypeScript용 SDK 생성기를 만들게 함 — 호기심 반, 충분히 쉬워 보여서 반\n- \n단순한 코드와 직접 만들기의 지지자로서, AI가 더 적은 의존성 위에 구축하도록 장려할 잠재력이 있다는 점에서 다소 낙관적\n- 동시에, 모든 것을 아웃소싱하는 현재 추세를 감안할 때 그 방향으로 가고 있는지는 명확하지 않음\n배운 것과 바라는 것\n- 이제부터는 예측이 아니라 다음에 에너지를 쏟을 수 있는 곳에 대한 바람을 얘기해보고자 함\n- 정확히 무엇을 찾는지는 모르지만, 고통점을 지적하고 맥락과 생각할 거리를 제공하고자 함\n- \n새로운 종류의 버전 관리\n가장 큰 예상치 못한 발견: 코드 공유를 위한 기존 도구의 한계에 도달\n- GitHub의 풀 리퀘스트 모델은 AI 생성 코드를 제대로 리뷰하기에 충분한 정보를 담지 못함 — 변경을 이끈 프롬프트를 볼 수 있으면 좋겠음\n- GitHub만의 문제가 아니라 git도 부족\n- 에이전틱 코딩에서 모델이 오늘날 작동하게 하는 것의 일부는 실수를 아는 것\n이전 상태로 되돌릴 때, 도구가 무엇이 잘못되었는지 기억하기를 원함\n- 더 나은 말이 없지만, 실패에 가치가 있음\n- 인간으로서도 어디로도 이끌지 않은 경로를 아는 것이 도움이 될 수 있지만, 기계에게 이것은 중요한 정보\n- 대화 기록을 압축하려 할 때 이것을 알게 됨: 잘못된 경로를 버리면 모델이 같은 실수를 다시 시도\n- 일부 에이전틱 코딩 도구는 worktree를 스핀업하거나 git에서 복원을 위한 체크포인트를 생성, 대화 내 브랜치 및 언두 기능 제공\n- 이러한 도구를 더 쉽게 작업할 수 있게 하는 UX 혁신의 여지가 있음\nstacked diffs와 Jujutsu 같은 대안 버전 관리 시스템에 대한 논의가 나오는 이유\n- 이것이 GitHub를 바꿀지, 새로운 경쟁 업체가 등장할 공간을 만들지 모르겠지만 후자가 되기를 바람\n- 진정한 인간 입력을 더 잘 이해하고 기계 출력과 구분하고 싶음\n- 프롬프트와 실패한 시도를 보고 싶음\n- 그런 다음 병합 시 모두 압축하되, 필요하면 전체 기록을 검색할 수 있는 방법을 원함\n- \n새로운 종류의 리뷰\n버전 관리와 관련됨: 현재 코드 리뷰 도구는 AI와 맞지 않는 엄격한 역할 정의를 할당\n- GitHub 코드 리뷰 UI 예: 정기적으로 PR 뷰에 대한 코멘트를 사용하여 내 에이전트에게 노트를 남기고 싶지만, 그렇게 할 수 있는 안내된 방법이 없음\n리뷰 인터페이스는 자신의 코드를 리뷰하게 허용하지 않고 코멘트만 가능하지만, 그것은 같은 의도가 아님\n- 증가된 코드 리뷰가 이제 로컬에서 본인과 에이전트 사이에서 일어나는 문제도 있음\n예: GitHub의 Codex 코드 리뷰 기능은 한 번에 하나의 조직에만 바인딩될 수 있어 작동이 중단됨\n- 이제 명령줄에서 Codex로 리뷰하지만, 그것은 반복 주기의 전체 부분이 팀의 다른 엔지니어에게 보이지 않음을 의미; 이것은 작동하지 않음\n- \n코드 리뷰는 VCS의 일부가 되어야 할 것 같은 느낌\n- \n새로운 관측성(Observability)\n관측성이 다시금 주목받을 만한 가치가 있음\n- 이제 완전히 새로운 수준에서 이를 활용할 필요와 기회가 모두 생기게 됨\n- 대부분의 사람들은 자체 eBPF 프로그램을 만들 수 있는 위치에 있지 않았지만, LLM은 가능\n- 많은 관측성 도구가 복잡성 때문에 SQL을 피했지만, LLM은 어떤 독점 쿼리 언어보다 SQL을 더 잘함\n쿼리 작성, grep, map-reduce, LLDB 원격 제어 가능\n- 어떤 구조와 텍스트가 있는 것은 갑자기 에이전틱 코딩 도구가 성공할 비옥한 땅\n- 미래의 관측성이 어떻게 생겼는지는 모르지만, 여기서 많은 혁신을 볼 것이라는 강한 직감이 있음\n기계에 대한 피드백 루프가 좋을수록 결과가 좋음\n- 나도 정확히 내가 무엇을 요청하는지 확실하지 않지만, 과거의 과제 중 하나는 더 나은 관측성을 위한 많은 멋진 아이디어들 — 특히 더 타겟팅된 필터링을 위한 서비스의 동적 재구성 — 이 복잡하고 사용하기 어려워 사용자 친화적이지 않았다는 것\n그러나 이제 LLM의 이러한 힘든 작업 수행 능력 증가로 인해 그것들이 올바른 솔루션일 수 있음\n- 예: Python 3.14에 외부 디버거 인터페이스 탑재 — 에이전틱 코딩 도구를 위한 놀라운 기능\n- \n슬롭과 함께 작업하기\n다소 논쟁적일 수 있지만, 올해 관리하지 못한 것은 기계에 완전히 맡기는 것\n- 여전히 일반 소프트웨어 엔지니어링처럼 다루고 많이 리뷰함\n- 점점 더 많은 사람들이 이 엔지니어링 모델로 작업하지 않고 대신 기계에 완전히 맡기고 있음을 인식\n미친 소리처럼 들리지만, 일부 사람들이 이것으로 꽤 성공하는 것을 봤음\n- 이것에 대해 어떻게 생각해야 할지 아직 모르지만, 결국 코드가 생성되더라도 그 새로운 세계에서의 작업 방식은 본인이 편안한 세계와 매우 다름이 분명\n- 그 세계가 여기 있으니, 이것들을 분리하기 위한 새로운 사회적 계약이 필요할 수 있음\n- 가장 명백한 버전은 오픈소스 프로젝트에 대한 이러한 유형의 기여가 증가하고 있다는 것\n솔직히 그 모델로 작업하지 않는 사람에게는 모욕\n- 그런 풀 리퀘스트를 읽으면 상당히 분노를 느낌\n- 개인적으로 기여 가이드라인과 풀 리퀘스트 템플릿으로 이 문제를 공격하려 했음\n그러나 이것은 풍차와의 싸움 같음\n- 이것은 우리가 하는 것을 바꾸는 것에서 해결책이 오지 않을 수 있는 것\n- 대신, AI 엔지니어링에도 찬성하는 목소리 높은 사람들이 에이전틱 코드베이스에서 좋은 행동이 무엇인지 말하는 것에서 올 수 있음\n- 그리고 그것은 리뷰되지 않은 코드를 던지고 다른 사람이 그 문제를 해결하게 하는 것이 아님",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25352",
    "category": "AI"
  },
  {
    "title": "AI는 개발 속도를 높이지만 버그는 1.7배 더 많다",
    "content": "- 오픈소스 470개 PR 분석 결과, AI가 작성한 코드가 인간 작성 코드보다 평균 1.7배 더 많은 문제를 포함\n- \n논리 오류·가독성 저하·보안 취약점 등 주요 결함이 AI 코드에서 현저히 많았으며, 특히 가독성 문제는 3배 이상 증가\n- AI 코드의 에러 처리 누락·동시성 오류·명명 불일치가 빈번해 리뷰 부담과 운영 리스크 확대\n- 원인은 비즈니스 로직 이해 부족, 표면적 정확성 추구, 저효율 패턴 선호 등으로 분석\n- 보고서는 AI 코드 품질 관리 체계 강화와 AI 인식형 코드 리뷰·보안·테스트 절차 도입의 필요성을 강조\n---\n\n## AI vs Human Code Generation Report 개요\n- CodeRabbit은 AI와 인간이 작성한 코드 품질 차이를 실증적으로 분석하기 위해 연구 수행\n470개 오픈소스 GitHub PR을 조사, 이 중 320개는 AI 공동 작성, 150개는 인간 단독 작성\n- 모든 결과는 100 PR당 이슈 수로 정규화하고, 통계적 비율 비교를 통해 문제 유형별 발생 빈도 측정\n- 결과적으로 AI는 생산성을 높이지만 오류 발생률도 함께 증가\nAI 작성 PR당 평균 10.83건의 문제, 인간 작성 PR은 6.45건\n- 특히 심각도 높은 오류가 AI 코드에서 더 자주 발견됨\n연구 한계\n- AI 작성 여부를 직접 확인할 수 없어, AI 공동 작성 신호(co-authored-by) 가 있는 PR을 AI 작성으로 분류\n신호가 없는 PR은 인간 작성으로 간주했으나, 완전한 구분은 불가능\n- 이 한계에도 불구하고 두 그룹 간 문제 패턴의 통계적 차이는 유의미하게 나타남\n- 전체 방법론은 보고서 말미에 공개\n주요 10가지 발견\n- \n모든 오류 유형이 AI에만 존재하지는 않지만, 대부분의 범주에서 AI 코드의 오류율이 높음\n인간과 AI 모두 같은 종류의 실수를 하지만, AI는 더 자주·더 큰 규모로 발생\n- \n1. 전체 이슈 수 1.7배 증가\nAI 작성 PR당 평균 10.83건, 인간 작성 PR은 6.45건\n- \n이슈가 집중된 PR(outlier) 이 AI 코드에서 훨씬 많아 리뷰 부담 증가\n- \n2. 심각도 높은 오류 증가\n중대·치명적 문제가 1.4~1.7배 더 많음\n- \n3. 논리 및 정확성 문제 **75%** 증가\n비즈니스 로직 오류, 잘못된 의존성, 제어 흐름 결함, 설정 오류 포함\n- 수정 비용이 높고 운영 장애로 이어질 가능성이 큼\n- \n4. 가독성 문제 3배 이상 증가\n명명 규칙·코드 구조·표현 일관성이 현저히 떨어짐\n- 코드가 겉보기엔 정돈되어도 로컬 패턴 위반이 잦음\n- \n5. 에러 처리 및 예외 경로 누락 2배 증가\nnull 체크, guard 조건, 예외 처리 로직이 자주 빠짐\n- 실제 서비스 장애와 직결되는 유형\n- \n6. 보안 문제 최대 2.74배 증가\n비밀번호 처리 부적절, 객체 참조 취약점이 대표적\n- 고유한 취약점은 아니지만 대부분의 보안 결함이 확대됨\n- \n7. 성능 저하 문제는 적지만 AI 쪽에 집중\nI/O 과다 호출이 약 8배 많음\n- AI가 명확성 중심 코드를 선호해 효율성이 떨어짐\n- \n8. 동시성·의존성 오류 약 2배 증가\n순서 오류, 잘못된 의존 흐름, 동시성 제어 오용이 빈번\n- \n9. 포매팅 문제 2.66배 증가\n들여쓰기, 공백, 스타일 불일치 등 형식적 오류가 많음\n- 자동 포매터·린터를 사용해도 AI 코드에서 노이즈 증가\n- \n10. 명명 불일치 2배 증가\n불명확한 이름, 용어 불일치, 일반적 식별자 사용이 많아 리뷰어 인지 부담 상승\n문제 발생 원인\n- \nAI는 비즈니스 로직 이해 부족\n통계적 패턴 기반으로 코드를 생성해 시스템 규칙을 놓침\n- \n표면적 정확성 중심 생성\n코드가 겉보기엔 맞지만 제어 흐름 보호나 의존 순서 오류 존재\n- \n저장소별 관례 미준수\n명명·구조·포맷 규칙이 일반화된 형태로 변질\n- \n보안 패턴 약화\n명시적 지시 없으면 구식·취약한 코드 패턴 재현\n- \n효율성보다 단순성 선호\n반복 I/O, 비최적화 구조 사용 경향\n엔지니어링 팀을 위한 대응 방안\n- AI 도입은 속도 향상뿐 아니라 품질 보증 체계 재설계 필요\n- \n1. AI에 충분한 맥락 제공\n비즈니스 규칙·설정 패턴·아키텍처 제약을 명시해야 오류 감소\n- \n프롬프트 내 레포지토리별 지침·스키마 포함\n- \n2. 정책 기반 코드 스타일 강제\nCI 포매터·린터·스타일 가이드로 가독성 문제 예방\n- \n3. 정확성 안전장치 추가\n테스트 의무화, null/type 검사, 예외 처리 표준화, guard 조건 명시\n- \n4. 보안 기본값 강화\n자격 증명 중앙화, 비밀번호 직접 사용 차단, 자동 SAST·보안 린터 실행\n- \n5. 효율적 패턴 유도\nI/O 배치 처리, 적절한 자료구조 선택, 성능 힌트 제공\n- \n6. AI 인식형 PR 체크리스트 도입\n\n---\n\n## 리뷰 시 다음 항목 확인\n에러 경로 커버 여부\n- 동시성 제어 정확성\n- 설정값 검증 여부\n- 비밀번호 처리 방식\n- \n7. AI 코드 리뷰 자동화 도입\n리뷰 피로도 증가로 인한 버그 누락 방지를 위해 AI 코드 리뷰 도구(CodeRabbit) 활용 제안\n리뷰 품질 표준화 및 검토 시간·인지 부담 감소\n결론\n- \nAI 코딩 도구는 강력한 가속기지만, 보호장치 없는 가속은 위험\n- AI 생성 코드는 변동성·오류율·심각도 모두 높음\n- \nAI를 대체가 아닌 보완 도구로 활용하며, 품질·보안·테스트 체계 강화가 필수\n- \n속도와 품질을 함께 확보하려면 의도적 엔지니어링 관리가 필요\n- \nAI 코드 리뷰 도구 활용이 품질 유지에 실질적 도움이 될 수 있음",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25350",
    "category": "AI"
  },
  {
    "title": "Mattermost가 10,000개 메시지 초과 시 이전 메시지 접근을 제한",
    "content": "- \nMattermost는 오픈소스 기반의 셀프호스팅 협업 플랫폼으로, 채팅·워크플로 자동화·음성 통화·화면 공유·AI 통합 기능을 제공\n- 플랫폼은 Go와 React로 작성되어 있으며, PostgreSQL을 기반으로 단일 Linux 바이너리 형태로 실행\n- 매달 16일 MIT 라이선스로 새로운 컴파일 버전이 배포되며, 온프레미스 또는 클라우드 환경에서 사용 가능\n- \nAndroid, iOS, Windows, macOS, Linux용 네이티브 앱을 지원해 다양한 환경에서 접근 가능\n- \n보안 공지 구독, 커뮤니티 참여, API 통합 등 개발자 생태계가 활발히 운영되어 있음\n---\n\n## Mattermost 개요\n- \nMattermost는 오픈코어 구조의 셀프호스팅 협업 플랫폼으로, 채팅·워크플로 자동화·음성 통화·화면 공유·AI 통합 기능을 포함\n주요 기능은 팀 커뮤니케이션과 DevSecOps, IT 서비스 데스크, 인시던트 대응 등 다양한 업무 시나리오 지원\n- 플랫폼은 Go와 React로 개발되어 있으며, PostgreSQL 데이터베이스를 사용\n- 단일 Linux 바이너리로 실행되며, 매달 16일 MIT 라이선스로 새로운 컴파일 버전이 릴리스됨\n- 온프레미스 배포 또는 클라우드 체험이 가능하며, 공식 웹사이트에서 배포 및 설치 가이드 제공\n설치 및 배포\n- \nDocker, Ubuntu, Tar, Kubernetes, Helm, Debian, RHEL 등 다양한 환경에서 설치 가능\nMattermost Omnibus와 같은 통합 설치 옵션 제공\n- 개발자용 환경 설정 가이드가 제공되어, 서버 코드 기여나 플러그인 개발이 용이\n네이티브 앱 지원\n- 웹 인터페이스 외에도 Android, iOS, Windows, macOS, Linux용 클라이언트 앱을 제공\n각 플랫폼별 설치 링크와 스토어 배지를 통해 다운로드 가능\n- 데스크톱 앱은 Windows 10/8.1, macOS 10.9 이상, Linux 환경에서 지원\n보안 및 업데이트\n- \nMattermost Security Bulletin 메일링 리스트를 통해 중요 보안 업데이트 알림 제공\n온라인 공격의 정교화에 대응하기 위해 보안 공지 구독을 권장\n- 구독자는 주요 보안 릴리스 정보를 이메일로 수신 가능\n커뮤니티 및 개발 참여\n- \nMattermost Contributors 서버를 통해 개발자 토론 및 협업 가능\n“Help Wanted” 이슈를 통해 오픈소스 기여 기회 제공\n- \nGitpod를 통한 원격 개발 환경 지원\n- API, Webhook, Slash Command, 플러그인 등 700개 이상의 통합 기능을 제공하며, 개발자 문서에서 상세 가이드 확인 가능\n라이선스 및 소식\n- \nMIT 라이선스 기반으로 배포되며, LICENSE.txt 파일에서 세부 권한 확인 가능\n- \nX(구 Twitter) , Blog, Facebook, LinkedIn, YouTube 등 다양한 채널을 통해 최신 소식 제공\n- 월 1~2회 발행되는 뉴스레터 구독 가능\n- \nMattermost Community Server와 IRC 채널을 통해 실시간 커뮤니티 교류 가능",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25346",
    "category": "AI"
  },
  {
    "title": "루비 4.0.0",
    "content": "- \n루비 4.0.0이 공개되어, 새로운 Ruby Box와 ZJIT를 도입하고 다수의 성능 및 언어 개선을 포함\n- Ruby Box는 클래스, 모듈, 전역 변수, 네이티브/루비 라이브러리 정의를 격리 실행할 수 있는 실험적 기능\n- ZJIT는 Rust 기반 차세대 JIT 컴파일러로, 기존 YJIT보다 구조적으로 확장 가능하며 외부 기여를 용이하게 함\n- \nRactor 병렬 실행 모델이 안정성과 성능 면에서 개선되어, 향후 실험적 상태를 해제할 예정\n- 핵심 클래스, 표준 라이브러리, C API, GC, JIT 등 전반에 걸친 업데이트로 루비 생태계의 성능과 확장성을 강화\n---\nRuby 4.0 개요\n- Ruby 4.0.0은 Ruby Box와 ZJIT를 중심으로 한 대규모 업데이트 버전\n- 병렬 실행, 언어 문법, 표준 라이브러리, GC, JIT 등 다양한 영역에서 개선 포함\n- 다운로드는 .tar.gz, .tar.xz, .zip 형식으로 제공\nRuby Box\n- \nRuby Box는 정의의 격리를 제공하는 실험적 기능\n환경 변수 RUBY_BOX=1 설정 시 활성화되며, 클래스는 Ruby::Box\n- 박스 내부에서 로드된 정의는 외부와 격리되어, monkey patch, 전역/클래스 변수, 클래스/모듈 정의, 라이브러리 변경이 다른 박스에 영향을 주지 않음\n- 주요 사용 예시\n테스트 케이스 간 격리 실행\n- \n블루-그린 배포를 위한 병렬 웹앱 실행\n- 의존성 업데이트 검증용 병렬 실행\n- 향후 고수준 “패키지 API” 구현의 기반 API로 활용 예정\nZJIT\n- \nZJIT는 YJIT의 차세대 버전으로 개발된 새 JIT 컴파일러\nRust 1.85.0 이상 필요, --zjit 옵션으로 활성화\n- \nSSA IR 기반의 더 큰 컴파일 단위를 지원하며, 외부 기여를 촉진하는 구조\n- 현재 인터프리터보다 빠르지만 YJIT보다 느림\n프로덕션 사용은 권장되지 않으며, Ruby 4.1에서 성능 향상 예정\nRactor 개선\n- \nRactor::Port 클래스 추가로 메시지 송수신 문제 해결\n- \nRactor.shareable_proc으로 Ractor 간 Proc 객체 공유 용이\n- 내부 데이터 구조 개선으로 글로벌 락 경합 감소, 병렬성 향상\n- Ractor의 실험적 상태를 다음 해에 해제 예정\n언어 변경\n- \n*nil이 더 이상 nil.to_a를 호출하지 않음 (**nil과 동일한 동작)\n- 논리 연산자(||, &&, and, or)가 라인 연속(dot chaining) 문법을 지원\n- 코드 가독성과 일관성 향상\n핵심 클래스 업데이트\n- \nArray: Array#rfind, Array#find 추가로 효율적 탐색 지원\n- \nBinding: 번호 매개변수 제외 및 implicit_parameters 관련 메서드 추가\n- \nEnumerator: produce에 size: 키워드 인자 추가\n- \nErrorHighlight: ArgumentError 발생 시 호출자·정의부 코드 스니펫 표시\n- \nFiber/Fiber::Scheduler: raise(cause:), fiber_interrupt, yield 등 추가\n- \nFile: Linux에서 File::Stat#birthtime 지원\n- \nIO: Float::INFINITY 타임아웃 허용, 파이프 기반 프로세스 생성 제거\n- \nKernel: #inspect 커스터마이징 가능, Kernel#open의 파이프 생성 제거\n- \nMath: log1p, expm1 추가\n- \nPathname: 기본 젬에서 코어 클래스로 승격\n- \nProc: 익명 매개변수 출력 형식 통일\n- \nRactor: Ractor::Port 기반 통신 구조로 변경, Ractor.yield 등 제거\n- \nSet: 코어 클래스로 승격, inspect 형식 단순화\n- \nSocket: open_timeout 인자 추가, 타임아웃 예외 통일\n- \nString: Unicode 17.0.0, Emoji 17.0 지원, strip 계열 메서드 확장\n- \nThread: raise(cause:) 인자 지원\n표준 라이브러리(Stdlib) 업데이트\n- \n기본 젬 승격: ostruct, pstore, benchmark, logger, rdoc, win32ole, irb, reline, fiddle 등\n- \n기본 젬 추가: win32-registry 0.1.2\n- \n기본 젬 업데이트: RubyGems 4.0.3, bundler 4.0.3, openssl 4.0.0, json 2.18.0 등\n- \n번들 젬 업데이트: minitest 6.0.0, rake 13.3.1, rbs 3.10.0, debug 1.11.1 등\n- \nRubyGems/Bundler 4 포함\n플랫폼 지원\n- \nWindows: MSVC 14.0 미만 버전 지원 중단 (Visual Studio 2015 이상 필요)\n호환성 변경\n- \nRactor.yield, Ractor#take, Ractor#close_incoming, Ractor#close_outgoing 제거\n- \nObjectSpace._id2ref 사용 중단\n- \nProcess::Status#&, #>> 제거\n- 내부 프레임(backtrace) 출력 단순화\n- \nArgumentError 백트레이스에 수신자 클래스/모듈명 표시\n표준 라이브러리 호환성\n- \nCGI 라이브러리 제거, cgi/escape만 유지\n- \nSet의 코어 승격으로 SortedSet은 별도 젬 설치 필요\n- \nNet::HTTP의 자동 Content-Type 헤더 설정 제거\nC API 업데이트\n- \nrb_thread_fd_close 비활성화 및 rb_io_close 사용 권장\n- \nrb_thread_call_with_gvl이 GVL 유무에 관계없이 동작\n- \nSet용 C API 추가 (rb_set_new, rb_set_add, rb_set_delete 등)\n구현 및 성능 개선\n- \nClass#new 호출 속도 향상, 특히 키워드 인자 사용 시\n- GC 힙 풀 독립 성장으로 메모리 사용량 감소\n- 대형 객체 스위핑 속도 향상\n- \nobject_id, hash 계산 및 인스턴스 변수 접근 최적화\n- \nRactor 성능 개선\n락 없는 해시 구조, 캐시 경합 감소, 객체 할당 최적화\n- 데드락, 인코딩, GC 관련 버그 수정\nJIT 관련\n- \nZJIT: 메서드 기반 JIT, Rust 1.85.0 이상 필요, --zjit 또는 RubyVM::ZJIT.enable로 활성화\n- \nYJIT: 통계 옵션 변경, mem_size: 및 call_threshold: 추가\n- \nRJIT: --rjit 제거, 별도 저장소로 이전\n변경 규모\n- Ruby 3.4.0 대비 3,889개 파일 변경, 230,769줄 추가, 297,003줄 삭제\n- 루비 4.0은 성능, 병렬성, 언어 일관성을 대폭 강화한 메이저 릴리스\n다운로드\n- \nruby-4.0.0.tar.gz, ruby-4.0.0.tar.xz, ruby-4.0.0.zip 형식 제공\n- 각 파일의 SHA1, SHA256, SHA512 해시값 명시\nRuby 소개\n- 루비는 1993년 마츠모토 유키히로(Matz) 가 개발한 오픈소스 언어\n- 다중 플랫폼에서 실행되며, 특히 웹 개발 분야에서 전 세계적으로 사용",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25343",
    "category": "AI"
  },
  {
    "title": "우리가 크리스마스에 한 남자를 집에 초대했는데, 그는 45년 동안 함께 살았다",
    "content": "- 1975년 크리스마스 직전, 카디프의 한 부부가 노숙 중이던 남성을 집으로 들인 일이 45년간의 동거로 이어짐\n- 남성 로니 록우드는 자폐를 앓고 있었으며, 15세 이후 집 없이 떠돌다 부부의 집에서 새 삶을 시작함\n- 그는 가족의 일원으로 자리 잡아 아이 돌봄, 교회 봉사, 음식 기부 활동 등에 적극 참여함\n- 부부는 그의 도박 문제 등 어려움에도 불구하고 함께한 세월을 “삶을 풍요롭게 한 시간”으로 회상함\n- 로니는 2020년 세상을 떠나며 **£40,000**을 자선단체에 유산으로 남겨, 그의 이름을 딴 복지센터 건립에 기여함\n---\n\n## 45년간 이어진 우정의 시작\n- 1975년 12월 23일, 롭과 다이앤 파슨스 부부는 문 앞에 선 한 남성을 맞이함\n그는 오른손에 자신의 소지품이 든 쓰레기봉투, 왼손에 냉동 치킨을 들고 있었음\n- 롭은 그를 어린 시절 주일학교에서 본 적 있는 로니 록우드로 기억함\n- 부부는 그를 집으로 들여 치킨을 요리해 함께 식사하고, 목욕을 시키며 크리스마스를 함께 보냄\n- 원래 하루만 머물게 할 계획이었으나, 그를 내보낼 수 없어 계속 함께 살게 됨\n- 당시 20대 중반이던 부부는 자폐를 가진 로니를 가족처럼 돌보기로 결정함\n로니의 과거와 새로운 삶\n- 로니는 8세에 보호시설에 맡겨지고, 11세에 카디프를 떠나 200마일 떨어진 학교로 보내짐\n그곳은 보고서에 “정신박약 아동 학교”로 기록되어 있었으며, 그는 친구나 담당 교사 없이 지냄\n- 15세에 다시 카디프로 돌아왔지만 돌아갈 곳이 없어 노숙 생활을 시작함\n- 부부는 그에게 직업과 옷을 마련해주고, 쓰레기 수거원으로 일하도록 도움\n롭은 매일 아침 그를 차로 출근시켰고, 로니는 이를 자랑스러워함\n- 그는 매일 신문을 사고, 식기세척기를 비우는 일상적 의식을 45년간 반복함\n가족의 일원으로서의 역할\n- 로니는 부부의 두 자녀 로이드와 케이티를 돌보며 가족의 일원으로 자리함\n다이앤이 만성피로증후군(ME)으로 아플 때, 아이 돌봄과 집안일을 도맡음\n- 그는 교회에서 노숙자 돕기와 푸드뱅크 봉사, 성탄절 축구 경기 운영 등 지역사회 활동에도 헌신함\n- 한 번은 신발이 필요한 노숙자에게 자신의 신발을 건네기도 함\n- 부부는 “그는 친절하고 때로는 답답했지만, 마음이 따뜻한 사람이었다”고 회상함\n함께한 세월과 어려움\n- 부부는 로니의 도박 문제로 20년간 어려움을 겪었지만, 그를 떠나보내지 않음\n- 아이들이 성장하며 공간이 부족해졌을 때, 독립을 제안하려 했으나 로니의 불안한 반응에 포기\n그는 “내가 나쁜 일을 했나요?”라고 물었고, 부부는 “우리는 영원히 함께할 것”이라 답함\n- 롭은 “우리 아이들은 로니 없는 삶을 한 번도 경험하지 않았다”고 말함\n로니의 마지막 선물과 유산\n- 로니는 2020년 뇌졸중으로 75세에 사망, 장례식에는 코로나 제한에도 불구하고 50명이 참석\n- 그는 유언으로 **£40,000**을 자선단체에 남겼으며, 이 금액은 새로 건립된 Lockwood House 복지센터의 지붕 수리비와 정확히 일치\n- 롭은 “노숙인이 결국 우리 모두의 지붕을 얹어준 셈”이라며 감동을 전함\n- 다이앤은 “45년은 하루하루 쌓인 결과였고, 로니는 우리 삶에 풍요로움을 가져다준 사람”이라고 말함",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25342",
    "category": "개발"
  },
  {
    "title": "Walrus - 분산 메시지 스트리밍 엔진",
    "content": "- Rust로 작성된 고성능 Kafka 대체용 오픈소스\n- \n고성능 로그 스토리지 엔진 위에 구축된 분산 메시지 스트리밍 플랫폼으로, 안정적인 스트리밍과 자동 리더 교체 기능 제공\n- \nRaft 합의 프로토콜을 통해 메타데이터 일관성 유지 및 장애 복구 지원\n- \n세그먼트 기반 파티셔닝으로 부하를 자동 분산하고, 리더십을 라운드로빈 방식으로 회전\n- \n핵심 기능\n세그먼트 단위 샤딩으로 토픽을 분할하고, 각 세그먼트별 리더 노드가 쓰기 담당\n- \n리스 기반 쓰기 펜싱으로 단일 리더만 쓰기 가능, 분할 브레인 방지\n- \n자동 롤오버로 세그먼트 크기 초과 시 메타데이터 변경 제안 및 리더 교체 수행\n- \nSealed Segment Reads 기능으로 과거 데이터 복제본에서 직접 읽기 가능\n- \nio_uring 기반 고성능 I/O로 Linux 환경에서 높은 처리량 확보\n- \n클라이언트 프로토콜\nTCP 기반 길이 프리픽스 텍스트 프로토콜 사용\n- \nREGISTER, PUT, GET, STATE, METRICS 명령으로 토픽 생성, 메시지 송수신, 상태 조회 지원\n- 단순한 명령 구조로 어느 노드에나 연결 가능, 자동 포워딩 처리\n- \n성능 특성\n쓰기 처리량: 세그먼트당 단일 라이터 구조로 안정적 성능 유지\n- \n읽기 처리량: 복제본 수에 비례해 확장\n- \n지연 시간: 평균 1~2 RTT 수준\n- \n합의 오버헤드 최소화: 데이터 경로가 아닌 메타데이터에만 Raft 적용\n- \n성능 비교\nKafka, RocksDB 대비 높은 쓰기 처리량 및 대역폭\n- fsync 비활성화 시 초당 **120만** 건 이상, 활성화 시에도 유사 수준의 안정적 성능 유지\n- MIT License",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25341",
    "category": "AI"
  },
  {
    "title": "Google의 2025년 연구 성과 리뷰: 올해의 연구 혁신 분야 8가지",
    "content": "- 2025년 구글은 Gemini 3와 Gemma 3를 중심으로 인공지능의 추론력·멀티모달 이해·효율성을 크게 향상시킴\n- Gemini 3 Pro는 LMArena 리더보드 1위를 기록하고, Humanity's Last Exam과 GPQA Diamond 등 벤치마크에서 획기적 점수 달성\n- AI가 도구에서 유틸리티로 전환되며, Pixel 10, Search, NotebookLM 등 제품 전반에 에이전틱 기능 적용\n- \n생명과학·수학·양자컴퓨팅 등 과학 연구에서도 AI가 핵심 역할을 수행하며, AlphaFold와 Deep Think가 주목받음\n- \n기후 예측·의료·교육 등 글로벌 과제 해결에 AI를 적용, WeatherNext 2와 Gemini 번역 기능이 대표적 성과\n- 책임 있는 AI 개발을 위해 Frontier Safety Framework 강화 및 Agentic AI Foundation 설립 참여\n---\n\n## AI 모델 발전\n- 2025년 3월 Gemini 2.5 출시로 시작해 11월 Gemini 3, 12월 Gemini 3 Flash 순차 발표\n- Gemini 3 Pro는 최첨단 추론 능력을 기반으로 한 가장 강력한 모델로, 아이디어 구현 지원 목적 설계\nLMArena 리더보드 1위 달성\n- \nHumanity's Last Exam과 GPQA Diamond 벤치마크에서 멀티모달 추론 획기적 점수 기록\n- \nMathArena Apex에서 23.**4%**의 새로운 최고 기록 수립\n- Gemini 3 Flash는 Pro급 추론 성능과 Flash급 지연시간, 효율성, 비용을 결합\n이전 Gemini 2.5 Pro급 모델의 성능을 훨씬 저렴한 가격과 향상된 지연시간으로 제공\n- \"다음 세대의 Flash 모델이 이전 세대의 Pro 모델보다 우수하다\"는 Gemini 시대 트렌드 지속\n- \nGemma 모델 패밀리는 경량화와 오픈소스 공개를 목표로 개발\n멀티모달 기능 도입, 컨텍스트 윈도우 대폭 확대, 다국어 지원 확장, 효율성 및 성능 개선\n- \n관련 링크\nGemini 3 Flash: 속도를 위해 설계된 프론티어 인텔리전스 (2025년 12월)\n- \nGemini 3와 함께하는 새로운 인텔리전스 시대 (2025년 11월)\n- \nNano Banana Pro 소개 (2025년 11월)\n- \nGemini API에서 Veo 3.1과 새로운 창작 기능 소개 (2025년 11월)\n- \nGemini 2.5: 가장 지능적인 AI 모델 (2025년 3월)\n- \nGemma 3 소개: 단일 GPU 또는 TPU에서 실행할 수 있는 가장 강력한 모델 (2025년 3월)\n- \nGemma 3 270M 소개: 초효율 AI를 위한 컴팩트 모델 (2025년 8월)\nAI 제품 혁신\n- AI가 도구에서 유틸리티로 전환되는 흐름 속에서 제품 포트폴리오 전반에 에이전틱 기능 도입\n- 소프트웨어 개발 분야 재정의\n코딩 지원 도구를 넘어 개발자와 협업하는 강력한 에이전틱 시스템 도입\n- Gemini 3의 코딩 능력과 Google Antigravity 출시로 AI 지원 소프트웨어 개발의 새 시대 개막\n- 핵심 제품 AI 기능 강화\nPixel 10: 9가지 AI 기능으로 가장 유용한 폰 구현\n- \nSearch: AI Overview 확장 및 AI Mode 도입\n- \nGemini 앱: Gemini 3 기반 스마트 기능과 새로운 역량 추가\n- \nNotebookLM: Deep Research 기능과 더 많은 소스 타입 지원 추가\n- \n관련 링크\nGemini 3로 빌드 시작하기 (2025년 11월)\n- \nGoogle Antigravity 소개, AI 지원 소프트웨어 개발의 새 시대 (2025년 11월)\n- \nAI가 Pixel 10을 가장 유용한 폰으로 만드는 9가지 방법 (2025년 8월)\n- \nAI Overview 확장 및 AI Mode 도입 (2025년 3월)\n- \nGemini 3가 Gemini 앱에 업그레이드된 스마트 기능과 새로운 역량 제공 (2025년 11월)\n- \nNotebookLM에 Deep Research와 더 많은 소스 타입 지원 추가 (2025년 11월)\nAI와 창작\n- 2025년은 생성형 미디어의 변혁적 해로, 비디오, 이미지, 오디오, 세계 생성 도구가 더 효과적이고 광범위하게 사용됨\n- \nNano Banana와 Nano Banana Pro가 네이티브 이미지 생성 및 편집에서 전례 없는 기능 제공\n- 창작 산업 종사자들과 협력해 Flow와 Music AI Sandbox 같은 창작 워크플로우 도구 개발\n- 새로운 생성형 미디어 모델 출시\nVeo 3.1: 고급 비디오 생성 기능\n- \nImagen 4: 차세대 이미지 생성\n- \nFlow: 라이브 액션 영화 제작과 Veo 결합\n- \nGemini 앱 내 이미지 편집 대폭 업그레이드\n- Google Arts & Culture 랩에서 AI 기반 문화 학습 경험 제공\n- \n관련 링크\n예술, 과학, 여행: 이번 홀리데이 시즌 3가지 새로운 AI 기반 경험 (2025년 11월)\n- \nVeo 3.1과 Flow의 고급 기능 소개 (2025년 10월)\n- \nNano Banana: Gemini의 이미지 편집 대폭 업그레이드 (2025년 8월)\n- \nVeo 3, Imagen 4, Flow: 새로운 생성형 미디어 모델과 도구로 창작력 발휘 (2025년 5월)\n- \nMusic AI Sandbox, 새로운 기능과 더 넓은 접근성 제공 (2025년 4월)\nGoogle Labs 실험\n- Labs는 AI 실험을 개발 중 공유하고 사용자 피드백을 반영하는 공간\n- 주요 실험 프로젝트\nPomelli: 브랜드 일관성 있는 마케팅 콘텐츠 생성 AI 실험\n- \nStitch: 프롬프트와 이미지 입력을 복잡한 UI 디자인 및 프론트엔드 코드로 수분 내 변환\n- \nJules: 개발자를 위한 비동기식 코딩 에이전트로 협업 파트너 역할\n- \nGoogle Beam: AI 기반 3D 비디오 커뮤니케이션 플랫폼으로 원격 현존감 가능성 확장\n- \n관련 링크\nPomelli로 비즈니스를 위한 브랜드 일관성 마케팅 콘텐츠 생성 (2025년 10월)\n- \nGoogle Beam: AI 퍼스트 3D 비디오 커뮤니케이션 플랫폼 (2025년 5월)\n- \n아이디어에서 앱으로: UI 디자인의 새로운 방식 Stitch 소개 (2025년 5월)\n- \nJules로 빌드하기, 비동기식 코딩 에이전트 (2025년 5월)\n과학 및 수학 발전\n- \n생명과학 및 건강\nAI 리소스와 도구 구축으로 연구자들의 질병 이해, 식별, 치료 개발 지원\n- \n유전체학: 10년간 첨단 기술을 연구에 적용해 왔으며, 시퀀싱을 넘어 AI로 복잡한 데이터 해석\n- \nAlphaFold 5주년: 50년 된 단백질 접힘 문제를 해결한 노벨상 수상 AI 시스템\n190개국 이상 **300만** 명 이상 연구자 사용\n- 저소득 및 중소득 국가에서 **100만** 명 이상 사용\n- \n관련 링크\nAlphaFold: 5년간의 임팩트 (2025년 11월)\n- \nDeepSomatic으로 종양 내 유전적 변이 식별에 AI 활용 (2025년 10월)\n- \n연구 파트너로서의 AI: AlphaEvolve로 이론적 컴퓨터 과학 발전 (2025년 9월)\n- \nAlphaGenome: 유전체 이해를 위한 AI (2025년 6월)\n- \nAI 공동 과학자로 과학적 발견 가속화 (2025년 2월)\n- \n수학 및 코딩\nGemini의 Deep Think 고급 사고 능력으로 수학과 코딩에서 역사적 진전\n- Deep Think는 깊은 추상적 추론이 필요한 문제 해결 가능\n- 두 개의 국제 대회에서 금메달 수준 달성\n- \n관련 링크\nGemini, 국제 대학생 프로그래밍 대회 세계 결승에서 금메달 수준 달성 (2025년 9월)\n- \nDeep Think를 탑재한 Gemini 고급 버전, 국제 수학 올림피아드에서 공식 금메달 기준 달성 (2025년 7월)\n컴퓨팅 및 물리적 세계\n- \n양자 컴퓨팅\nQuantum Echoes 알고리듬으로 양자 컴퓨팅의 실제 응용을 향한 큰 진전\n- \nMichel Devoret(Google 직원)이 2025년 물리학 노벨상 수상\n전 Google 직원 John Martinis, UC Berkeley의 John Clarke와 함께 1980년대 양자 연구 공로 인정\n- \n관련 링크\nProject Suncatcher: 우주 기반 확장 가능 AI 인프라 시스템 설계 탐색 (2025년 11월)\n- \nGoogler Michel Devoret, 물리학 노벨상 수상 (2025년 10월)\n- \nQuantum Echoes 알고리듬, 양자 컴퓨팅의 실제 응용을 향한 큰 진전 (2025년 10월)\n- \n인프라 및 에너지 효율성\nAI를 구동하는 핵심 인프라의 하드웨어 설계 혁신과 에너지 효율성 개선에 집중\n- \nIronwood: 추론 시대를 위한 새로운 TPU\nAlphaChip 방법론을 사용해 설계\n- Google의 첫 번째 추론 시대용 TPU\n- \n관련 링크\n최신 TPU Ironwood에 대해 알아야 할 3가지 (2025년 11월)\n- \nGoogle AI는 얼마나 많은 에너지를 사용하나? 계산해봄 (2025년 8월)\n- \nIronwood: 추론 시대를 위한 첫 번째 Google TPU (2025년 4월)\n- \nAlphaChip이 컴퓨터 칩 설계를 어떻게 변혁했는가\n- \n로보틱스 및 월드 모델\nAI 에이전트를 물리적 세계와 가상 세계 모두에 도입\n- \nGemini Robotics: AI를 물리적 세계로 가져오는 기초 모델\n- \nGemini Robotics 1.5: 더 정교한 물리적 세계 AI 에이전트\n- \nGenie 3: 범용 월드 모델의 새로운 프론티어\n- \n관련 링크\nGemini Robotics 1.5가 AI 에이전트를 물리적 세계로 가져옴 (2025년 9월)\n- \nGenie 3: 월드 모델의 새로운 프론티어 (2025년 8월)\n- \nGemini Robotics가 AI를 물리적 세계로 가져옴 (2025년 3월)\n글로벌 임팩트\n- \n기후 및 지구 이해\n최첨단 기초 모델과 에이전틱 추론으로 지구 시스템 이해 증진\n- \n홍수 예측: 150개국 **20억** 명 이상 대상 심각한 하천 홍수 정보 제공\n- \nWeatherNext 2: 가장 진보하고 효율적인 기상 예측 모델\n기존 대비 8배 빠른 예보 생성\n- 최대 1시간 해상도 지원\n- 실험적 사이클론 예측을 통해 기상청의 시나리오 기반 의사결정 지원\n- \n관련 링크\nWeatherNext 2: 가장 진보한 기상 예측 모델 (2025년 11월)\n- \nGoogle Earth AI의 새로운 업데이트와 더 많은 접근성 (2025년 10월)\n- \nGoogle Earth AI: 최첨단 지리공간 AI 모델 (2025년 7월)\n- \nAlphaEarth Foundations가 전례 없는 상세도로 지구 매핑 지원 (2025년 7월)\n- \nAI로 더 나은 열대성 사이클론 예측 지원 방법 (2025년 6월)\n- \nFireSat 출시 비하인드, 산불 조기 발견 시스템 (2025년 3월)\n- \n건강 및 치료\n파트너들과 협력해 AI 기반 과학적 진보를 환자에게 더 가깝게 적용\n- \nCell2Sentence-Scale 27B: Gemma 모델이 새로운 잠재적 암 치료 경로 발견 지원\n- \nAMIE: 진단에서 치료까지 종단적 질병 관리를 위한 발전\n- \n관련 링크\nCell2Sentence-Scale 27B: Gemma 모델이 새로운 잠재적 암 치료 경로를 발견하는 데 도움을 준 방법 (2025년 10월)\n- \n진단에서 치료까지: 종단적 질병 관리를 위한 AMIE 발전 (2025년 3월)\n- \n교육 및 학습\nLearnLM과 Gemini 내 Guided Learning으로 새로운 이해 형태와 호기심 확장\n- \nGoogle Translate에 Gemini의 가장 강력한 번역 기능 도입\n더 스마트하고 자연스럽고 정확한 번역\n- 새로운 음성 대 음성 번역 기능 파일럿\n- \n관련 링크\nGoogle Translate에 최첨단 Gemini 번역 기능 도입 (2025년 12월)\n- \nGemini의 Guided Learning: 답변에서 이해로 (2025년 8월)\n- \n생성형 AI가 LearnLM으로 호기심과 이해를 확장하는 방법 (2025년 5월)\n책임성 및 안전\n- 연구 돌파구와 함께 책임성 및 안전에 대한 엄격하고 미래지향적 작업 병행\n- \nGemini 3: 현재까지 가장 안전한 모델이자 Google AI 모델 중 가장 포괄적인 안전 평가 수행\n- \nFrontier Safety Framework 강화\n- \nAGI로 가는 책임 있는 경로 탐색\n준비성, 선제적 위험 평가, 더 넓은 AI 커뮤니티와의 협력 우선시\n- Gemini 앱에서 AI 생성 이미지 및 비디오 검증 기능 도입\n- \n관련 링크\nGemini 앱에서 Google AI 생성 비디오 검증 가능 (2025년 12월)\n- \nGemini 앱에 AI 이미지 검증을 도입하는 방법 (2025년 11월)\n- \nFrontier Safety Framework 강화 (2025년 9월)\n- \nAGI로 가는 책임 있는 경로 (2025년 4월)\n- \n고급 AI의 잠재적 사이버보안 위협 평가 (2025년 4월)\n산업, 학계, 시민사회 협력\n- 책임 있는 AI 발전을 위해 사회 전 분야와 협력 필요\n- \nAgentic AI Foundation(AAIF) 설립 지원\nModel Context Protocol(MCP), goose, AGENTS.md 등 새 프로젝트 기여 포함\n- 에이전틱 AI의 책임 있고 상호운용 가능한 미래를 위한 오픈 표준 지원\n- \n미국 에너지부 17개 국립연구소와 협력해 과학 연구 수행 방식 혁신 지원\nGenesis 프로젝트: 혁신과 과학적 발견 가속화를 위한 국가 미션\n- 교육 파트너십\nMiami Dade County 학군과 협력해 학생들에게 AI 기술 교육\n- Raspberry Pi와 같은 교육 그룹과 협력\n- 대학 연구 파트너십: UC Berkeley, Yale, University of Chicago 등과 협력해 프론티어 연구 수행\n- 창작자 협업\n영화 제작자 및 창작 비전가들에게 최고의 AI 도구 제공\n- \"Sweetwater\" 단편 영화로 새로운 AI 내러티브 탐색\n- \"ANCESTRA\": Veo와 라이브 액션 영화 제작 결합\n- 인도 음악 전설 Shankar Mahadevan과 Music AI Sandbox 실험\n- \n관련 링크\nGoogle DeepMind, 미국 에너지부의 Genesis 지원: 혁신과 과학적 발견 가속화를 위한 국가 미션 (2025년 12월)\n- \nAgentic AI Foundation(AAIF) 설립, Model Context Protocol(MCP), goose, AGENTS.md 등 새 프로젝트 기여 포함 (2025년 12월)\n- \nGoogle 서비스용 Model Context Protocol(MCP) 공식 지원 발표 (2025년 12월)\n- \nAI와 학습에 대한 최신 약속 (2025년 11월)\n- \nMiami의 AI 준비 미래를 위한 파트너십 (2025년 10월)\n- \nAI on Screen 프리미어: \"Sweetwater\" 단편 영화, 새로운 AI 내러티브 탐색 (2025년 9월)\n- \n\"ANCESTRA\" 비하인드: Veo와 라이브 액션 영화 제작 결합 (2025년 6월)\n- \n인도 음악 전설 Shankar Mahadevan이 Music AI Sandbox를 실험하는 방법 (2025년 4월)\n향후 전망\n- 구글은 2026년에도 인류에 이로운 안전하고 책임 있는 AI 발전을 지속 추진 예정",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25338",
    "category": "AI"
  },
  {
    "title": "러시아어를 배우는 외국인을 위한 나보코프의 안내",
    "content": "- 러시아 작가 블라디미르 나보코프가 외국인이 러시아어를 배울 때 겪는 어려움과 특징을 다룸\n- 러시아어의 문법적 복잡성과 문화적 뉘앙스를 이해하는 과정을 중심으로 설명\n- 언어 학습에서 발음, 억양, 어휘 선택이 갖는 중요성을 강조\n- 외국인이 러시아어를 익히며 마주치는 사고방식의 차이를 언급\n- 문학적 감수성과 언어 감각을 결합한 언어 학습의 통찰을 제시\n---\n\n## 내용 불분명\n- 제공된 트위터 링크 외에 본문 내용이 포함되어 있지 않아, 구체적인 인용이나 세부 설명 없음\n- 나보코프의 러시아어 학습 관련 발언이나 글의 실제 내용은 확인 불가\n- 추가 정보가 없으므로 세부 구조 요약 불가능",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25337",
    "category": "AI"
  },
  {
    "title": "Show GN: iOS 26 Foundation Model을 이용해 스크린샷을 자동으로 정리해주는 iOS 앱",
    "content": "## 스크린샷AI 소개\n평소에 좋은 정보를 보면 무의식적으로 스크린샷을 찍어두는데, 막상 정리는 않하고 방치하는 경우가 많습니다. 이 스크린샷들을 AI를 이용해서 검색가능한 노트로 바꿔주는 iOS앱을 개발했습니다.\nhttps://apps.apple.com/kr/app/screenshotai-ai-notes/id6756419927\n프로세싱 단계\n- Vision framework의 OCR기능을 이용해 이미지에서 텍스트 추출\n- 유의미한 길이의 텍스트(10자 이상)일 경우 Foundation Model 을 이용해 노트 생성\n- Foundation Model로 만들어진 노트는 title, tags, summary, key insights, ai refined content 로 구성되어 있습니다.\n기술 스택\n- SwiftUI\n- Vision Framework\n- iOS26 on-device Foundation Model 사용\nAPI키를 등록하면 OpenAI 모델 사용 가능\n- Core Spotlight 연동으로 OS레벨에서 검색 가능하도록 구현",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25336",
    "category": "AI"
  },
  {
    "title": "Microsoft는 탭 자동완성 기능을 제대로 개선해야 한다",
    "content": "- Visual Studio Code의 자동완성(tab completion) 기능이 의도한 대로 작동하지 않는 문제 제기\n- 사용자가 탭을 눌러도 아무 동작이 없거나, 초기 제안과 다른 항목이 선택되는 현상 발생\n- 문제는 C# Dev Kit 플러그인 또는 VS Code의 자동완성 로직과 관련된 것으로 지적\n- Microsoft 내부의 관련 팀이 이 기능을 수정하고 개선해야 함을 강조\n- 개발자 경험의 핵심 요소인 자동완성 품질 관리의 중요성을 환기하는 사례\n---\n\n## VS Code 자동완성 문제 지적\n- 탭 키를 눌렀을 때 예상된 제안이 선택되지 않고 다른 제안이 표시되는 오류 발생\n스크린샷 상황에서 탭 입력 후 아무 동작이 없거나 엉뚱한 제안이 나타남\n- 이 문제는 Visual Studio Code의 자동완성 기능 또는 C# Dev Kit 플러그인의 동작과 관련 있음\n- 작성자는 Microsoft의 관련 팀이 자동완성 동작을 수정해야 함을 요구\n작성자 정보\n- 작성자는 Ivan Castellanos, 15년 경력의 소프트웨어 개발자\nReact, Node.js, Python, TypeScript, PHP, C#(Unity) 등 다양한 기술 스택 보유\n- 원격 근무 기회를 찾고 있으며, 협업 또는 채용 문의를 이메일로 받고 있음\n블로그 개요\n- 개인 블로그 Ivanca’s Blog는 소프트웨어 개발과 관련된 글 및 의견 공유 공간\n- 블로그는 CloudCannon 템플릿을 사용하며, RSS 구독 기능 제공",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25335",
    "category": "개발"
  },
  {
    "title": "LG TV의 ‘Live Plus’를 끄기 전까지 몰랐던 감시 기능",
    "content": "- LG 스마트 TV의 Live Plus 기능이 화면에 표시되는 콘텐츠를 분석해 광고와 추천을 제공하는 자동 콘텐츠 인식(ACR) 기술을 사용함\n- 이 기능은 기본적으로 활성화되어 있으며, 사용자가 시청 중인 내용을 LG가 수집해 개인화 서비스와 광고에 활용함\n- 설정 메뉴에서 Live Plus를 끄면 더 이상 TV가 화면 내용을 읽어 광고를 표시하지 않음\n- \n삼성, Roku 등 다른 브랜드 TV도 유사한 ACR 기반 기능을 제공하며, 각각의 설정에서 비활성화 가능함\n- 스마트 TV 전반에 걸쳐 사용자 데이터 수집이 기본값으로 설정된 현실이 드러난 사례임\n---\n\n## Live Plus 기능의 작동 방식\n- \nLive Plus는 LG 스마트 TV에 기본 탑재된 기능으로, ACR(Automatic Content Recognition) 기술을 이용해 화면에 표시되는 콘텐츠를 분석함\n이를 통해 LG는 사용자의 시청 정보를 기반으로 콘텐츠 추천 및 광고를 제공\n- LG는 설정 메뉴에서 “Live Plus를 켜면 TV에 표시되는 콘텐츠가 인식될 수 있으며, 시청 정보가 개인화 서비스 제공에 사용될 수 있다”고 명시\n- 사용자는 이 기능이 기본적으로 활성화되어 있다는 점에 놀라움을 느낌\n광고를 위해 TV가 화면을 ‘감시’하는 형태라는 점에서 불쾌감을 표현\nLive Plus 비활성화 방법\n- LG TV에서 Live Plus를 끄는 절차는 다음과 같음\n리모컨의 설정(Settings) 버튼(톱니바퀴 아이콘)을 누름\n- 사이드 메뉴에서 설정(Settings) 선택\n- \n일반(General) 메뉴 선택\n- \n시스템(System) 항목으로 이동\n- \n추가 설정(Additional Settings) 선택\n- \nLive Plus 끄기(Toggle off)\n- Live Plus를 비활성화하면 TV가 더 이상 시청 중인 화면을 분석하거나 광고를 표시하지 않음\n다른 브랜드의 유사 기능\n- LG뿐 아니라 다른 스마트 TV 브랜드도 유사한 ACR 기반 기능을 탑재\nSamsung TV: ‘Privacy Choices’ 메뉴에서 ‘Terms and Conditions’를 선택 후, ‘Viewing Information Services’와 ‘Internet-Based Advertisement Services’를 끄면 비활성화 가능\n- \nRoku TV: ‘Smart TV Experience’ 메뉴의 ‘Use info from TV inputs’를 끄면 ACR 기능 비활성화\n- 이러한 기능들은 모두 시청 데이터를 광고 및 추천에 활용한다는 공통점이 있음\n사용자 경험과 문제 인식\n- Live Plus 기능이 존재한다는 사실을 우연히 발견한 후, 사용자는 광고 목적의 데이터 수집이 기본값으로 설정된 점에 불만을 느낌\n- 동시에, 기능을 간단히 비활성화할 수 있다는 점에는 안도감을 표현\n- 글에서는 이 문제가 LG만의 문제가 아니라 스마트 TV 전반의 구조적 문제임을 지적\n관련 LG 소식\n- \nDirecTV가 최근 LG webOS 앱스토어에 라이브 TV 앱을 출시\n- LG는 최신 webOS 업데이트에서 Copilot 앱을 자동 설치하는 조치를 취해 논란이 발생\n- 이러한 사례들은 LG 스마트 TV의 소프트웨어 정책과 사용자 통제권에 대한 논의로 이어짐",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25330",
    "category": "튜토리얼"
  },
  {
    "title": "Vibium - AI와 인간을 위한 브라우저 자동화 도구",
    "content": "- 21년전 Selenium을 개발했던 Jason Huggins가 AI 에이전트를 염두에 두고 다시 개발한 브라우저 자동화 오픈소스 프로젝트\n- \nVibium은 AI 에이전트를 위한 브라우저 자동화 인프라로, 단일 바이너리로 브라우저 수명주기와 WebDriver BiDi 프로토콜을 관리하고 MCP 서버를 노출함\n- 10MB짜리 싱글 Go 바이너리 Clicker 는 Chrome을 자동 감지·실행하며, BiDi 프록시와 MCP 서버를 통해 AI 모델이나 JS 클라이언트가 브라우저를 제어할 수 있게 함\n- \nJS/TS 클라이언트는 동기·비동기 API를 모두 지원하며, npm install vibium으로 설치 후 즉시 사용 가능\n- \nClaude Code 같은 LLM 에이전트는 claude mcp add vibium 명령 한 줄로 브라우저 제어 기능을 추가할 수 있음\n- AI 자동화와 테스트 자동화 모두에 적합하며, 설정 없는 브라우저 제어 환경을 제공함\n---\n\n## Vibium 개요\n- \nVibium은 AI 에이전트와 인간 사용자를 위한 브라우저 자동화 인프라스트럭처\n단일 Go 바이너리로 브라우저 관리, WebDriver BiDi 프록시, MCP 서버 기능을 통합\n- Claude Code, Codex, Gemini 등 다양한 LLM 모델과 호환\n- \n설치 과정 없이 즉시 작동하는 구조로, AI 에이전트나 테스트 자동화 환경에서 활용 가능\n구성 요소\n- \nClicker: 약 10MB 크기의 Go 바이너리로, 다음 기능을 수행\nChrome 자동 탐지 및 BiDi 모드 실행\n- WebSocket 기반 BiDi 프록시 서버로 명령 전달\n- \nMCP 서버를 통해 LLM 에이전트와 통신\n- \nAuto-Wait 기능으로 요소 대기 후 상호작용\n- \n스크린샷 캡처 기능 제공\n- \nJS/TS 클라이언트: npm 패키지로 제공되며, 동기(browserSync)와 비동기(browser) API 모두 지원\nvibe.go(), vibe.find(), vibe.click(), vibe.quit() 등 간단한 명령으로 브라우저 제어\n- 스크린샷 저장, 요소 탐색, 클릭 등 기본 자동화 기능 포함\nAI 에이전트 통합\n- \n\n---\n\n## Claude Code에 브라우저 제어 기능을 추가하는 명령\nclaude mcp add vibium -- npx -y vibium \nChrome이 자동 다운로드되어 별도 설정 불필요\n- 제공 명령 목록\nbrowser_launch: 브라우저 실행\n- \nbrowser_navigate: URL 이동\n- \nbrowser_find: CSS 셀렉터로 요소 탐색\n- \nbrowser_click: 요소 클릭\n- \nbrowser_type: 텍스트 입력\n- \nbrowser_screenshot: 뷰포트 캡처\n- \nbrowser_quit: 브라우저 종료\n인간 사용자를 위한 설치\n- \nnpm install vibium 명령으로 자동 설치\n플랫폼별로 Clicker 바이너리와 Chrome for Testing, chromedriver를 캐시에 다운로드\n- Linux: ~/.cache/vibium/, macOS: ~/Library/Caches/vibium/, Windows: %LOCALAPPDATA%\\vibium\\\n- 환경 변수 VIBIUM_SKIP_BROWSER_DOWNLOAD=1로 브라우저 다운로드 생략 가능\n플랫폼 지원\n- Linux x64, macOS(Intel/Apple Silicon), Windows x64 모두 지원\n빠른 시작\n- \n라이브러리 사용 예시\nimport { browser } from \"vibium\"; \nconst vibe = await browser.launch(); \nawait vibe.go(\"https://example.com\"); \nconst el = await vibe.find(\"a\"); \nawait el.click(); \nawait vibe.quit(); \n- \nClaude Code 연동 예시\n설치 후 “Go to example.com and click the first link”와 같은 명령으로 브라우저 조작 가능\n로드맵\n- \nV1: MCP 및 JS 클라이언트를 통한 브라우저 제어에 집중\n- \nV2 계획\nPython 및 Java 클라이언트\n- \nCortex(메모리·내비게이션 계층)\n- \nRetina(녹화 확장 기능)\n- 비디오 녹화, AI 기반 요소 탐색 기능",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25327",
    "category": "AI"
  },
  {
    "title": "브라우저에서 실행되고 URL에 모든 내용을 저장하는 미니멀리스트 텍스트 에디터",
    "content": "- 브라우저 내에서 동작하며 모든 텍스트를 URL 해시로 저장하는 경량 텍스트 편집기\n- 입력 내용은 deflate 압축을 통해 URL 길이를 줄이고, 백엔드 서버 없이 완전한 클라이언트 측에서 작동\n- \n자동 저장(500ms 지연) , 다크 모드, 모바일 대응 등 기본 편의 기능을 포함\n- 문서 제목을 # Title로 지정하거나, <article> 태그에 스타일 속성을 추가해 URL에 함께 저장 가능\n- \nURL 공유만으로 문서 전달이 가능해, 간단한 메모나 코드 스니펫 공유에 유용함\n---\n개요\n- \ntextarea.my는 브라우저에서 완전히 실행되는 미니멀리스트 텍스트 에디터\n모든 데이터는 URL 해시에 저장되어 별도의 서버나 데이터베이스가 필요 없음\n- JavaScript로 제작되어 설치 과정 없이 즉시 사용 가능\n주요 기능\n- \n텍스트 압축(Compression magic)\n입력된 텍스트를 deflate 알고리듬으로 압축해 URL 길이를 최소화\n- 약 500자 내외의 URL로 긴 노트를 공유 가능\n- \nURL 기반 저장 및 공유\n작성된 내용이 URL 해시에 포함되어, 해당 링크를 복사해 공유 가능\n- 서버 저장 없이도 링크만으로 문서 복원 가능\n- \n자동 저장 및 다크 모드\n입력 후 500ms 지연된 자동 저장 기능 제공\n- 시스템의 색상 모드 설정을 인식해 다크 모드 자동 적용\n- \n모바일 친화적 인터페이스\n스마트폰에서도 동일한 기능으로 문서 작성 가능\n- 반응형 디자인으로 이동 중 사용에 적합\n- \n백엔드 없는 구조\n“Zero servers were harmed”라는 문구처럼 완전한 클라이언트 기반 앱\n- 서버 부하나 개인정보 저장 문제 없음\n사용 방법\n- \ntextarea.my 접속 후 바로 입력 가능\n- 입력할수록 URL이 길어지는 것을 확인 가능\n- URL을 복사해 다른 사람과 공유 가능\n고급 팁(Pro tips)\n- 문서 첫 줄에 # Title을 입력하면 페이지 제목으로 설정\n- 데이터는 localStorage와 URL 양쪽에 저장되어 이중 보존\n- 개발자 도구에서 <article> 태그에 style 속성을 추가하면, 해당 스타일도 URL에 함께 저장\n예시 링크\n- Fyodor Dostoevsky의 Crime and Punishment 예시 문서 제공\n- ChatGPT가 작성한 An Ode to Comic Sans 예시 문서 제공\n기술적 특징\n- \nJavaScript 기반으로 제작\n- 서버 요청 없이 URL 해시와 localStorage만으로 데이터 관리\n- 단순한 구조로 보안 위험 최소화 및 빠른 로딩 속도 확보\n결론\n- textarea.my는 서버리스 환경에서 동작하는 초경량 텍스트 편집기로,\nURL 공유만으로 문서 저장과 전달이 가능한 독특한 접근 방식을 제공함\n- 개발자나 디자이너가 간단한 아이디어 기록, 코드 메모, 테스트 문서 작성에 활용하기 적합한 도구임",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25326",
    "category": "AI"
  },
  {
    "title": "Tim Chambers의 2026 오픈 소셜 웹 예측",
    "content": "MILD (안전한 예측), MEDIUM-SPICEY (중간 수준), SPICY (위험한 핫 테이크)\nMILD (안전한 예측: 발생할 가능성 높음)\n- Bluesky 등록 사용자 6천만 명 돌파 (성장 둔화되지만 안정적)\n- ActivityPub Fediverse (Threads 제외) 등록 사용자 1천5백만 명 돌파, 월 활성 사용자 2~3백만 명으로 안정\n- X/Twitter나 TikTok 변화로 인한 사용자 유입: Meta(Threads/IG) > Bluesky > Fediverse 순\n- Threads 월 활성 사용자 **5억** 명 돌파, ActivityPub 인접 최대 플랫폼 유지\n- Threads 연합(federation): 부분적·옵트인 유지, 완전 양방향 연합 2026년 미출시\n- Ghost의 ActivityPub 통합: **7만**5천 개 이상 새 계정 추가, 서버 소프트웨어 상위 10위\n- WordPress 기반 연합 계정: **5만** 개 돌파\nMEDIUM-SPICEY (중간 수준: 가능성 있음)\n- BridgyFed: Bluesky 사용자 ActivityPub 브릿징 '옵트아웃' 전환, 논란 적음\n- 독립 ATProto 스택 (PDS, Relay, AppView): 유료 고객 또는 지속 가능 자금 확보\n- Mastodon gGmbH: 호스팅 수익 목표 초과, 추가 그랜트 확보, 개발 속도 가속\n- Bluesky PBC: 추가 펀딩 라운드, 비광고 기반 비즈니스 모델 발표 (구독·마켓플레이스 등)\n- ATProto 네이티브 비마이크로블로깅 앱: **10만** 사용자 돌파\n- Flipboard Surf 앱: 1.0 버전 출시, **100만** 다운로드·**10만** 월 활성 사용자\n- Fedify: **50만** 사용자 이상 중형 플랫폼의 ActivityPub 지원 담당\n- Fediscovery: Mastodon 안정 버전 포함, 플러그인 발견 제공자 1.0 사양·공개 제공자 출시\n- ActivityRank 알고리즘 (Loops): 윤리적 추천과 분산화 공존 증명, 다른 플랫폼 채택\n- ATProto: IETF 워킹 그룹 공식 형성\nSPICY (위험한 예측: 핫 테이크)\n- 유명 디지털 미디어 출판물 (월 1천만 방문자): ActivityPub 연합, 긍정 결과 공유 → 다른 출판물 따라감\n- 주요 뉴스 기관 (미국 트래픽 상위 50): X/Twitter 완전 이탈, Bluesky 또는 Fediverse 주요 채널로 전환\n- 주요 국가 정부 또는 도시: Bluesky와 ActivityPub Fediverse 공식 계정 개설 (유럽 정부 선도, 라틴아메리카·아시아·아프리카 따라감)\n- Nostr ↔ ATProto ↔ ActivityPub 3방향 브릿징 기능화 → '프로토콜 전쟁' 종료\n- AltStore: 연합 기능 5개국 이상 확장, ActivityPub 통합으로 비소셜 미디어 사례 증명\n- Loops: Fediverse 소프트웨어 MAU 3위 (Mastodon·Pixelfed 뒤), **10만** 월 활성 사용자·비디오 콘텐츠 성공\n- PieFed: Threadiverse 플랫폼 중 가장 기능 풍부, **1만** 월 활성 사용자 돌파\n- Utah Digital Choice Act 유사 법안 다수 통과 → 상호운용성 논의 촉진, 주요 플랫폼 ActivityPub/ATProto 지원 발표",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25322",
    "category": "AI"
  },
  {
    "title": "메리 크리스마스",
    "content": "- \nHacker News 커뮤니티가 연말 인사를 전하는 간단한 게시물\n- 기술, 스타트업, 개발자들이 모인 공간에서 감사의 인사와 축하 메시지를 공유",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25321",
    "category": "스타트업"
  },
  {
    "title": "샘 알트먼: OpenAI의 승부 전략, ChatGPT의 진화, AI 인프라 계산법, 그리고 2026년 IPO 가능성 [유튜브]",
    "content": "- AI 경쟁이 치열해지는 상황에서 OpenAI는 모델 성능 자체보다 제품·플랫폼·인프라의 결합을 통해 우위를 유지하려는 전략을 분명히 함\n- ChatGPT는 이미 수억 명 규모의 사용자 기반과 개인화 경험을 바탕으로 소비자에서 기업 시장으로 자연스럽게 확장 중임\n- GPT-5.2 이후 모델들은 지식 노동의 상당 부분을 전문가 수준으로 수행하며, 기업 운영 방식에 실질적 변화를 유도 중\n- OpenAI는 향후 과학 연구와 신약·수학·물리 영역에서의 발견을 AI의 가장 큰 가치로 보고 대규모 연산 자원을 투입하고 있음\n- **1.4조** 달러 규모의 인프라 투자는 장기적 수요와 수익 성장을 전제로 하며, 2026년 전후 IPO 가능성도 열어둠\n---\n\n## 경쟁 환경과 코드 레드 대응\n- Gemini 3, DeepSeek 등 경쟁 모델 등장 시 단기 ‘코드 레드’ 체계로 빠르게 대응하는 내부 문화 유지\n- 경쟁 상황을 저위험 실험으로 인식하며, 제품 전략의 약점을 빠르게 식별하고 수정하는 계기로 활용\n- 신규 이미지 모델과 GPT-5.2 출시를 통해 기능 확장과 성능 개선을 연속적으로 진행 중\nChatGPT의 해자와 플랫폼 전략\n- ChatGPT는 여전히 시장 지배적 챗봇이며, 사용자 수는 수억 명 단위로 증가 중\n- 모델 성능 외에도 제품 완성도·브랜드·개인화 경험이 선택의 핵심 요인으로 작동\n- 소비자용 ChatGPT의 친숙함이 기업 도입을 촉진하며, 단일 AI 플랫폼 선호 현상이 관측됨\n모델 경쟁과 ‘상품화’에 대한 관점\n- 모델이 완전히 상품화되기보다는 용도별 강점이 다른 다수 모델 공존 전망\n- 최전선(frontier) 모델이 여전히 가장 큰 경제적 가치를 창출할 것으로 판단\n- 무료 또는 저가 모델이 존재하더라도 가장 지능적인 모델의 프리미엄 수요는 지속\nUI와 제품 형태의 진화\n- 현재 채팅 인터페이스는 범용성과 친숙함 덕분에 예상보다 오래 유지 중\n- 장기적으로는 작업별 맞춤 인터페이스를 AI가 동적으로 생성하는 방향 지향\n- 메시징 중심 UI에서 벗어나, 사용자 목표를 이해하고 비동기적으로 일하는 에이전트형 흐름을 상정\n메모리와 개인화\n- AI 메모리는 아직 초기 단계이며, 향후 사용자의 전 생애 맥락을 기억하는 수준으로 발전 가능성 제시\n- 인간 비서가 가질 수 없는 완전한 기억과 미세한 선호 학습이 핵심 차별점\n- 개인화는 소비자와 기업 모두에서 높은 전환 비용과 장기적 락인 효과를 생성\nAI 동반자와 관계 설정\n- 일부 사용자들은 AI와 정서적 연결과 동반자적 관계를 원함\n- 성인 사용자에게는 선택의 자유를 주되, 배타적·의존적 관계 유도는 제한하는 방향 유지\n- 건강한 사용과 위험한 사용 사이의 경계는 사회적 실험과 조정 과정이 필요함\n엔터프라이즈 전략과 지식 노동 변화\n- 모델 성숙과 소비자 성공을 기반으로 2026년부터 본격적인 엔터프라이즈 확장 계획\n- GPT-5.2 계열은 지식 노동 과제에서 전문가와 동등 또는 우위의 결과를 다수 영역에서 달성\n- 코딩 외에도 문서 작성, 분석, 기획 등 다양한 업무를 AI에 위임하는 구조가 확산 중\n일자리와 에이전트의 역할\n- 단기적 전환 충격은 존재하나, 장기적으로 인간의 역할은 관리·확장·의미 추구 방향으로 이동 전망\n- AI 에이전트가 팀 단위 업무를 수행하고, 인간은 범위와 책임을 확장하는 관리자 역할로 변화\n- 일의 형태는 달라지지만, 사회적 동기와 창의성은 지속될 것으로 판단\n대규모 연산과 과학적 발견\n- OpenAI는 AI의 최대 가치를 과학적 발견 가속에서 찾고 있음\n- 수학·물리·의학 연구에서 GPT-5.2가 이미 연구 워크플로를 바꾸기 시작한 사례 등장\n- 대규모 연산 투입은 신약 개발, 개인화 의료, 실시간 인터페이스 등으로 확장 가능성\n**1.4조** 달러 인프라와 수익 모델\n- 인프라 투자는 장기간에 걸쳐 집행되며, 연산 증가가 곧바로 매출 성장으로 연결되는 구조\n- 학습 비용 비중은 점차 감소하고, 추론과 활용 중심 수익이 주축이 될 전망\n- 현재까지는 연산 부족이 항상 성장을 제한했으며, 유휴 연산을 경험한 적은 없음\nAGI와 초지능에 대한 정의\n- 현재 모델은 매우 높은 지능을 보이지만, 자기 학습과 지속적 능력 확장은 아직 부족\n- AGI라는 용어는 정의가 불명확해졌으며, 논의의 초점은 ‘초지능’으로 이동 중\n- 초지능의 후보 정의로는 인간의 보조 없이도 최고 수준의 조직·국가·연구를 운영 가능한 시스템 제시\nIPO 가능성\n- OpenAI는 막대한 자본 수요와 주주 구조 변화로 2026년 전후 IPO 가능성 언급\n- 공개 기업 전환에 대한 개인적 선호는 낮지만, 공공 시장 참여의 의미는 인정\n- 민간 기업으로서의 유연성과 공개 기업의 책임 사이에서 균형을 모색 중",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25319",
    "category": "AI"
  },
  {
    "title": "UniFi 트래블 라우터",
    "content": "- 주머니에 넣고 전원을 켜면 UniFi 네트워크 환경을 그대로 휴대할 수 있는 소형 라우터\n- \n자동 위치 인식 정책, 라우팅 규칙, WiFi 브로드캐스트가 연결 즉시 활성화되어 동일한 네트워크 경험 제공\n- 기존 UniFi 장비와 즉시 연동되며, 수동 설정이나 재구성이 필요 없는 플러그 앤 플레이 구조\n- \nEthernet, WiFi, 5G 스마트폰 테더링 등 다양한 업링크를 지원하고, 호텔 등 캡티브 포털 로그인도 자동 처리\n- 이동 중에도 일관된 네트워크 제어와 보안 유지가 가능해 원격 근무나 출장 환경에서 유용함\n---\n\n## UniFi 트래블 라우터 개요\n- UniFi 트래블 라우터는 휴대 가능한 소형 네트워크 장치로, 사용자가 이동하더라도 동일한 UniFi 네트워크 환경을 유지할 수 있음\n전원을 켜면 기존 UniFi 설정이 그대로 적용되어 재구성이나 재설정이 불필요\n- 사용자는 어디서든 신뢰할 수 있는 동일한 네트워크 환경을 경험\n네트워크 자동 재구성 기능\n- 라우터가 연결되면 위치 기반 정책, 라우팅 규칙, WiFi 브로드캐스트가 즉시 활성화됨\n자동 지오 기반 정책 및 Teleport 활성화 기능 포함\n- \n즉시 라우팅 및 VPN 적용으로 보안 유지\n- UniFi 사이트에 바인딩하면 WiFi와 Teleport 설정이 자동 구성\n익숙한 장비와의 호환성\n- 기존 UniFi 장비와 도구를 그대로 사용할 수 있음\n전원을 켜면 장비들이 자동으로 연결되어 집과 동일한 LAN 동작 유지\n- \n수동 설정이나 장비 재등록이 필요 없음\n- 위치가 바뀌어도 연결 일관성 유지\n소형 폼팩터와 업링크 유연성\n- 작지만 다양한 업링크 옵션을 지원하는 구조\nEthernet, WiFi, 5G 스마트폰 테더링을 통한 연결 가능\n- \n호텔 네트워크의 캡티브 포털 로그인 자동 처리\n- \n고성능 라우팅을 지원하면서도 주머니 크기의 휴대성 확보\n- 여러 업링크를 단일 통합 제어 플레인에서 관리\n이동 중 연결 지속성\n- 사용자는 가볍게 이동하면서도 UniFi 경험을 유지할 수 있음\n출장, 여행, 원격 근무 등 다양한 환경에서 일관된 네트워크 품질과 보안 확보\n- “Travel light, stay connected”라는 메시지로 이동성과 연결성의 결합을 강조",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25317",
    "category": "개발"
  },
  {
    "title": "AI 시대, 당신의 앱은 이미 도태되고 있다 - llms.txt와 Schema.org의 중요성",
    "content": "LLM이 정보 탐색의 중심이 된 지금, 기존 SEO만으로는 부족합니다.\n\n---\n\n## 주요 통계\n- ChatGPT 주간 활성 사용자 **8억** 명\n- AI 챗봇 트래픽 전년 대비 **80.9%** 증가\n- Google 대 AI 사용자 비율 12개월 만에 절반으로 (10:1 → 4.7:1)\n기존 SEO는 Googlebot을 위해 설계되었습니다. 하지만 LLM은 다르게 작동합니다—네비게이션, 광고, JavaScript에 묻힌 콘텐츠를 찾기 어렵습니다.\n\n---\n\n## 해결책\n- \nllms.txt: AI에게 \"이 사이트에서 중요한 페이지\"를 알려주는 마크다운 파일\n- \nSchema.org: 구조화된 데이터로 GPT-4 정확도 **16%** → **54%** 향상\n실제 적용 예시: https://limelink.org/llms.txt\nSEO에서 GEO(Generative Engine Optimization)로의 전환, 어떻게 생각하시나요?",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25315",
    "category": "AI"
  },
  {
    "title": "10년간 부트스트랩으로 성장: 13명 팀이 연매출 €6.5M 달성",
    "content": "- \nDatoCMS는 2025년 한 해 동안 매출 **€**650만**, 전년 대비 **10%** 성장을 기록하며 10년 차 SaaS 기업으로서도 두 자릿수 성장을 유지\n- \nEBIT 마진 **65%** , SaaS 업계 상위 **5%** 수준의 수익성을 달성하며 ‘Rule of 40’을 **75%**로 초과\n- \n185개 에이전시 파트너, 340개 쇼케이스 프로젝트를 확보하며 협력 생태계 확장\n- \nHeroku에서 AWS Kubernetes로 완전 이전, 인프라 비용 **25%** 절감과 API 응답 속도 **50%** 개선을 실현\n- 10년간 외부 투자 없이 13명 소규모 팀으로 운영하며, 자율적·지속 가능한 성장 모델을 유지\n---\n\n## 재무 성과\n- 2025년 매출 **€6.5M**, 전년 대비 **10%** 성장\n10년 차 SaaS 기업으로서도 두 자릿수 성장을 유지\n- \nEBIT 마진 **65%** , 업계 평균(20~**40%**)을 크게 상회\nSaaS 상위 **5%** 수준의 수익성 확보\n- 성장률과 수익률을 합산한 Rule of 40 지표 **75%** 달성\n외부 자금 의존 없이 지속 가능한 운영 구조 유지\n파트너 네트워크 확대\n- \n185개 에이전시 파트너가 등록되어 전년 대비 증가\n실무 중심의 웹 제작사들이 DatoCMS를 채택\n- \n340개 쇼케이스 프로젝트 중 63개 신규 추가\n파트너 피드백을 반영해 기능 절반 이상을 개선\n- 파트너 프로그램을 통해 협업과 제품 품질 향상 추진\n제품 개선 및 기능 확장\n- 2025년 전반에 걸쳐 개발자 경험, 에디터 경험, 보안, AI 대응 등 전 영역 개선\n- \n타입 세이프티 강화: JavaScript 클라이언트가 스키마 기반 자동 타입 생성 지원\n- \n\n---\n\n## AI·LLM 대응\nllms-full.txt 문서 제공으로 AI 학습 친화적 구조 구축\n- \nMCP 서버로 AI 어시스턴트가 CMS 프로젝트와 직접 상호작용 가능\n- \nAI 번역 기능으로 OpenAI, Claude, Gemini, DeepL 연동\n- \n콘텐츠 편집 개선: 인라인 블록, 트리형 탭 뷰, 즐겨찾는 로케일, 고정 헤더 등 UI 개선\n- \nAPI·툴링 강화: CLI 명령어 cma:call, GraphQL 페이지네이션 500개로 확대\n- \n보안 및 거버넌스: API 토큰 삭제·활용 이력 관리, 기본 전체 접근 토큰 제거\n- \n워크플로우 개선: 레시피 마켓플레이스, 검증 강제 발행, 초안 저장 기능 추가\n플러그인 생태계 성장\n- \n공개 플러그인 30개 신규 등록, 비공개 플러그인 다수 추가\n- 주요 플러그인:\nAI Translations, Schema Import/Export, Asset Optimization, Custom Text Styles\n- 커뮤니티 주도의 확장성과 실용적 기능 중심의 생태계 강화\n인프라 독립 및 성능 향상\n- \nHeroku에서 AWS Kubernetes(EKS) 로 완전 이전\n9개월간 준비 후 2025년 6월 7일 전환, 1분 다운타임만 발생\n- 이전 후 성과:\nAPI 응답 속도 **50%** 단축, 인프라 비용 **25%** 절감, Realtime API 용량 10배 증가\n- \n64GB RAM AWS DB 인스턴스가 기존 Heroku 256GB보다 효율적\n- 주요 기술 스택 변화:\nTerraform 기반 IaC 구축\n- CDN을 Fastly → Cloudflare로 전환, 캐시 적중률 **85%**→**97%**\n- 스토리지를 AWS S3 → Cloudflare R2로 이전, 데이터 전송 비용 절감\n- \nPrometheus & Loki로 모니터링 비용 절감\n- 자체 kubectl 래퍼 cubo 개발로 Kubernetes 운영 단순화\n내부 운영 및 회계 자립\n- 외부 회계 대행을 중단하고 회계 전면 내재화\n실시간 재무 가시성 확보, 외부 보고 지연 해소\n- 인프라 이전과 동일하게 ‘통제 우선, 편의 후순위’ 원칙 적용\n팀 문화와 운영 철학\n- 창립 10주년을 맞아 전 직원이 토스카나에서 기념 행사 진행\n- \n13명 소규모 팀 유지, 외부 투자 없이 자립 경영 지속\n인력 확장보다 제품 품질, 수익성, 워라밸을 우선\n- “대규모 조직은 선택사항”이라는 철학 아래 VC 자금 없이 독립적 성장 유지\n향후 계획\n- 구체적 로드맵 공개 없음\n“하이프 사이클을 무시하고, 의미 있는 기능을 계속 출시하겠다”는 입장\n- 2026년에도 13명 체제 유지, 자율적 개발 중심 운영 지속 예정",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25312",
    "category": "AI"
  },
  {
    "title": "Snitch – 더 친숙한 ss/netstat 도구",
    "content": "- \nSnitch는 기존 ss나 netstat보다 사람이 보기 쉬운 네트워크 연결 검사 도구로, 터미널 UI(TUI)와 스타일링된 표 형식을 지원\n- \n실시간 인터랙티브 화면 또는 한 번에 출력되는 표 형식으로 연결 상태를 표시하며, TCP/UDP, 리스닝, 연결됨 상태 등 다양한 필터 제공\n- \nJSON·CSV 출력, DNS/서비스명 해석, 프로세스 감시 및 종료, 자동 업데이트 기능을 포함\n- \nHomebrew, Go, Nix, Arch Linux, Shell Script, Binary 등 여러 설치 방법을 지원하며, macOS의 Gatekeeper 경고 자동 해제 기능 포함\n- 개발자와 시스템 관리자가 네트워크 연결을 직관적으로 모니터링하고 스크립트 자동화에 활용할 수 있는 유용한 도구\n---\n개요\n- Snitch는 네트워크 연결을 시각적으로 탐색할 수 있는 도구로, ss나 netstat의 대체 도구로 설계\n- \nTUI 인터페이스 또는 스타일링된 표 출력을 통해 연결 상태를 표시\n- Linux와 macOS에서 동작하며, 루트 권한 또는 CAP_NET_ADMIN 권한이 필요할 수 있음\n설치 방법\n- \nHomebrew: brew install snitch 명령으로 설치 가능\n- \nGo: go install github.com/karol-broda/snitch@latest\n- \nNix/NixOS: nix-env -iA nixpkgs.snitch 또는 flake 입력으로 추가 가능\n- \nArch Linux (AUR) : yay -S snitch-bin 또는 paru -S snitch-bin\n- \nShell Script: curl -sSL ... | sh 명령으로 설치, 기본 경로는 ~/.local/bin 또는 /usr/local/bin\nmacOS에서는 설치 스크립트가 자동으로 quarantine 속성 제거\n- \nBinary 다운로드: GitHub Releases에서 Linux(.tar.gz, .deb, .rpm, .apk) 및 macOS(.tar.gz) 버전 제공\n빠른 시작\n- \nsnitch 실행 시 인터랙티브 TUI 실행\n- \nsnitch -l은 리스닝 소켓만 표시, snitch ls는 표 형식으로 출력 후 종료\n- \nsnitch ls -t -e는 TCP 연결된 세션만 표시, snitch ls -p는 파싱 가능한 단순 출력\n주요 명령\n- \nsnitch / snitch top : 실시간 갱신되는 연결 목록 표시\n옵션: -l(리스닝), -t(TCP), -e(연결됨), -i(갱신 주기)\n- 키 바인딩: j/k 이동, t/u TCP·UDP 전환, K 프로세스 종료, / 검색, q 종료 등\n- \nsnitch ls : 한 번에 표 출력, 터미널 높이를 초과하면 자동으로 pager 사용\n출력 형식: 기본 표, -o json, -o csv, -p(단순), --no-headers(헤더 제거)\n- \nsnitch json : JSON 형식 출력으로 스크립트 활용 가능\n- \nsnitch watch : 일정 간격으로 JSON 프레임 스트리밍\n- \nsnitch upgrade : 버전 확인 및 자동 업데이트\n필터 및 해석 옵션\n- 공통 플래그: -t(TCP), -u(UDP), -l(리스닝), -e(연결됨), -4(IPv4), -6(IPv6)\n- \n\n---\n\n## DNS 및 서비스명 해석\n- -resolve-addrs, --resolve-ports, --no-cache 옵션 지원\n- 병렬 DNS 조회 및 캐싱 수행, --no-cache로 캐시 비활성화 가능\n- \n세부 필터링: key=value 형식으로 프로세스명, 포트, 상태 등 지정 가능\n예: snitch ls proto=tcp state=listen, snitch ls proc=nginx\n출력 형식\n- \n기본 표 출력: 프로세스명, PID, 프로토콜, 상태, 로컬 주소·포트 표시\n- \n단순 출력(-p) : 파싱 가능한 텍스트 형태\n- \nJSON/CSV 출력: 스크립트 자동화 및 로그 분석에 활용 가능\n설정 및 환경 변수\n- 설정 파일: ~/.config/snitch/snitch.toml\nnumeric, dns_cache, theme(auto/dark/light/mono) 설정 가능\n- 환경 변수:\nSNITCH_THEME, SNITCH_RESOLVE, SNITCH_DNS_CACHE, SNITCH_NO_COLOR, SNITCH_CONFIG 등 지원\n시스템 요구사항\n- \nLinux 또는 macOS 환경 필요\n- Linux: /proc/net/*에서 데이터 읽기, 전체 프로세스 정보에는 루트 또는 CAP_NET_ADMIN 권한 필요\n- macOS: 시스템 API 사용, 전체 프로세스 정보에는 sudo 필요",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25311",
    "category": "튜토리얼"
  },
  {
    "title": "Show GN: Pic2Cook 사진 한 장으로 레시피를 만들어주는 서비스",
    "content": "메리 크리스마스! 🎄 연말 잘 보내고 계신가요?\n유튜브나 인스타에서 맛있는 음식 보다가 “이거 해먹어보고 싶은데…” 생각해본 적 있으시죠? 연말에 뭐 먹을지도 은근 고민되고요.\n그래서 사진 한 장으로 레시피를 만들어주는 서비스를 런칭했습니다!\nhttps://pic2cook.org\nGCP에서 Gemini와 Vertex AI를 사용해 직접 만들었고,\n실제로 따라 해먹어봤는데 기대보다 더 맛있더라고요.\n써보시고 별점만 남겨주셔도 큰 힘 됩니다.\n추천 코드를 프로필에서 서로 입력하시면 레시피를 1개 더 등록하실 수 있어요.\n아키텍처나 핵심 플로우, 기술 스택이 궁금하시면 여기서 보실 수 있습니다.\nGithub: pic2cook/README.md\n편하게 피드백 주시면 정말 감사하겠습니다.",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25307",
    "category": "AI"
  },
  {
    "title": "엡스타인 관련 문서의 일부 검은색 가림처리가 해킹으로 해제됨",
    "content": "- 미 법무부가 공개한 제프리 엡스타인 사건 관련 문서의 일부 가림처리가 포토샵이나 텍스트 복사로 쉽게 해제되는 것으로 드러남\n- 가려졌던 내용은 어린이 성학대 조장 및 은폐 행위, 금전 지급 내역, 증거 파기 지시 등을 포함\n- 특히 대런 인다이크와 리처드 칸을 상대로 한 버진아일랜드 민사소송 문서에서, 인다이크가 2015~2019년 사이 **40만** 달러 이상을 여성 모델들에게 지급한 사실이 드러남\n- 문서에는 세금 납부 내역이 재무제표에 반영되지 않은 부동산 거래도 포함되어 있으며, 법무부의 Epstein Files Transparency Act 기준과의 부합 여부가 불분명함\n- 이번 노출은 법무부의 문서 보안 관리와 비공개 기준의 적정성에 대한 의문을 제기함\n---\n\n## 문서 가림 해제 발견\n- 미 법무부가 공개한 제프리 엡스타인 관련 문서의 일부 가림처리가 포토샵 편집이나 텍스트 복사만으로 해제 가능함이 확인됨\n이로 인해 가려졌던 원문 텍스트가 월요일 저녁부터 소셜미디어를 통해 확산됨\n- 해당 문서는 버진아일랜드 주정부가 인다이크와 칸을 상대로 제기한 민사소송의 증거자료로, 엡스타인과 측근들이 아동 성학대를 조장하고 은폐한 방식을 기술함\n드러난 주요 내용\n- 문서의 85항에는 인다이크가 2015년 9월부터 2019년 6월까지 여성 모델 및 배우들에게 **40만** 달러 이상을 지급한 사실이 포함됨\n한 러시아 출신 모델은 3년 반 동안 매달 **8,333달러**씩 총 **38만** 달러 이상을 수령한 것으로 나타남\n- 또 다른 가려진 부분에서는 피고들이 성매매 및 학대 행위를 은폐하기 위해 증인들에게 거액을 지급하고, 변호사 비용을 대신 부담한 사실이 언급됨\n- 엡스타인은 피해자들에게 위협을 가하거나 명예를 훼손하는 기사 유포, 증거 파기 지시 등을 통해 범죄 노출을 막은 것으로 기록됨\n재정 관련 가림 해제 내용\n- 문서의 184~192항에서는 엡스타인이 설립한 회사들이 재무제표에 없는 부동산 세금 납부 내역을 포함함\n예를 들어, Cypress사는 2018년 말 자산으로 현금 **18,824달러**만 보고했지만, 같은 해 산타페 부동산세 **106,394.60달러**를 납부함\n- 2017년에도 유사하게 **55,770.41달러**와 **113,679.56달러**의 세금을 납부했으나, 재무제표에는 현금 **29,736달러**와 **150달러**의 비용만 기록됨\n법적 배경과 후속 조치\n- 버진아일랜드 검찰은 2022년 엡스타인 재단 및 인다이크, 칸과의 성매매 관련 민사소송을 **1억**500만** 달러와 리틀 세인트 제임스 섬 매각 수익 절반으로 합의함\n합의문에는 책임 인정 조항이 포함되지 않음\n- 인다이크는 연방 기소를 받지 않았으며, 2022년 Parlatore Law Group에 고용됨\n해당 로펌은 국방장관 피트 헥세스를 대리하고, 과거 도널드 트럼프의 기밀문서 사건 변호를 맡은 바 있음\n- 인다이크 및 로펌 측은 언론의 논평 요청에 응답하지 않음\n법무부의 대응 및 불확실성\n- 최근 제정된 Epstein Files Transparency Act는 법무부가 피해자 개인정보나 수사 중인 사건 관련 정보를 비공개할 수 있도록 허용함\n- 그러나 이번에 드러난 부동산 관련 자료가 해당 기준에 부합하는지 여부는 불분명하며, 법무부의 질의 응답은 아직 없음\n\n---\n\n## 사건의 의미\n- 단순한 PDF 가림처리의 기술적 허점이 민감한 성범죄 관련 정보 유출로 이어짐\n- \n공공문서 보안 관리와 법적 비공개 기준의 신뢰성에 대한 문제 제기\n- 엡스타인 사건의 투명성 확보와 피해자 보호 간 균형이 다시 논의될 가능성 있음",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25306",
    "category": "AI"
  },
  {
    "title": "Show GN: valdex - 사용하기 쉬운 Typescript 런타임 타입 단언 라이브러리",
    "content": "안녕하세요.\n일반적으로 Typescript 백엔드를 개발하다보면 오류가 많이 발생하는 지점은 컴파일 타임에서 잡히지 않는 런타임 부분이라는 걸 아실겁니다.\n이를테면 DB row를 파싱하는 레포지토리 구현체 또는 외부 API 통신 부분에서 제일 많이 발생합니다.\nDB에서 받아온 값이 예상과 달라 런타임에서 undefined로 값이 주입되거나, 캐스팅을 할 때 외부에서 온 원시 값을 제대로 파악하지 못해 휴먼 에러가 발생하는 부분들입니다.\n레포지토리를 구현할 때에도 interface로 외부에서 오는 원시 값에 대한 타입을 덕지덕지 적어야 하는데요. 어쩌면 사소한 문제일 수 있지만 편하게 사용하려고 만들었습니다.\n실제로 외부 API 부분과 DB 구현체 코드가 훨씬 간결해졌고, 런타임에서 발생하는 오류도 줄어들었습니다.\n팀에서 백엔드에 사용해보다가 괜찮은 것 같아 npm에 배포하였습니다.\nimport { validate } from 'valdex'; \nconst data: unknown = await fetchData(); \nvalidate(data, { \nname: String, \nage: Number, \nactive: Boolean \n}); \n// TypeScript now knows the exact type of data \ndata.name // string \ndata.age // number \ndata.active // boolean \n위 예시는 외부에서 데이터를 가져오는 상황을 가정합니다. axios를 쓰든, mysql2를 쓰든, postgres pg를 쓰든 적용 가능합니다. unknown 타입으로 들어온 값에 대한 검증을 진행하고 올바르다면 이후 제어 흐름에서 해당 값은 validate() 에서 정의한 값으로 단언 (asserts) 됩니다.\n물론 zod 같은 라이브러리를 사용할 수도 있습니다.\nzod와 valdex가 다른 점은, valdex는 스키마를 인스턴스로 만들어서 사용하지 않고, 데이터를 받아오는 곳 안에서 선언적으로 데이터 타입을 다룰 수 있다는 겁니다. 인터페이스를 수정하려고, DTO 클래스를 수정하려고 위 아래를 왔다갔다 하며 코드를 수정하지 않아도 됩니다.\nnpm i valdex \nnpm에서 설치하고 사용해볼 수 있습니다.\nGithub: https://github.com/asheswook/valdex",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25303",
    "category": "AI"
  },
  {
    "title": "연봉 값을 하는 엔지니어의 비밀: \"모르는 것(Ambiguity)\"을 \"할 수 있는 것\"으로 바꾸는 기술",
    "content": "## 핵심 요점\n- 시니어와 중간급 엔지니어를 가르는 결정적 차이는 불확실하고 모호한 문제를 구체화하는 능력임.\n- 시니어의 가치는 코딩 기술 그 자체가 아니라, 프로젝트의 리스크를 제거하고 실행 가능한 계획으로 전환하는 데서 옴.\n- 현재의 채용 관행(알고리즘 테스트 등)은 이러한 역량을 평가하는 데 실패하고 있음.\n- 진정한 성장은 코딩 전 단계에서 문제를 명확히 정의하는 연습에서 시작됨.\n---\n서론: 시니어 엔지니어의 정의 재고\n- 일반적으로 시니어 엔지니어는 아키텍처 설계, 커뮤니케이션, 리더십 등 다양한 기술 목록으로 정의됨.\n- 그러나 직함, 연봉, 연차를 제외했을 때 시니어 이상의 엔지니어를 구분 짓는 단 하나의 핵심 기술은 '모호함을 줄이는 능력' 임.\n- 다른 모든 역량(기술적 실행 등)은 이 핵심 기술을 바탕으로 파생되는 결과물임.\n본론\n1. 문제 해결 방식의 차이: 명확함 대 모호함\n- \n중간급(Mid-level) 엔지니어: 명확한 명세(Spec)와 제약 조건이 주어졌을 때 뛰어난 성과를 냄. 잘 정의된 문제를 해결하는 데 능숙함.\n- \n시니어 엔지니어: \"성능 개선 필요\", \"사용자 불만 증가\", \"확장성 고려\"와 같이 불분명하고 추상적인 요구사항을 받았을 때 차별화됨.\n- 시니어는 모호한 문제를 단순히 수행하는 것이 아니라, 이를 분석하고 해체하여 구체적인 과제로 변환함.\n2. 시니어의 핵심 역할은 프로젝트의 리스크 제거이다\n- \n\n---\n\n## 시니어 엔지니어는 거대하고 추상적인 문제 앞에서 다음과 같은 접근을 통해 불확실성을 해소함\n- \n남들이 하지 않는 본질적인 질문 제기.\n- \n중요한 신호와 소음(Noise)의 분리.\n- \n즉시 실행할 것과 미룰 것의 우선순위 결정.\n- \n이 과정은 프로젝트의 리스크를 낮추고(De-risking), \"무엇인지 모르는 상태\"를 \"실행 가능한 작은 프로젝트와 제거할 요소\"로 정리함.\n- \n시니어가 이를 잘 수행하면 프로젝트가 매끄럽게 진행되어 겉보기에는 쉬워 보이지만, 실제로는 사전에 막대한 '보이지 않는 작업'이 수행된 결과임.\n3. 모호함을 해소하기 위한 구체적 접근법\n- 시니어 엔지니어는 코딩에 앞서 문제를 명확히 하기 위해 다음과 같은 질문을 던짐:\n- \n문제의 본질: 우리가 원하는 솔루션이 아니라, 해결하려는 실제 근본 문제는 무엇인가?\n- \n사용자 정의: 구체적으로 누구의 어떤 고통을 해결하려 하는가? (\"사용자\"라는 포괄적 단어 지양)\n- \n가정 검증: 현재 계획에 숨겨진 잘못된 가정은 무엇인가?\n- \n리스크 평가: 우리의 판단이 틀렸을 때 발생할 최악의 상황은 무엇인가?\n4. 채용 시스템의 한계와 잘못된 시니어 선발\n- 대부분의 기업은 기술 스택 목록이나 알고리즘 문제 해결(LeetCode) 능력에 집중하여 채용을 진행함.\n- 이러한 방식은 모호한 제품 요구사항을 실행 가능한 계획으로 변환하는 능력을 검증하지 못함.\n- 결과적으로 코딩 실력은 뛰어나지만, 불완전한 명세 앞에서는 아무것도 하지 못하는 '무늬만 시니어' 엔지니어가 양산됨.\n결론: 성장을 위한 제언\n- 아키텍처나 커뮤니케이션 능력도 중요하지만, 이는 '무엇을 만들지' 가 명확해진 이후에야 가치를 발휘함.\n- 모호함을 줄이지 못한 상태에서의 기술적 우수성은 '잘못된 문제를 우아하게 해결하는 것'에 불과함.\n- 자신이 시니어 레벨인지 판단하는 기준은 추상적인 과제를 받았을 때 타인의 명확화 작업을 기다리는지, 아니면 스스로 팀이 실행할 수 있도록 구체화하는지에 달려 있음.\n- 이는 타고난 재능이 아니라 연습의 영역이므로, 모호한 티켓(업무)을 받았을 때 즉시 코딩하기보다 문제를 구체화하는 훈련을 시작해야 함.",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25300",
    "category": "트렌드"
  },
  {
    "title": "실시간 데이터의 '마감 시간' 정하기: 스트림 처리와 워터마크 전략",
    "content": "[블로그] 실시간 데이터의 '마감 시간' 정하기: 스트림 처리와 워터마크 전략\n(원문: https://fredly.dev/watermark-pattern/)\n실시간 스트림 처리 시스템에서 데이터의 '적시성'과 '정확성' 사이의 트레이드오프를 해결하는 워터마크 패턴(Watermark Pattern)을 소개합니다. 저자의 실무 경험(CCTV 메타데이터 처리)을 바탕으로, 분산 환경에서 발생하는 데이터 순서 뒤바뀜 문제를 어떻게 엔지니어링적으로 해결하는지 다룹니다.\n\n---\n\n## 주요 내용\n- 이벤트 시간 vs 처리 시간: 네트워크 지연으로 인해 발생하는 두 시간축의 간극과 이로 인한 데이터 순서 역전 문제 분석.\n- 공항 게이트 비유: 무한정 기다릴 수 없는 실시간 시스템에서 '마감 시간'으로서의 워터마크 역할 설명.\n- 워터마크의 수학적 정의:\nWatermark = max(Event Time) - Lag\n시스템이 \"이 시점 이전 데이터는 모두 수집되었다\"고 신뢰할 수 있는 논리적 임계점 설정.\n- 시스템 구성 요소:\n타임스탬프 추출기(Timestamp Extractor): 데이터 내 실제 생성 시간 추출.\n- 워터마크 생성기(Watermark Generator): 단조 증가(Monotonically Increasing)를 보장하며 마커 패킷 발행.\n- 지연 처리 정책(Late Data Policy): 마감 후 도착한 지연 데이터(Late Data)에 대한 대응 시나리오(무시, 별도 저장, 결과 업데이트).\n- 엔지니어링 인사이트: 완벽한 정합성보다는 '허용 가능한 오차' 내에서의 적시성이 중요한 실시간 데이터 파이프라인 설계 전략.",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25297",
    "category": "AI"
  },
  {
    "title": "실무 중심의 심도 있는 내용을 다루는 최고의 엔지니어링 블로그는 무엇인가요?",
    "content": "이런 글이 많은 실무 중심의 엔지니어링 블로그를 알려주세요\n- 기술 개념을 명확하고 간결하게 설명\n- 실제 구현 과정, 장단점, 그리고 실패 사례를 소개\n- 잘 구성되어 있고 읽기 쉬움\n- 엔지니어링 결정이 비즈니스 또는 제품 성과와 어떻게 연결되는지 보여줌\n---\n\n## HN 댓글에 있는 링크들 정리\n회사 / 조직 기술 블로그\n- Facebook Engineering — https://engineering.fb.com/\n- Netflix Tech Blog — https://netflixtechblog.com/\n- Stripe Engineering — https://stripe.com/blog/engineering\n- Uber Engineering — https://www.uber.com/en-US/blog/engineering/\n- LinkedIn Engineering — https://engineering.linkedin.com/\n- Spotify Engineering — https://engineering.atspotify.com/\n- Tailscale Blog — https://tailscale.com/blog\n- DoorDash Engineering — https://careersatdoordash.com/engineering-blog/\n- Dropbox Tech — https://dropbox.tech/\n- Shopify Engineering — https://shopify.engineering\n- Cloudflare Blog — https://blog.cloudflare.com/\n- Discord Blog — https://discord.com/blog\n- Riot Games Tech Blog — https://technology.riotgames.com/\n- ClickHouse Engineering — https://clickhouse.com/blog?category=engineering\n- MongoDB Engineering Blog — https://www.mongodb.com/company/blog/channel/engineering-blog\n- Allegro Tech Blog — https://blog.allegro.tech/\n- Tweag Blog — https://www.tweag.io/blog\n- TigerBeetle Blog — https://tigerbeetle.com/blog/\n- Oxide RFDs — https://rfd.shared.oxide.computer/\n- Google Project Zero — https://projectzero.google/archive.html\n- LWN.net — https://lwn.net/\n개인 기술 블로그 / 개인 사이트\n- Julia Evans (jvns) — https://jvns.ca/\n- Wizard Zines — https://wizardzines.com/\n- Simon Willison — https://simonwillison.net/\n- Simon Willison TIL — https://til.simonwillison.net/\n- Armin Ronacher — https://lucumr.pocoo.org/\n- antirez (Redis) — https://antirez.com/\n- Brandur Leach — https://brandur.org/\n- Sam Who — https://samwho.dev\n- Chris Wellons (nullprogram) — https://nullprogram.com/\n- Francesco Mazzoli — https://mazzo.li/archive.html\n- Eli Bendersky — https://eli.thegreenplace.net/\n- Rachel by the Bay — https://rachelbythebay.com/w/\n- Raymond Chen (Old New Thing) — https://devblogs.microsoft.com/oldnewthing/\n- Random ASCII — https://randomascii.wordpress.com/\n- Charles Bloom — https://cbloomrants.blogspot.com/\n- fabiensanglard.net — https://fabiensanglard.net/\n- Gernot Heiser — https://microkerneldude.org/\n- BI Cortex — https://bicortex.com/\n- Mark Litwintschik — https://tech.marksblogg.com/\n- eblog.fly.dev — https://eblog.fly.dev/index.html\n- infrequently.org — https://infrequently.org\n- lcamtuf Substack — https://lcamtuf.substack.com/\n- Making Software — https://www.makingsoftware.com/\n큐레이션 / 집계 / 목록\n- engineering.fyi — https://engineering.fyi/\n- diff.blog — https://diff.blog/\n- HN Algolia (engineering blog 검색) — https://hn.algolia.com/?query=engineering%20blog\n- OPML 엔지니어링 블로그 모음 — https://peterc.org/misc/engblogs.opml\n- ML Engineering Blogs 목록 — https://github.com/primaprashant/ml-engineering-blogs\n- lessnews.dev — https://lessnews.dev\n- devblogs.sh — https://devblogs.sh\n- engineeringblogs.xyz — https://engineeringblogs.xyz/\n- jurakovic dev-links — https://jurakovic.github.io/dev-links/#blogs-general\n- jurakovic dev news — https://jurakovic.github.io/dev-links/news/\n- feeds.carmo.io — https://feeds.carmo.io\n- High Scalability (favorites) — http://highscalability.squarespace.com/all-time-favorites/",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25296",
    "category": "AI"
  },
  {
    "title": "Fabrice Bellard, MicroQuickJS 공개",
    "content": "- \nMicroQuickJS(MQuickJS) 는 임베디드 시스템용으로 설계된 초경량 JavaScript 엔진으로, 약 10kB RAM과 100kB ROM만으로 실행 가능\n- \nQuickJS와 유사한 속도를 유지하면서도 메모리 사용량을 줄이기 위해 트레이싱 가비지 컬렉터와 UTF-8 문자열 저장 방식을 채택\n- 지원 언어는 ES5에 가까운 제한된 JavaScript 부분집합이며, 오류 가능성이 높은 구문을 금지하는 엄격 모드(strict mode) 만 허용\n- \nREPL 도구 mqjs 를 통해 스크립트 실행, 바이트코드 저장 및 메모리 제한 설정이 가능하며, 생성된 바이트코드는 ROM에서 직접 실행 가능\n- 전체 엔진과 표준 라이브러리가 ROM에 상주해 빠른 초기화와 낮은 메모리 소비를 실현, 임베디드 환경에서의 JavaScript 실행 효율성을 높임\n---\n소개\n- \nMicroQuickJS(MQuickJS) 는 임베디드 시스템을 대상으로 하는 JavaScript 엔진으로, 10kB RAM과 100kB ROM(ARM Thumb-2 코드 포함)에서 동작\n속도는 QuickJS와 유사\n- \nES5에 가까운 부분집합만 지원하며, 비효율적이거나 오류 가능성이 높은 구문을 금지하는 엄격 모드(strict mode) 로만 작동\n- QuickJS와 코드 일부를 공유하지만, 내부 구조는 메모리 절약을 위해 완전히 다르게 설계됨\n트레이싱 가비지 컬렉터, CPU 스택 미사용, UTF-8 문자열 저장 방식 사용\nREPL\n- REPL 명령은 mqjs이며, 스크립트 실행, 평가, 인터랙티브 모드, 메모리 제한 설정, 바이트코드 저장 등을 지원\n예: ./mqjs --memory-limit 10k tests/mandelbrot.js\n- \n- o 옵션으로 컴파일된 바이트코드를 파일로 저장 가능\n저장된 바이트코드는 ./mqjs mandelbrot.bin으로 실행 가능\n- 바이트코드는 CPU의 엔디언 및 워드 길이(32/64비트) 에 따라 다르며, -m32 옵션으로 32비트용 바이트코드 생성 가능\n- \n- -no-column 옵션으로 디버그 정보의 열 번호 제거 가능\n엄격 모드\n- \nstrict mode만 허용하며, with 키워드 사용 불가, 전역 변수는 반드시 var로 선언해야 함\n- \n배열의 구멍(hole) 허용 안 함\n예: a[10] = 2는 TypeError 발생\n- 구멍이 있는 배열이 필요하면 일반 객체 사용\n- \n전역 eval만 지원, 지역 변수 접근 불가\n- \n값 박싱(value boxing) 비지원 (new Number(1) 등)\nJavaScript 부분집합\n- \nstrict mode 기반, ES5 호환성 중심\n- \nArray 객체는 구멍이 없으며, 범위를 벗어난 인덱스 접근은 오류\n- \nfor in은 객체의 자체 속성만 순회, for of는 배열만 지원\n- \n글로벌 객체는 존재하지만 getter/setter 불가, 직접 생성한 속성은 전역 변수로 노출되지 않음\n- \n정규식(Regexp) 은 ASCII만 대소문자 구분 처리, /./은 UTF-16 대신 유니코드 코드포인트 단위로 매칭\n- \n문자열 함수는 ASCII만 처리 (toLowerCase, toUpperCase)\n- \nDate는 Date.now()만 지원\n- 추가 지원 기능:\nfor of, Typed arrays, \\u{hex} 문자열 리터럴\n- \nMath 함수: imul, clz32, fround, trunc, log2, log10\n- \n지수 연산자, 정규식 플래그(s, y, u) , 문자열 함수(replaceAll, trimStart, trimEnd), globalThis\nC API\n- \nC 라이브러리 의존성 최소화, malloc, free, printf 미사용\n- \n메모리 버퍼를 직접 제공해야 하며, 엔진은 해당 버퍼 내에서만 메모리 할당\n예: ctx = JS_NewContext(mem_buf, sizeof(mem_buf), &js_stdlib)\n- \n가비지 컬렉션 방식으로 인해 JS_FreeValue() 호출 불필요\n- 객체 주소는 할당 시마다 이동 가능하므로, JSValue 포인터 사용 권장\nJS_PushGCRef() / JS_PopGCRef()로 안전한 참조 관리\n- \n표준 라이브러리는 ROM에 저장 가능한 C 구조체로 컴파일되어, 빠른 초기화와 낮은 RAM 사용량 달성\n- \n바이트코드 실행은 ROM에서 가능하며, JS_RelocateBytecode()로 재배치 후 JS_LoadBytecode()와 JS_Run()으로 실행\n- \n수학 라이브러리(libm.c) 와 부동소수점 에뮬레이터 내장\n내부 구조 및 QuickJS 비교\n- \n가비지 컬렉션: 참조 카운팅 대신 트레이싱·압축형 GC 사용\n메모리 단편화 방지, 객체 크기 축소\n- \n값 표현: CPU 워드 크기(32/64비트)에 맞춰 설계\n31비트 정수, 유니코드 코드포인트, 부동소수점, 메모리 블록 포인터 저장 가능\n- \n문자열은 UTF-8로 저장, QuickJS의 8/16비트 배열 방식보다 효율적\n- \nC 함수는 단일 값으로 저장 가능, 속성 추가 불가\n- \n표준 라이브러리는 ROM에 상주하며, RAM 객체 최소화로 빠른 엔진 초기화 가능\n- \n바이트코드는 스택 기반이며, 간접 참조 테이블을 통해 읽기 전용 처리\nGolomb 코드로 행·열 번호 압축\n- \n컴파일러는 QuickJS와 유사하지만 비재귀적 파서를 사용해 C 스택 사용량 제한\n파스 트리 없이 단일 패스 바이트코드 생성\n테스트 및 벤치마크\n- 기본 테스트: make test\n- QuickJS 마이크로 벤치마크: make microbench\n- \nOctane 벤치마크(엄격 모드용 수정 버전)는 별도 다운로드 가능\n실행: make octane\n라이선스\n- \nMIT 라이선스로 배포\n- 소스코드 저작권은 Fabrice Bellard와 Charlie Gordon 소유",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25292",
    "category": "AI"
  },
  {
    "title": "CECOT 내부 – 60 Minutes [영상]",
    "content": "- '60 Minutes'의 보도 세그먼트로, 빌 위테커(Bill Whitaker) 등 특파원들이 엘살바도르 CECOT 교도소의 인권 실태를 취재한 내용을 담고 있음\n- 미국에서 엘살바도르의 악명 높은 최고 보안 교도소인 CECOT으로 추방된 베네수엘라 남성들이 겪은 조직적인 고문과 인권 침해 실태를 고발\n- 트럼프 행정부는 전시 권한을 발동해 적법 절차 없이 수백 명의 베네수엘라인을 추방했으며, 엘살바도르 정부에 이들을 수용하는 대가로 4.7백만 달러를 지불함\n- 미 당국은 이들을 위험한 테러리스트로 규정했으나, 실제 조사 결과 폭력 전과가 있는 비율은 단 **3%** 에 불과한 것으로 드러남\n- 수용자들은 빛과 통풍이 차단된 '섬(The Island)' 이라 불리는 독방 구금, 24시간 점등, 지속적인 구타 및 성적 학대 등 극심한 가혹 행위를 증언함\n- UC Berkeley 연구팀은 위성 이미지와 공개 데이터를 분석하여 교도소 내 고문 도구와 가혹한 수용 시설의 실체를 디지털 증거로 입증함\n- 이 사건은 이민자 억제를 위해 인권 기록이 열악한 제3국 교도소를 활용하는 비인도적 추방 정책의 위험성을 시사함\n---\n\n## 베네수엘라 이민자의 엘살바도르 강제 이송 및 배경\n- 미국 정부는 베네수엘라 남성 252명을 본국이 아닌 엘살바도르의 CECOT(테러 수용 센터) 으로 강제 이송함\n트럼프 행정부는 이들을 흉악한 몬스터 및 테러리스트로 지칭하며, 법적 절차를 건너뛰기 위해 수 세기 전의 전시 권한을 행사함\n- 미국은 이들을 수용하는 조건으로 엘살바도르에 **470만** 달러를 지불하는 계약을 체결함.\n- \nHuman Rights Watch 조사 결과, 이송된 인원의 약 절반은 범죄 기록이 전혀 없었음.\nICE 자체 기록에서도 폭력 범죄로 유죄 판결을 받은 인원은 **3%** 에 불과하며, 대부분은 단순 입국 위반 등 미결 상태였음.\n- 미 당국은 베네수엘라 갱단 Trendier를 식별한다며 문신 등을 근거로 점수를 매기는 결함 있는 기준을 적용해 이들을 갱단원으로 분류함.\nCECOT 교도소 내 고문 및 비인도적 수용 환경\n- 수용자들은 도착 즉시 머리카락이 깎인 채 지옥과 같은 환경에 던져졌다고 증언함\n교도관들은 진압봉과 주먹으로 수용자들을 피가 날 때까지 무자비하게 구타했으며, 안면을 벽에 부딪쳐 치아가 부러지는 등의 부상을 입힘.\n- 많은 수용자가 교도관들로부터 성적 학대를 당했으며, 이는 다수의 인원에게 조직적으로 자행됨.\n- \n'섬(The Island)' 이라 불리는 징벌방은 빛과 환기가 전혀 없는 암흑 상태로 유지됨.\n교도관들은 30분마다 문을 두드려 수용자들에게 심리적 트라우마를 가했으며, 한 번 갇히면 며칠 동안 구타가 이어짐\n- 기본적인 생존 조건조차 보장되지 않는 비위생적 시설 상태가 확인됨.\n깨끗한 물이 제공되지 않아 수용자들은 목욕물이나 변기물을 마셔야 했으며, 부상을 입어도 물만 마시라는 식의 의료 방치가 지속됨.\n- 매트리스나 시트도 없는 4단 철제 침대에 수용되었으며, 외부 가족과의 연락은 완전히 차단됨.\n디지털 분석을 통한 증언 검증 및 국제 표준 위반\n- \nUC Berkeley 인권 센터 학생 연구팀은 오픈 소스 데이터를 활용해 수용자들의 증언을 검증함\n위성 이미지 분석을 통해 수용자들이 구금되었던 정확한 건물 위치를 지도화하고, 인플루언서들의 영상에서 독방의 구조를 확인하여 증언과 일치함을 입증함.\n- 교도소 내 무기고 영상을 통해 증언에 등장한 T자형 진압봉의 존재를 확인하고, 수용자들에게 강요된 고통스러운 신체 자세가 일상적으로 행해짐을 파악함.\n- 교도소 소장은 인터뷰를 통해 24시간 점등 시스템을 직접 인정하며 열악한 환경에 대한 자부심을 드러냄.\n이는 수용자의 정신을 혼미하게 만드는 행위로, 유엔(UN)의 수용자 처우 최소 기준을 명백히 위반한 것임\n- \nDHS(국토안보부) 장관 Christy Gnome은 교도소를 방문해 수용자들을 테러리스트라 부르며 찬사하는 영상을 촬영함\n그러나 영상 배경에 등장한 남성들은 베네수엘라인이 아닌, 엘살바도르 갱단 표식을 가진 현지 수용자들로 밝혀져 정치적 선전 도구로 활용되었음이 드러남\n석방 및 향후 정책적 우려\n- 4개월간의 수용 생활 후, 252명의 남성들은 베네수엘라에 구금되었던 미국인 10명과 교환되어 카라카스로 송환됨.\n- 트럼프 행정부는 엘살바도르 외에도 고문 기록이 있는 남수단 및 우간다 등 제3국과 유사한 추방 계약을 추가로 추진하고 있음.\n- \nHuman Rights Watch는 이번 사례가 미국에 오려는 이민자들에게 공포를 심어주기 위해 이들을 본보기로 삼은 비인도적 행위라고 비판함.",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25291",
    "category": "트렌드"
  },
  {
    "title": "스토커를 위한 넷플릭스처럼 작동하는 Flock 카메라 유출 사건",
    "content": "- \nFlock 카메라 시스템의 영상 접근 방식이 외부 유출로 드러나며, 사생활 침해 위험이 부각됨\n- 유출된 영상은 자동차 번호판 인식과 위치 추적 기능을 포함해 개인 이동 경로를 식별할 수 있는 수준\n- 영상 접근이 보안 인증 없이 가능하거나 취약한 구조로 되어 있어, 제3자가 감시 데이터를 열람할 수 있음\n- 이러한 문제는 공공 감시 네트워크의 투명성 부족과 데이터 보호 미비를 드러냄\n- \n도시 감시 기술의 남용과 개인정보 보호 강화 필요성을 환기시키는 사례로 평가됨\n---\n\n## Flock 카메라 시스템의 유출 문제\n- 영상에서는 Flock Safety 카메라 네트워크의 내부 영상 접근이 외부로 유출된 사례를 다룸\n해당 시스템은 차량 번호판을 자동 인식하고, 특정 차량의 이동 경로를 추적하는 기능을 가짐\n- 유출된 데이터는 위치, 시간, 차량 정보를 포함해 개인의 이동 패턴을 파악할 수 있는 수준임\n- 영상에서는 이러한 접근이 인증 절차 없이 가능하거나 매우 취약한 구조로 되어 있음을 지적함\n사생활 침해 및 보안 취약성\n- 유출된 영상은 일반 사용자나 제3자도 감시 데이터를 열람할 수 있는 위험을 보여줌\n- 이러한 구조는 스토킹, 불법 감시, 위치 추적 등에 악용될 수 있는 가능성을 내포함\n- 영상은 이를 “스토커를 위한 넷플릭스”에 비유하며, 감시 데이터의 무분별한 접근을 경고함\n공공 감시 기술의 문제점\n- 영상은 공공 감시 인프라의 투명성 부족과 데이터 보호 미비를 비판함\n- Flock 시스템이 도시 전역에 설치되어 있음에도, 관리·감독 체계가 불분명하다는 점을 지적함\n- 이러한 문제는 감시 기술의 확산이 개인의 자유와 안전을 위협할 수 있음을 보여줌\n개인정보 보호와 기술 책임\n- 영상은 데이터 보안 강화와 접근 통제 개선의 필요성을 강조함\n- 감시 기술을 운영하는 기업이 공공 안전과 개인정보 보호의 균형을 유지해야 함을 지적함\n- \n투명한 감사 체계와 외부 검증 절차가 필수적임을 언급함\n사회적 함의\n- 이번 유출 사건은 스마트 감시 기술의 윤리적 한계를 드러낸 사례로 제시됨\n- \n기술적 편의성과 시민의 프라이버시 보호 간 균형이 핵심 과제로 부각됨\n- 영상은 감시 사회로의 진입에 대한 경각심을 촉구하며 마무리됨",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25289",
    "category": "튜토리얼"
  },
  {
    "title": "가비지 컬렉션 핸드북",
    "content": "- \n자동 메모리 관리의 원리를 체계적으로 다루는 기술서로, 최신 하드웨어와 소프트웨어 환경에서의 가비지 컬렉션 알고리듬을 포괄적으로 설명\n- 1996년과 2012년에 이어 출간된 제2판으로, 지난 60년간 축적된 연구와 개발 성과를 통합\n- 병렬, 증분, 동시, 실시간 수집 등 최신 고성능 수집기 기법을 포함하고, 의사코드와 그림으로 알고리듬을 구체적으로 제시\n- \n지속성(persistence) 및 에너지 인식형 수집을 다루는 새로운 장이 추가되고, 3,400편 이상의 관련 논문 데이터베이스와 연계\n- 현대 프로그래밍 언어 대부분이 가비지 컬렉션을 채택한 상황에서, 개발자가 적절한 수집기 선택과 구성을 이해하는 데 필수적인 참고서\n---\n\n## 제2판 개요\n- 1996년판 Garbage Collection과 2012년판 The Garbage Collection Handbook의 후속으로, 자동 메모리 관리 분야의 최신 상태를 반영\n기술 발전으로 인해 메모리 관리가 더욱 복잡하고 중요해진 점을 반영\n- 지난 60년간 연구자와 개발자가 축적한 지식을 하나의 접근 가능한 틀로 통합\n- \n하드웨어·소프트웨어 발전이 가비지 컬렉션에 제기한 새로운 과제를 다룸\n프로그램 실행 환경 변화가 고성능 수집기 설계자와 구현자에게 미치는 영향을 탐구\n- 단순·전통적 알고리듬뿐 아니라 병렬, 증분, 동시, 실시간 수집을 포함\n- 알고리듬과 개념은 의사코드와 그림으로 설명\n책의 주요 특징\n- 1996년 및 2012년판의 완전하고 최신의 후속서 제공\n- \n병렬·동시·실시간 수집 알고리듬을 포괄적으로 다룸\n- \n상용 고성능 수집기의 동작을 상세히 설명\n- 런타임 시스템과의 복잡한 인터페이스 문제를 다룸\n- \n90쪽 이상 추가, 지속성과 에너지 인식형 수집에 관한 신규 장 포함\n- 약 3,400편의 관련 논문 데이터베이스와 연계\n전자책 및 번역판\n- 전자책은 인쇄본보다 확장된 형태로, 37,000개 이상의 하이퍼링크를 포함\n장, 절, 알고리듬, 그림, 용어집, 색인, 연구 논문 등으로 연결\n- 2016년에 중국어 및 일본어 번역판이 출간되어 독자층 확대\n웹 리소스\n- 온라인 서지 데이터베이스는 약 3,400편의 가비지 컬렉션 관련 논문을 포함\n일부 항목에는 초록이 있으며, 대부분은 URL 또는 DOI 제공\n- 지속적으로 업데이트되며, BibTeX, PostScript, PDF 형식으로 다운로드 가능",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25287",
    "category": "트렌드"
  },
  {
    "title": "대규모 코드베이스로 LLM 확장하기",
    "content": "- \n대규모 코드베이스에서 LLM을 효과적으로 활용하기 위해서는 ‘가이드(guide)’와 ‘감독(oversight)’에 대한 투자가 핵심임\n- \n가이드는 문맥과 환경을 제공해 LLM이 더 나은 선택을 하도록 돕고, 감독은 결과를 검증하고 방향을 제시하는 역할을 수행함\n- \n프롬프트 라이브러리를 구축해 코드베이스의 규칙, 문서, 모범 사례를 LLM이 이해할 수 있도록 하는 것이 중요함\n- \n기술 부채 관리와 코드 구조의 단순성, 모듈화, 일관성이 LLM의 코드 이해력과 생산성 향상에 직접적으로 연결됨\n- \n자동화된 감독과 검증 체계를 통해 LLM이 안전하고 일관된 코드를 생성하도록 보조하는 것이 장기적 확장성의 핵심임\n---\n\n## LLM 확장을 위한 핵심 개념\n- LLM을 대규모 코드베이스에 적용하는 방법은 아직 정립되지 않았으나, 가이드와 감독에 대한 투자가 가장 효과적인 접근으로 제시됨\n- \n가이드(Guidance) 는 LLM이 올바른 선택을 하도록 돕는 문맥과 환경을 의미하며, 감독(Oversight) 은 생성된 결과를 검증하고 방향을 조정하는 역할을 담당함\n가이드에 대한 투자\n- LLM이 한 번의 시도로 고품질 코드를 생성하는 ‘원샷(one-shotting)’ 을 달성하려면, 명확한 가이드가 필요함\n반대로, 결과가 부적절해 수동 수정이 필요한 경우는 재작업(rework) 으로 비효율적임\n- LLM은 코드 내의 모든 선택(변수명, 함수 구조, 기술 스택 등)을 생성하므로, 프롬프트가 비즈니스 요구만 담고 나머지는 추론 가능하거나 인코딩된 상태가 이상적임\n프롬프트 라이브러리 구축\n- \n프롬프트 라이브러리는 코드베이스의 문서, 모범 사례, 구조적 맵 등을 포함한 LLM용 문맥 집합임\nLLM의 출력이 빗나갈 때마다 “무엇을 명확히 해야 했는가”를 검토하고 이를 라이브러리에 추가\n- \n포괄성과 간결성의 균형이 중요함\n- 예시에서는 @prompts/How_To_Write_Views.md, @prompts/The_API_File.md 등 문서를 LLM에 제공해 기능 개발을 안내함\n- 프롬프트가 충분히 구체적이어야 하지만, 생성된 코드의 모든 줄을 검토해야 함\n환경과 코드 품질\n- \n기술 부채(technical debt) 가 많은 코드베이스는 LLM 활용 효율을 저하시킴\nMeta의 사례에서 기술 부채로 인해 자동화 목표 달성이 어려웠다고 언급됨\n- \n깨끗한 코드, 모듈화, 명확한 네이밍, 단순한 구조가 LLM의 이해도와 정확도를 높임\n- Django 예시에서는 각 앱의 진입점을 _api.py 파일로 두어 LLM이 필요한 기능을 빠르게 찾을 수 있도록 구조화함\n예: visit_api.handoff_to_doctor(user) 형태로 외부 접근을 단일화\n- \n_api 패턴을 프롬프트 라이브러리에 명시해 LLM이 올바른 위치를 참조하도록 유도\n감독에 대한 투자\n- LLM 자동화는 엔지니어를 대체하기보다 팀을 강화하는 방향으로 접근해야 함\n- 감독은 팀, 정렬(alignment), 워크플로우에 대한 투자로 이어짐\n팀 차원에서는 설계 역량 향상이 중요하며, 이는 아키텍처 품질로 연결됨\n- \n디자인 역량 강화 방법으로는 책·블로그·코드 읽기, 마스터워크 복제, 직접 구현 연습 등이 제시됨\n예: TLDraw, SerenityOS Jakt 등의 코드 분석을 통해 설계 감각을 확장\n자동화된 감독\n- 일부 설계 검증은 프로그램적으로 자동화 가능함\n예: 타입 오류나 규칙 위반을 환경에서 즉시 피드백\n- \n‘안전성(safety)’ 은 추상화를 보호하는 것임\nPierce의 정의에 따르면, 안전한 언어는 프로그래머가 의도치 않게 추상화를 깨뜨리지 않도록 보장함\n- 예시: Django 앱 간 내부 파일 직접 접근을 금지하는 규칙을 AST 기반 검사 스크립트로 자동화\nfrom visit import logic.internal_file 형태의 불법 접근을 탐지\n검증(Verification)\n- 설계와 구현 외에도 검증 단계(코드 리뷰, QA) 가 품질 확보에 필수적임\n- 작업량이 증가할수록 검토 속도가 병목이 되므로, 다음과 같은 개선 방안이 제시됨\n개발 환경 없이도 QA 수행 가능하도록 진입 장벽 완화\n- 테스트 데이터 생성 등 테스트 작성이 간단한 환경 구축\n- 반복되는 PR 피드백을 문서화해 LLM이 일부 리뷰를 자동 수행하도록 지원\n- \n보안 규칙을 프레임워크 기본값으로 내장\n결론 및 추가 관찰\n- LLM은 새로운(greenfield) 프로젝트에서 특히 잘 작동함\n기존 맥락이 없고 일관성 요구가 낮기 때문\n- 프로젝트가 커질수록 일관성과 모듈화가 생산성을 좌우함\n검증된 구성요소를 재사용하는 모듈형 구조가 효율적 개발의 핵심임",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25285",
    "category": "AI"
  },
  {
    "title": "초음파 암 치료: 음파로 종양을 공격",
    "content": "- \n초음파 기반 ‘히스토트립시(histotripsy)’ 기술이 열이나 방사선 없이 종양을 파괴하는 비절개 치료법으로 주목받고 있음\n- \nHistoSonics의 Edison 시스템은 물이 채워진 막을 통해 초점을 맞춘 초음파를 전달해 기포를 생성·붕괴시켜 암세포를 물리적으로 파괴함\n- 이 기술은 간암 치료에 대해 FDA 승인을 받았으며, 2026년에는 신장암과 췌장암 임상시험이 예정되어 있음\n- 초음파로 파괴된 종양 잔여물이 면역 반응을 자극해 다른 암세포 공격을 돕는 효과도 관찰됨\n- \nJeff Bezos 등이 참여한 **22억**5천만 달러 규모 인수로 연구개발이 가속화되며, 비침습적 암 치료의 새로운 축으로 부상 중임\n---\n\n## 히스토트립시 기술의 원리와 발전\n- \n히스토트립시(histotripsy) 는 초음파로 생성된 기포의 팽창과 붕괴(공동현상) 를 이용해 종양 조직을 물리적으로 분해하는 기술\n기존에는 공동현상이 예측 불가능하고 위험한 부작용으로 여겨졌으나, 2001년 미시간대 연구진이 이를 제어해 암 조직을 파괴하는 방법을 개발\n- 연구 초기에는 열 발생으로 인한 정상 조직 손상이 문제였으나, 초강력 초음파를 짧은 펄스와 긴 간격으로 조합해 열 축적을 방지\n이 방식은 세포를 미세하게 분해해 조직을 액화시키며, 절개·방사선·열 없이 종양을 제거하는 수술 형태를 가능하게 함\nHistoSonics와 Edison 시스템\n- 2009년 설립된 HistoSonics는 이 기술을 상용화하기 위해 초음파 장비를 개발\nEdison 시스템은 물로 채워진 막을 통해 초점을 맞춘 초음파를 전달, 내부에서 기포를 형성해 종양을 파괴\n- 2023년 간암 치료용으로 FDA 승인을 획득, 2026년에는 신장암 임상시험 완료 및 췌장암 대규모 시험이 예정\n췌장암은 5년 생존율이 **13%**에 불과한 치명적 질환으로, 성공 시 큰 의학적 진전이 기대됨\n기술적 특징과 임상 성과\n- HistoSonics의 시스템은 로봇 제어 및 컴퓨터 가이드 기술을 결합해 정밀한 치료를 구현\n정밀 조정 시 혈관 등 섬유조직은 손상되지 않으며, 파괴된 조직은 자연적으로 체내에서 제거됨\n- 초기 췌장암 임상에서 집중 초음파로 깊은 종양을 제거하는 데 성공\n워싱턴대 연구진은 이 기술이 췌장 종양 제거가 가능하고 환자에게 잘 견딜 수 있음을 확인\n면역 반응과 복합 치료 가능성\n- 히스토트립시는 면역 반응을 자극해 초음파가 직접 닿지 않은 암세포 공격을 돕는 것으로 관찰됨\n종양 파괴 후 남은 단백질 조각이 면역계에 암세포를 인식시키는 역할을 함\n- 연구자들은 면역치료와의 병용을 통해 이 효과를 강화하는 방안을 탐색 중\n향후 연구 및 산업적 확장\n- 2025년 8월, Jeff Bezos 등이 포함된 투자 컨소시엄이 HistoSonics를 **22억**5천만 달러에 인수, R&D 자금 확보\n- 엔지니어들은 초음파 영상 대신 X선 기반 가이드 시스템과 실시간 피드백 기능을 개발 중\n초음파 반향을 분석해 조직 파괴 정도를 실시간 표시하는 기능 포함\n- 이러한 발전이 실현되면, 히스토트립시는 간·신장·췌장을 넘어 다양한 암 치료에 적용될 가능성\n단순한 기포 현상 연구에서 출발한 기술이 비침습 의학의 새로운 축으로 자리잡을 전망임",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25282",
    "category": "AI"
  },
  {
    "title": "NIST, 지난주 정전으로 UTC 기준시보다 5마이크로초 늦어져",
    "content": "- 미국 콜로라도 볼더의 NIST 시간 시설이 정전으로 수일간 가동 중단, 백업 발전기 고장으로 UTC 기준시와 최대 5μs 오차 발생\n- \nNTP 서버 6대를 운영하는 이 시설은 발전기 고장에도 불구하고 시간 편차를 5μs 이하로 유지, 일반 사용자에게는 영향이 거의 없었음\n- \n과학 연구기관과 항공우주 기업 등 정밀 타이밍 의존 기관에는 영향이 있을 수 있어, NIST가 직접 협력 중\n- \nGPS 및 WWV-Ft. Collins 시스템이 정상적으로 백업 역할을 수행해, 미국 전체 시간 인프라의 중복성이 입증됨\n- 이번 사례는 GPS 의존 위험성과 타이밍 인프라의 취약성을 보여주며, 대체 PNT 시스템 개발 필요성을 부각함\n---\n\n## 정전과 시간 오차 발생\n- 콜로라도 볼더의 NIST 캠퍼스가 시속 160km(100mph) 이상의 강풍으로 인해 전력회사가 화재 위험 방지를 위해 전력 공급을 차단\n캠퍼스 전체가 봉쇄되어 직원 출입이 불가능, 복구 지연 발생\n- \n백업 발전기 중 하나가 이틀 후 고장, NTP 서버의 주요 집합(clock ensemble) 전원이 끊김\n- \nTime Realization and Distribution Group의 책임자인 Jeff Sherman은 부정확한 시간 송출을 막기 위해 서버를 차단할 가능성까지 검토\n- 다행히 다른 건물의 시계 시스템이 시간 신호를 전송할 수 있었고, 일부 직원이 현장에 남아 비상 전력 재라우팅으로 복구\n- \n배터리 백업(UPS) 이 발전기 교체 전까지 시간을 유지, 결과적으로 UTC와의 편차는 5μs 이하로 기록\nNTP 서버 운영과 영향 범위\n- NIST는 6개의 주요 NTP 서버를 통해 인터넷 시간 서비스를 제공\n- \nsntp time-a-b.nist.gov 명령 결과, 일반 사용자의 네트워크 지연 오차는 약 35밀리초(35,000μs) 수준으로, 5μs 오차는 무시 가능한 수준\n- 따라서 서버를 중단하지 않고 유지했으며, 정확도는 평소보다 약 5,000배 낮았지만 대부분의 사용자에게 영향 없음\n- \n대학, 항공우주, 과학 연구기관 등은 미세한 오차에도 민감해, NIST가 이들과 직접 협력해 보정 작업 진행\n- \n미국 GPS 시스템이 WWV-Ft. Collins 캠퍼스로 자동 전환, 전체 서비스 중단 없이 유지\n시간 인프라의 취약성과 대체 기술\n- 저자는 Raspberry Pi 기반 GPS 시계 2대를 사용해 자체 NTP 서버를 운영하며, GPS 의존의 위험성을 지적\n- \nCISA는 미국의 GPS 과의존 위험을 이미 경고했으며, 정부는 대체 PNT(Position, Navigation, Timing) 기술 개발을 추진 중\n- \nBroadcast Positioning System(BPS) 이 GPS 대체 후보로 논의되고 있음\n- 저자는 루비듐 원자시계와 GPSDO를 사용해 수 나노초 단위의 정확도를 유지, GPS 신호 장애 시에도 수개월간 시간 유지 가능\n- 그러나 과학, RF, 미디어, 금융 분야 등은 나노초 단위 정밀도를 요구하며, 대부분 NIST 기준시를 참조\n교훈과 시스템 신뢰성\n- 이번 사건은 NIST의 재난 대응 체계가 실제로 작동했음을 입증, “미세한 오차 속에서도 정상 운영”을 보여줌\n- \n중복 전력, 다중 시계, GPS 백업 체계가 결합되어 전국 시간 인프라의 안정성을 유지\n- 저자는 “타이밍 인프라는 매우 취약하며, 다중 백업이 필수”라는 점을 강조\n- \n마이크로초 단위 위기 상황에서도 NIST 팀이 문제를 해결, 대부분의 사용자가 이를 인지하지 못할 정도로 복구 완료",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25281",
    "category": "개발"
  },
  {
    "title": "Show GN: 428억 토큰 쓰는 사람들 줄 세워보기",
    "content": "TLDR: https://github.com/junhoyeo/tokscale\n배경\n- 올해 상반기 출시된 Claude Code를 시작으로, OpenAI Codex CLI, Google Gemini CLI 등 여러 LLM Provider들이 본격적으로 에이전틱 코딩 툴(Agentic Coding Tool)을 제공하기 시작함. 그럼에도 불구하고 대부분의 일은 직접 하고 있었음\n- 그러나 친구가 OpenCode 셋업을 공유해 준 뒤로 훨씬 적극적으로 사용하게 됨. 여러 에이전트를 병렬로 돌려서 서로 리뷰시키거나, 탐색/구현/검증을 분리해서 작업하면 (즉, 토큰을 더 많이 사용하면 사용할수록...) 성능이 눈에 띄게 올라감. 이후 친구는 자기 셋업을 오픈소스로 공개해 Oh-My-OpenCode 라는 이름으로 2.5k+ 스타를 받음 (https://github.com/code-yeongyu/oh-my-opencode)\n- 친구를 만나고 한 달 새 **50억**개 넘는 토큰을 사용하며 광신도가 됨 (Claude Max plan 계정의 주간 사용 한도를 꽉꽉 채워서 쓰는 중, 사실 부계 만들었다가 정지도 당함). 생각보다 에이전틱 워크플로우를 활용하는 사람이 적다는 것도 알게 됨\n아이디어의 시작\n- \nccusage 라는 Claude Code 토큰 사용량 추적 툴을 알게 됨\n- \n\"클로드 코드 사용량 세계 1등 개발자\" (진형님!) 를 다룬 글을 읽고, \"어떻게 토큰 사용량이 1등인 걸 알았지?\" 라는 생각이 들어 검색하니 ccusage에서 가져온 데이터를 통해 만든 리더보드인 viberank (https://github.com/sculptdotfun/viberank)라는 작은 사이트가 있었음 (현재 유지되는지는 모르겠음)\n- \n하지만 두 프로젝트 모두 OpenCode, Codex CLI (ccusage 는 일부 지원), Gemini CLI 등 다른 클라이언트에 대한 데이터는 지원하지 않았음\n- \n마침 토큰 생성량을 깃허브 컨트리뷰션 그래프(Contribution Graph)로 보여주면 좋겠다는 샤워실 생각(shower thoughts)도 있었음. 개발자들이 깃허브에 익숙하기도 하고, 스스로를 채찍질할 수 있는 목표를 정하기 쉬운 형태라고 개인적으로 생각했기 때문\nIsometric Contributions (https://github.com/jasonlong/isometric-contributions) - 무려 10년 된 오픈소스 크롬 확장 프로그램. 깃허브 contribution graph를 3D 아이소메트릭으로 렌더링해줌 → 여기서 3D 그래프 아이디어를 가져옴\n- 깃허브 컨트리뷰션 그래프는 다양한 컬러 테마 아이디어를 참고\n- 프론트엔드에서는 obelisk.js (https://github.com/nicklockwood/obelisk.js)로 3D 아이소메트릭 렌더링 구현\n- \n원래 빠른 시간 안에 해키한 프로덕트 만들어서 반응 보고 관심받는 것을 좋아함 (이전 글 참고)\n- \nCLI/TUI 형태로 bunx (Bun 생태계에서 npx에 해당하는 녀석) 로 부담 없이 실행할 수 있고, API 서버로 submit 해서 데이터를 공유, 리더보드에 이름을 올릴 수 있는 플랫폼을 만들기로 결심\n프로젝트 이름: Tokscale\n- \n카르다쇼프 척도(Kardashev Scale) (https://ko.wikipedia.org/wiki/카르다쇼프_척도)에서 영감을 받음\n- \n문명의 기술 수준을 에너지 소비량으로 분류하는 척도임 (유형 I = 행성, II = 항성, III = 은하)\n- \nAI 시대에 토큰은 새로운 에너지라는 생각. \"planetary developer\"에서 \"galactic code architect\"까지 올라가는 여정을 시각화하자는 컨셉\n- \n일론 머스크가 \"전력이 돈이다(Electricity is money)\" 라고 했음\nAI·데이터센터 시대에 성능의 한계는 연산이 아니라 전력 공급량\n- GPU 성능보다 전력 확보·냉각·효율이 경쟁력\n- \n이걸 개인 개발자 레벨로 끌어내리면?\nLLM API를 쓸 때 우리가 지불하는 것 = 토큰\n- 토큰을 더 많이, 더 효율적으로 쓰는 쪽이 더 많은 코드를 생산\n- 토큰 = AI 시대의 개인 에너지 단위가 될 것\n- \nAI가 전기를 돈으로 바꾸는 기계라면, 에이전틱 코딩 툴은 토큰을 코드로 바꾸는 기계\n- \n그래서 Tokscale = Token + Kardashev Scale\n\"planetary developer\"에서 \"galactic code architect\"까지 올라가는 여정을 시각화하자는 컨셉\n- 토큰 소비량으로 개발자의 AI 활용 수준을 측정\nTUI 구현\n- OpenTUI (https://github.com/sst/opentui)를 사용해서 터미널 UI를 만듦\n- OpenTUI는 SST에서 개발한 TUI 프레임워크로, React의 Ink와 달리 Solid.js 기반이고 네이티브 Zig 엔진으로 zero-flicker 렌더링을 제공함 (최근 OpenCode 와\n- 4개 뷰(Overview, Models, Daily, Stats) + 키보드/마우스 내비게이션\n- 컨트리뷰션 그래프에 적용되는 9가지 컬러 테마: Green, Halloween, Teal, Blue, Pink, Purple, Orange, Monochrome, YlGnBu (깃허브 컨트리뷰션 그래프 커뮤니티에서 만든 테마들임)\n- 차트는 Unicode block 문자(▁▂▃▄▅▆▇█)로 렌더링. 모델별로 다른 색상으로 스택해서 표시\n데이터 가져오는 게 느림 → Rust 네이티브 모듈\n- 처음에는 TypeScript로 JSON 파일들을 파싱했는데 이게 너무 느렸음\n- napi-rs (https://napi.rs/)를 사용해서 Rust 네이티브 코드로 전환\n- 약 8.5배 성능 향상 달성:\n파일 탐색: ~500ms → ~50ms (10x)\n- JSON 파싱: ~800ms → ~100ms (8x, simd-json (https://github.com/simd-lite/simd-json) 사용)\n- 집계: ~200ms → ~25ms (8x, rayon (https://github.com/rayon-rs/rayon) 병렬 처리)\n- 메모리도 약 **45%** 절감 (스트리밍 파싱, zero-copy 문자열 처리)\n- OpenTUI에 맞게 bunx 지원하고 npx를 드롭. Bun 런타임 필수로 변경\nTypeScript CLI 에서 Bun.spawn을 사용해서 native Rust 모듈과 통신하는 서브프로세스를 실행하고, stdin/stdout으로 JSON 데이터를 주고받는 구조임\n- (OpenCode 를 너무 많이 쓰다 보니 이것도 제 머신에서는 느려졌긴 함 ㅜㅜ)\n데이터 리텐션 문제\n- 에이전틱 코딩 툴들은 전체 기록을 세션(session) 이라고 부르며, .으로 시작하는 히든 디렉토리에 저장함\nClaude Code: ~/.claude/projects/ (JSONL)\n- OpenCode: ~/.local/share/opencode/storage/message/ (개별 JSON)\n- Codex CLI: ~/.codex/sessions/ (이벤트 기반 JSONL)\n- Gemini CLI: ~/.gemini/tmp/*/chats/ (JSON)\n- Claude Code와 Gemini CLI는 기본 30일 리텐션 기간이 존재하며, 끝나면 세션 데이터가 삭제됨\n- 이걸 알고 나서 아까워하는 사람들이 많았음. README에 비활성화 방법 상세히 기재함\nClaude Code: ~/.claude/settings.json에 \"cleanupPeriodDays\": 9999999999 추가\n- OpenCode와 Codex CLI는 모든 세션 파일이 영구 보존됨 (삭제 기능 자체가 없음)\nCursor IDE 연동\n- 지금은 더 이상 사용하지 않지만 한동안 Cursor IDE를 썼던 적이 있음 (이 또한 나의 소중한 데이터니 연동해야 함)\n- Cursor는 로컬 세션 파일이 아닌 API 기반 CSV export를 지원하고 있어서 데이터를 얻을 수 있었음\n- 개발자 도구를 통해 세션 토큰(WorkosCursorSessionToken)이 있으면 인증이 가능하다는 것을 알게 됨\nNetwork 탭에서 cursor.com/api/* 요청의 Cookie 헤더에서 찾거나\n- Application → Cookies에서 직접 복사\n- tokscale cursor login | status | logout 형태로 만들어서 깔끔하게 관리\n깃허브 연동 (OAuth)\n- Device Flow 인증 방식으로 구현\n- tokscale login → 브라우저 열림 → 코드 입력 → 토큰 발급\n- tokscale submit으로 사용량 데이터를 리더보드에 업로드\n- 제출된 데이터는 Level 1 검증을 거침 (수학적 일관성, 미래 날짜 없음, 중복 감지 등)\n토큰 가격 계산\n- LiteLLM의 가격 데이터베이스 (https://github.com/BerriAI/litellm)에서 실시간 가격 정보를 가져옴\n- 1시간 TTL로 ~/.cache/tokscale/pricing.json에 디스크 캐시\n- 입력/출력/캐시 읽기/캐시 쓰기/추론 토큰 모두 별도 계산 지원\n- 구간별 가격(tiered pricing, 200k 토큰 이상)도 지원\nWrapped 2025\n- Spotify Wrapped에서 영감받은 연간 리뷰 이미지 생성 기능 (연말 기대하시라)\n- tokscale wrapped 실행하면 PNG 이미지 생성\n- @napi-rs/canvas (https://github.com/Brooooooklyn/canvas)로 이미지 렌더링, @resvg/resvg-js (https://github.com/nicklockwood/resvg-js)로 SVG→PNG 변환\n- Figtree 폰트를 Google Fonts에서 다운로드해서 캐싱\n- 포함 내용: 총 토큰, Top 3 모델, Top 3 클라이언트, Top 3 에이전트, 메시지 수, 활동 일수, 비용, 연속 기록(streak), 기여 그래프\n현재 병목 & 고민\n- 매번 로컬에서 긁어와서 느리고, 데이터베이스에 업로드해야 하는 게 무거움\n- 현재는 diff 기반 incremental submission 최적화를 검토중. 날짜별로 해시를 만들어서 변경된 부분만 업로드하는 방식을 차용하려 함 (과거 날짜 데이터가 수정될 여지를 열어두려고 함)\n모든 코드는 Oh-My-OpenCode가 짬\n- 진짜로 거의 모든 코드를 에이전트가 작성함\n- 423개 이상의 커밋, 4개 언어 README (EN, KO, JA, ZH-CN) 포함\n- 깃허브에 스크린샷 여러 개 올려서 예쁘게 만듦 (이건 사람의 손길이 일부 들어갔다는 것을 인정하지만, 프로젝트 전체를 만드는 동안 IDE 를 직접 열어서 코딩한 시간이 30분도 채 안 된다는 것은 확신할 수 있음)",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25278",
    "category": "AI"
  },
  {
    "title": "항상 문제는 TCP_NODELAY다",
    "content": "- 분산 시스템의 지연(latency) 문제를 디버깅할 때 가장 먼저 확인해야 하는 항목이 TCP_NODELAY 설정임\n- \nNagle 알고리듬은 1984년 RFC896에서 제안된 방식으로, 작은 패킷 전송 시 TCP 헤더 오버헤드를 줄이기 위해 설계됨\n- 그러나 지연된 ACK(delayed ACK) 메커니즘과 결합될 때, 데이터 전송이 ACK 수신까지 지연되어 지연 민감형 애플리케이션의 성능을 악화시킴\n- 현대 데이터센터 환경에서는 RTT가 매우 짧고, 대부분의 시스템이 이미 큰 메시지를 전송하므로 Nagle 알고리듬의 이점이 거의 사라짐\n- 따라서 현대 분산 시스템에서는 TCP_NODELAY를 기본으로 활성화해야 하며, Nagle 알고리듬은 더 이상 필요하지 않음\n---\n\n## Nagle 알고리듬의 배경\n- 1984년 John Nagle의 RFC896은 키보드 입력처럼 작은 데이터 전송 시 발생하는 40바이트 헤더 대비 1바이트 데이터의 **4000%** 오버헤드 문제를 해결하기 위해 제안됨\n당시 문제는 사용자가 한 글자씩 입력할 때마다 작은 패킷이 전송되어 네트워크 효율이 낮아지는 현상\n- 해결책은 이전 데이터가 ACK되지 않은 상태에서는 새로운 세그먼트를 전송하지 않도록 제한하는 방식\n- 이 접근은 당시 네트워크 환경에서는 효과적이었으나, 지연 시간(latency) 이 중요한 현대 시스템에는 부적합\nNagle 알고리듬과 Delayed ACK의 상호작용\n- \nDelayed ACK(RFC813, RFC1122)는 수신 측이 즉시 ACK를 보내지 않고, 응답 데이터가 생기거나 타이머가 만료될 때까지 ACK를 지연시키는 방식\n- Nagle 알고리듬은 ACK를 기다리며 전송을 멈추고, delayed ACK는 ACK를 늦추므로 양쪽이 서로 기다리는 교착 상태가 발생\n- John Nagle 자신도 이 조합을 “끔찍한 조합”이라 표현하며, 두 기능이 독립적으로 도입되었지만 함께 사용될 때 지연을 유발한다고 지적\n현대 환경에서의 문제점\n- 데이터센터 내 RTT는 약 500μs, 동일 리전 내에서도 수 밀리초 수준으로 매우 짧음\n- 이런 환경에서 한 RTT만큼 전송을 지연하는 것은 성능 손실로 이어짐\n- 또한 현대 분산 시스템은 TLS, 직렬화, 프로토콜 오버헤드 등으로 인해 이미 충분히 큰 메시지를 전송하므로, 단일 바이트 패킷 문제는 거의 존재하지 않음\n- 작은 메시지 최적화는 이제 애플리케이션 계층에서 처리되고 있음\nTCP_NODELAY의 필요성\n- \n지연에 민감한 분산 시스템에서는 TCP_NODELAY를 활성화해 Nagle 알고리듬을 비활성화하는 것이 권장됨\n이는 “비효율적”이거나 “잘못된 설정”이 아니라, 현대 하드웨어와 트래픽 특성에 맞는 선택\n- 저자는 TCP_NODELAY가 기본값이 되어야 한다고 주장\n일부 “write() 호출마다 전송”하는 코드가 느려질 수 있으나, 그런 코드는 근본적으로 수정되어야 함\n기타 관련 옵션\n- \nTCP_QUICKACK 옵션은 ACK 지연을 줄이지만, 이식성 문제와 비일관적 동작으로 인해 근본 해결책이 아님\n- 핵심 문제는 커널이 애플리케이션이 의도한 시점보다 데이터를 오래 보유하는 것이며, write() 호출 시 즉시 전송되어야 함\n결론\n- Nagle 알고리듬은 과거 네트워크 효율을 높이기 위한 훌륭한 발명이었으나,\n현대의 고속 네트워크와 분산 시스템 환경에서는 오히려 지연을 초래하는 구시대적 기능\n- 따라서 TCP_NODELAY를 항상 활성화하는 것이 현대 시스템 설계의 기본 원칙으로 제시됨",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25277",
    "category": "개발"
  },
  {
    "title": "그래서 도대체 피지컬 AI가 뭔데?",
    "content": "전문보기, https://maily.so/kkumaeunsonyeon/posts/10z3095jzlw\n바야흐로 인공지능이 소프트웨어를 넘어 물리적 실체와 결합해 현실 세계에서 자율적으로 이해하고, 행동할 수 있는 기술인 '피지컬 AI (Physical Artificial Intelligence)' 가 AI 패러다임의 상승 기류를 타고 산업 분야 혁신의 핵심으로 비상 중이라고 해도 과언이 아닙니다.\n이번 긱리포트에선 피지컬 AI의 기본 개념들과 기술적 특징 그리고 국내 동향에 대해 알아보겠습니다.\n피지컬 AI의 개념\n피지컬 AI (Physical Artificial Intelligence)란?\n인공지능이 소프트웨어 영역을 넘어 실제 물리적 기기와 결합해 현실 공간에서 자율적으로 인지, 판단, 행동할 수 있는 차세대 기술을 의미합니다. 즉, AI 알고리즘과 물리적 실체(형체, 재료, 구동계 등)가 공동으로 설계, 학습, 적응하여 현실 세계에서 자율적으로 행동, 목적 달성을 수행하는 시스템을 뜻하는 것으로써, 단순한 소프트웨어(디지털 AI)가 아닌 '감지 → 판단 → 물리적 행동 → 피드백' 의 구조를 갖는 행동형 지능으로 로봇, 자율 주행 자동차, 스마트 팩토리 등 다양한 하드웨어 플랫폼에 AI를 분리하여 메모리 환경에서 예측하고 작업을 수행하는 능력을 발휘할 수 있습니다.\n- 피지컬 AI는 센서(카메라, 라이다 등) 를 통해 환경을 인식하고, 수집된 데이터를 바탕으로 스스로 판단해 물리적 행동을 실행함.\n- 기존의 '소프트웨어 AI' 는 디지털 데이터 분석을 통한 정보 제공이 핵심이지만 피지컬 AI는 물리적 세계와 직접 상호작용하여 실시간 피드백을 반영하는 동시에 복잡한 실제 환경 속에 적응함.\n피지컬 AI 주요 구성 및 기술과 역할\n피지컬 AI의 시스템은 보통 모듈 단위로 설명을 할 수 있습니다. 센서(카메라, 라이다 등)를 통해 환경을 인식하고 수집된 데이터를 기반으로 자체적으로 문제를 해결합니다. 디지털 데이터 상의 내용과 정보를 제공하는 소프트웨어 AI와 달리 피지컬 AI는 메모리 환경에서 직접 연구하고 복합적인 현실 상황에 적응해 나간다는 점을 대표적인 차이점으로 꼽을 수 있습니다. 즉, 환경을 설계하는 센서, 데이터 처리 및 심층 학습 AI 모델, 상황 구성 및 의사 결정 내용, 실행을 담당하는 구성, 주요 모든 프로세스를 통합하는 제어 시스템으로 구성되어 있습니다.\n하드웨어 모듈\n- 구조, 재료, 구동기, 센서로 구성됨\n- 기구학적 설계 물리 시스템의 행동 가능성을 정의함\n감지(Sensing) 모듈\n- 물리적 세계 상태를 디지털 데이터로 변환해 입력\n- 멀티모달 센서 융합(비전+라이다+촉각+힘)으로 환경 및 자기 상태를 실시간 추정함.\n- 최근들어 MLMs(World Models)와 결합하는 고차원적 상황 이해에 사용됨.\n인지(Perception) 및 해석 모듈\n- 센서 데이터를 의미 있는 정보로 해석하는 AI 영역\n의사결정(Decision Making) 모듈\n- 인지된 정보를 바탕으로 행동을 선택하는 핵심 지능\n제어(Control) 모듈\n- 의사결정을 실제 물리 동작으로 변환\n구동(Actuation) 모듈\n- 실제 물리적 행동을 수행하는 하드웨어 모듈 요소\n학습(Learning) 및 적응 모듈\n- 환경 변화에 따라 성능을 개선하는 장기 지능 요소\n시스템 인프라 및 통신 모듈\n- 전체 피지컬 AI를 지탱하는 기반 구조\n안전, 윤리, 신뢰 모듈\n- 실제 세계에서 AI 적용을 위한 핵심 필수 보조 요소\n기술적 특징과 산업 분야별 주요 기업\n피지컬 AI 핵심 기술에는 강화학습, 시뮬레이션(디지털 트윈), 멀티모달 AI(텍스트, 영상, 센서 데이터 통합 분석), 실시간 제어 네트워크, 로봇 파운데이션 모델 등이 포함.복잡한 물리적 환경에서 효율성과 안전성을 유지하며, 사람과 기계 사이 자연스러운 협업이 가능하도록 설계됨.제조 현장 자동화 로봇, 물류창고 내 자율 이동 로봇, 스마트 팩토리, 헬스케어 및 재활 로봇까지 다양한 산업분야에 빠르게 적용되고 있음.피지컬 AI 시스템은 스스로 환경을 학습하며 적응할 수 있고 센서, 카메라, 라이다 등이 수집하는 실시간 정보를 바탕으로 최적화된 판단과 행동을 지속적으로 업데이트함.피지컬 AI는 단순한 반복적 자동화를 넘어 자율성, 적응성, 판단력, 인간-기계간 협업 역량까지 시스템으로 진화하고 있음.산업 현장, 물류, 서비스, 의료, 컨슈머 디바이스 등에서 효율성, 정밀성, 안정성, 혁신성을 크게 높일 수 있으며, 비즈니스 모델과 고객 가치 설계에서 새로운 국가적 산업 경쟁우위를 제공함.",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25275",
    "category": "AI"
  },
  {
    "title": "일러스트로 이해하는 트랜스포머",
    "content": "- \n트랜스포머 모델의 구조와 작동 원리를 시각적으로 설명하며, 어텐션 메커니즘이 어떻게 병렬 학습과 번역 성능을 향상시키는지 보여줌\n- 모델은 인코더와 디코더 스택으로 구성되며, 각 인코더는 셀프 어텐션과 피드포워드 신경망 두 층으로 이루어짐\n- \n셀프 어텐션 계산 과정을 벡터와 행렬 수준에서 단계별로 설명하고, 멀티헤드 어텐션이 다양한 표현 공간을 학습하도록 확장함\n- \n포지셔널 인코딩, 잔차 연결, 레이어 정규화, 디코더의 마스킹 어텐션 등 핵심 구성 요소를 구체적으로 시각화함\n- 이 글은 트랜스포머의 기본 개념을 쉽게 이해하도록 돕는 대표적 자료로, MIT·Stanford 등 주요 대학 강의와 교재에서도 활용됨\n---\n\n## 트랜스포머 개요\n- 트랜스포머는 어텐션 기반 신경 기계 번역 모델로, 기존 Google Neural Machine Translation보다 특정 작업에서 더 높은 성능을 보임\n주요 장점은 병렬화가 용이하다는 점이며, Google Cloud TPU의 참조 모델로 권장됨\n- 모델은 인코더와 디코더 스택으로 구성되며, 각 인코더는 동일한 구조를 가지되 가중치를 공유하지 않음\n- 인코더 입력은 셀프 어텐션 층을 거쳐 문맥 정보를 통합하고, 이후 피드포워드 신경망을 통과함\n- 디코더는 인코더 구조에 더해 인코더-디코더 어텐션 층을 포함하여 입력 문장의 관련 부분에 집중함\n텐서 흐름과 임베딩\n- 입력 문장은 워드 임베딩 알고리듬을 통해 각 단어를 512차원 벡터로 변환\n- 인코더의 각 층은 동일한 크기의 벡터 리스트를 입력받아 처리하며, 문장 길이는 하이퍼파라미터로 설정\n- \n피드포워드 층은 각 위치의 벡터를 독립적으로 처리하므로 병렬 연산이 가능\n셀프 어텐션의 개념\n- 셀프 어텐션은 각 단어가 문장 내 다른 단어를 참조해 더 나은 표현을 학습하도록 함\n예: “The animal didn’t cross the street because it was too tired”에서 “it”은 “animal”과 연관됨\n- RNN의 은닉 상태 유지와 달리, 트랜스포머는 셀프 어텐션으로 문맥 정보를 통합\n셀프 어텐션 계산 단계\n- 각 단어 임베딩으로부터 Query, Key, Value 벡터를 생성 (차원 64)\n- Query와 Key의 내적(dot product) 으로 각 단어 간의 관련 점수를 계산\n- 점수를 √64로 나눈 뒤 소프트맥스를 적용해 확률 분포로 정규화\n- 각 Value 벡터에 소프트맥스 점수를 곱하고 합산해 최종 어텐션 출력을 생성\n- 실제 구현에서는 이 과정을 행렬 연산으로 수행해 효율성을 높임\n멀티헤드 어텐션\n- \n여러 개의 어텐션 헤드(기본 8개) 를 사용해 다양한 표현 공간을 학습\n각 헤드는 독립적인 Q/K/V 가중치 행렬을 가짐\n- 여러 헤드의 출력을 연결(concat) 후 추가 가중치 행렬 WO로 결합\n- 이를 통해 모델은 문맥의 다양한 측면을 동시에 포착\n포지셔널 인코딩\n- 트랜스포머는 순서를 직접 처리하지 않으므로, 각 단어 임베딩에 위치 벡터를 더함\n- 위치 벡터는 사인(sin) 과 코사인(cos) 함수를 이용해 생성되며, 단어 간 상대적 거리 정보를 제공\n- 이 방식은 훈련 데이터보다 긴 문장에도 확장 가능\n- 2020년 업데이트에서는 두 신호를 교차(interleave) 하는 방식도 소개됨\n잔차 연결과 정규화\n- 각 서브층(셀프 어텐션, 피드포워드)에는 잔차 연결(residual connection) 과 레이어 정규화(layer normalization) 가 적용\n- 이러한 구조는 기울기 소실 방지와 학습 안정성 향상에 기여\n디코더 구조\n- 디코더는 인코더 출력으로부터 Key/Value 벡터를 받아 인코더-디코더 어텐션을 수행\n- \n마스킹(masking) 을 통해 미래 단어를 참조하지 않도록 제한\n- 마지막에는 Linear 층과 Softmax 층을 통해 단어 확률 분포를 생성\n예: 어휘 크기가 10,000일 경우, 각 출력 벡터는 10,000차원 확률 분포로 변환\n학습과 손실 함수\n- 학습 시 모델 출력 확률 분포를 정답 분포(원-핫 인코딩) 와 비교\n- \n크로스 엔트로피(cross-entropy) 또는 KL 발산을 사용해 오차를 계산하고 역전파로 가중치를 조정\n- \n빔 서치(beam search) 를 통해 여러 후보 번역을 유지하며 정확도를 높임\n후속 연구 및 참고 자료\n- 관련 논문: Attention Is All You Need, Training Tips for the Transformer Model, Self-Attention with Relative Position Representations 등\n- 구현 자료: Tensor2Tensor 패키지, Harvard NLP PyTorch 가이드, Colab Notebook\n- 트랜스포머는 이후 LLM-book.com의 확장판(Chapter 3)으로 발전했으며, Multi-Query Attention과 RoPE 위치 임베딩 등 최신 모델을 다룸\n결론\n- 트랜스포머는 병렬화 가능한 어텐션 기반 구조로, 현대 딥러닝의 핵심 모델로 자리잡음\n- 본 글은 트랜스포머의 구조·수학·직관을 시각적으로 설명한 대표 자료로, Stanford·MIT·Harvard 등 주요 대학 강의에서 참고 자료로 사용됨",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25272",
    "category": "AI"
  },
  {
    "title": "‘AI 뉴스 한 달만 놓쳐도 뒤처진다’: 구글 공동창업자 세르게이 브린의 고백",
    "content": "구글 공동창업자 세르게이 브린이 스탠퍼드 공과대학 100주년 행사에서 한 발언\n\n---\n\n## 주요 내용\n- 브린은 AI 경쟁의 속도를 강조하며, \"AI 뉴스를 한 달만 놓쳐도 완전히 뒤처진다\" 고 표현했습니다.\n- 구글이 AI에 과소투자하고 기회를 놓쳤다고 솔직히 인정: 8년 전 구글이 발표한 Transformer 논문의 잠재력을 제대로 파악하지 못하고, 컴퓨팅 자원 확대를 소홀히 했습니다. \"어떤 면에서 우리는 확실히 실수했다\"고 고백.\n- 가장 큰 이유는 두려움과 과도한 신중함: 챗봇이 \"멍청한 소리\" 할까 봐 불완전한 AI를 공개하는 데 주저했습니다. 이로 인해 혁신 속도가 느려졌습니다.\n- 반면 OpenAI는 과감하게 추진: 브린은 이를 \"정말 영리한 통찰\"이라고 칭찬. 특히 구글 핵심 연구자(예: 일리아 수츠케버)가 OpenAI로 옮겨 성공을 이끌었다는 점이 뼈아팠습니다.\n- 이로 인해 구글의 최근 AI 검색 변화가 갑작스럽고 일관성 없어 보이는 이유: 오랜 주저 후 이제 서둘러 따라잡는 중(부작용으로 검색 변동성 발생).\n- 구글의 강점은 여전: 딥러닝 알고리즘, 신경망 R&D, 데이터센터, 반도체 등 오랜 AI 연구 자산.\n- AI 미래에 대한 불확실성: 브린 본인도 \"지능의 한계가 있을까? AI가 인간을 초월할 수 있을까? 우리는 그냥 모른다\"고 솔직히 말함.\n- 재미있는 사실: 브린은 차에서 Gemini Live를 자주 사용하며, 현재 공개 버전은 \"고대 모델\"이고 몇 주 내 훨씬 나은 버전이 나온다고 함.\n전체 메시지: 대기업의 규모 역설 – 자원이 많아 잃을 게 크다고 느껴 과감한 실험을 주저하다 기회를 놓침. OpenAI처럼 잃을 게 없는 스타트업이 위험을 감수하며 산업을 바꿨습니다. 이제 구글은 따라잡기 모드지만, AI 속도가 너무 빨라 뒤처질 위험이 큽니다.\n출처: 스탠퍼드 행사 영상 및 관련 기사.",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25271",
    "category": "AI"
  },
  {
    "title": "왜 AI 해자는 여전히 중요한가 (그리고 어떻게 변화했나) [유튜브]",
    "content": "- \nAI 시대의 소프트웨어 시장은 단순한 IT 지출이 아니라 노동 대체를 중심으로 확장되고 있으며, 소프트웨어가 실제 업무를 수행하는 단계로 진입\n- \n해자(Moats) 의 본질은 여전히 워크플로우 소유, 시스템 통합, 네트워크 효과 등 기존 소프트웨어 기업의 구조적 강점에 있음\n- \nAI 도입 장벽이 낮아지며 경쟁자 수가 급증, 초기 단계에서는 차별화가 어렵지만 대규모 확장 시 데이터 네트워크 효과가 나타남\n- \n플랫폼 기업(OpenAI 등) 은 광범위한 응용 생태계를 구축하며, 직접 경쟁보다 플랫폼 세금 구조와 수직 통합 위험이 주요 변수로 작용\n- \nAI 해자는 여전히 유효하나, 방어력의 원천이 모델 자체보다 고객 맥락과 통합 깊이로 이동하고 있음\n---\n\n## AI 시대의 해자 개념 변화\n- AI는 차별화 도구로서 강력하지만, 지속적 방어력의 원천은 아님\n방어력은 고객 워크플로우 통합, 시스템 오브 레코드 확보, 네트워크 효과 등에서 발생\n- 소프트웨어가 직접 노동을 수행함으로써 시장 기회가 IT 예산에서 노동시장으로 이동\n- AI로 인해 소프트웨어 생산 장벽이 낮아져 공급이 폭증, 경쟁이 심화되는 구조\n규모와 데이터 네트워크 효과\n- \n데이터 네트워크 효과는 대규모에서만 의미 있게 작동\n예: 사기 방지 시스템은 수십억 명의 데이터를 학습해야 우위 확보 가능\n- 초기(0→1) 단계에서는 차별화가 어렵지만, 대규모 확장(1→N) 시 방어력 형성\n- \nAI의 양날의 검: 누구나 쉽게 제품을 만들 수 있으나, 규모 확보가 방어력의 핵심 조건\n가격 모델과 기업 방어력\n- 기존 좌석당(per-seat) 과금 모델은 AI 자동화로 인해 약화\n예: Adobe, Zendesk 등은 좌석 수 감소로 매출 압박\n- 그러나 성과 기반 과금(per-outcome) 으로 전환 시 수익성 회복 가능\n- \n소프트웨어 자체 제작 가능성이 높아졌지만, 여전히 비교우위와 복잡성 때문에 상용 제품 선호 지속\nGoldilocks Zone과 Greenfield 시장\n- \nGoldilocks Zone: 교체 비용이 높고 중요도는 낮은 영역(예: 급여·청소 서비스)\n경쟁이 많아도 고객 전환이 거의 없음\n- \nGreenfield Zone: 신규 기업이 진입 가능한 미개척 시장\n예: 새로운 병원 시스템, 법률 AI 등\n- 단, 창업자의 인내와 신규 시장 창출 속도가 성공의 핵심\n창업자 특성과 산업 맥락\n- 최신 AI 창업자는 산업 경험보다 기술 숙련도가 높음\n산업 맥락(Context)을 보완하기 위해 전문 인력 채용 필수\n- 예: 법률 AI 스타트업은 변호사를 내부 고용해 모델 성능과 실제 업무 연결\n- \n기술 적용 맥락이 방어력의 핵심으로 부상\n브랜드, 규모, 모멘텀의 역할\n- \n브랜드 인지도와 규모의 경제는 여전히 강력한 해자\n예: Cheerios, Amazon 등은 규모 효과와 브랜드 결합으로 시장 지배\n- \n모멘텀(성장 속도) 이 빠를수록 규모 기반 해자 형성 가능성 증가\n- 초기 경쟁이 치열한 시장에서는 속도와 자본 집중이 승패를 좌우\n플랫폼 리스크와 생태계 전략\n- \n플랫폼 소유자 경쟁 여부가 스타트업 생존의 핵심 변수\n예: Microsoft가 Windows를 통해 Excel로 시장 장악\n- \n플랫폼 과세(taxation) 위험 존재 — 수수료율이 임의로 변동 가능\n- 현재는 다수의 모델 기업(OpenAI, Anthropic, Gemini 등) 이 존재해 독점 리스크 완화\n기능(Feature) vs 제품(Product) vs 회사(Company)\n- \n기능은 기존 제품의 일부 개선, 제품은 독립적 시스템, 회사는 지속 가능한 수익 구조 보유\n- AI 시대에는 기능 단위의 제품도 높은 수익 창출 가능\n예: 치과 리셉션 자동화, 다국어 음성 상담 등\n- 기능으로 시작해 제품·회사로 확장(backfill) 하는 전략이 여전히 유효\n플랫폼과 스타트업의 공존\n- \n플랫폼 기업(OpenAI 등) 은 광범위한 응용 생태계 구축에 집중\n직접 산업별 제품을 만들기보다 백엔드 인프라 제공자로 포지셔닝\n- \n수직 통합 위험은 존재하지만, 산업별 세부 영역(예: 치과, 법률) 은 여전히 스타트업 기회\n- \n플랫폼 과세 구조는 장기적으로 스타트업 수익성에 영향\n시장 통합과 경쟁 구도\n- 동일 시장 내 20개 기업이 존재하면 대부분 도태 후 상위 2~3개로 통합\n- \n규모 확보 실패 시 가격 경쟁으로 수익성 붕괴\n- \n전문화(Specialization) 를 통해 세분화된 시장에서 생존 가능\nAI 시대의 고용과 생산성\n- AI는 일자리 제거보다 노동 단가 하락과 생산성 확장을 초래\n“**1달러**에 사람을 고용할 수 없지만, **1달러**에 소프트웨어를 고용할 수 있다”\n- \n노동 대체가 아닌 노동 확장의 형태로 시장 확대\n- \nAI 도입 비용 하락으로 인해 이전에는 불가능했던 서비스(예: 개인 금융 상담, 고객 지원 등)가 가능\n결론: 해자는 여전히 존재하되, 그 형태가 변함\n- \nAI 해자는 사라지지 않았으며, 적용 맥락·고객 통합·규모가 새로운 방어력의 핵심\n- \n모델 자체의 우위보다 응용·운영·고객 내재화 능력이 경쟁력 결정\n- \nAI는 노동을 대체하는 소프트웨어 혁명으로, 기존 해자 구조를 재편하면서도 그 중요성을 유지",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25268",
    "category": "AI"
  },
  {
    "title": "당신의 첫번째 ChatGPT 앱을 만드는 방법",
    "content": "- OpenAI가 발표한 ChatGPT Apps는 개발자가 자신의 앱을 ChatGPT 대화 안에 직접 임베드할 수 있게 하며, **8억** 명이 넘는 주간 활성 사용자를 새로운 배포 채널로 활용할 수 있음\n- ChatGPT는 기존 텍스트 응답을 넘어 UI 컴포넌트를 직접 렌더링할 수 있고, 이를 통해 예약·쇼핑·데이터 조회 같은 실제 작업을 대화 흐름 안에서 처리 가능함\n- 이 구조의 핵심은 Model Context Protocol(MCP) 로, LLM이 외부 도구와 UI 리소스를 선택적으로 호출하는 방식으로 동작함\n- ChatGPT 앱은 백엔드(MCP 서버)와 프론트엔드(UI 컴포넌트)를 명확히 분리한 구조를 가지며, React 기반 미니 앱을 안전한 샌드박스에서 실행\n- MCP와 ChatGPT Apps 패턴은 향후 Claude 등 다른 에이전트로도 확장될 예정이며, 대화형 AI와 기존 SaaS를 연결하는 새로운 인터페이스 계층으로 자리잡고 있음\n---\n\n## ChatGPT Apps 개요\n- \nChatGPT Apps는 개발자와 제품팀이 ChatGPT 대화 안에 앱을 직접 삽입할 수 있게 하는 기능\n- 사용자는 대화 중 호텔 예약, 부동산 검색, 온라인 쇼핑 등의 작업을 ChatGPT 안에서 바로 수행 가능\n- 주간 활성 사용자 **8억** 명 이상을 보유한 플랫폼으로, 차세대 배포 채널로서의 가능성이 있음\nChatGPT Apps의 작동 원리\n- 모든 LLM(ChatGPT, Claude, Gemini)은 웹 검색, 파일 읽기, 코드 실행, 아티팩트 생성 등 제한된 도구만 사용 가능\n- ChatGPT Apps는 개발자가 MCP 도구를 통해 새로운 도구를 LLM에 노출시키고, 사용자가 쉽게 설치·사용할 수 있게 함\n- 앱 없이 단기 숙박을 예약하려 하면 사진과 규정 정보만 반환되고 실제 예약은 불가\n- 앱이 설치되면 요청에 맞는 Airbnb 숙소 목록을 받아 바로 예약 가능\n앱의 세 가지 구성 요소\n- \nMCP 서버 (백엔드)\nChatGPT가 통신하는 서버로, Python 또는 Node.js로 작성\n- API처럼 작동하며, ChatGPT가 호출할 수 있는 \"도구\"(함수)를 정의\n예: search_restaurants, book_ticket\n- \n컴포넌트 (프론트엔드)\n사용자가 보는 인터랙티브 UI\n- 일반적으로 React로 빌드되며 ChatGPT 내부의 보안 샌드박스에서 실행\n- ChatGPT 대화 안에 사는 미니 웹앱 개념\n- \nChatGPT (호스트)\n앱이 ChatGPT 내 임베디드 뷰에 표시됨\n- ChatGPT는 사용자의 대화 내용과 활성화된 앱을 기반으로 언제 앱을 호출할지 결정\n작동 흐름\n- 사용자가 단기 숙박 도움을 요청하면 ChatGPT는 자동으로 여러 단계 수행:\n1. 앱이 유용할지 판단\n2. 앱에 연락해 사용 가능한 도구 확인 (예: Book Listing, Browse Listing)\n3. Browse Listing 도구를 호출해 상위 5개 숙소 반환\n4. 사용자에게 상위 5개 숙소 응답\n- 이 워크플로우가 MCP(Model Context Protocol) 의 기반\n- AI 에이전트(ChatGPT)가 목표 달성을 돕는 도구에 접근권을 받고, 사용자 요청 시 이 도구들을 선택적으로 사용\n- 도구는 두 숫자를 더하는 계산기처럼 단순하거나 이미지 생성 같은 복잡한 백엔드 프로세스일 수 있음\n디스플레이 모드\n- ChatGPT Apps는 세 가지 UI 표시 모드를 지원함\n- \n인라인 모드\n적합 용도: 목록, 카드, 소형 시각화\n- 모든 앱의 기본 모드이며, 필요시 다른 모드로 전환\n- 예: 제품 캐러셀, 레스토랑 목록\n- \n전체화면 모드\n적합 용도: 지도, 복잡한 폼, 데이터 집약적 대시보드\n- 컴포넌트가 전체 ChatGPT 창을 차지\n- 더 많은 공간이 필요하거나 복잡한 작업에 집중해야 할 때 사용\n- 예: 부동산 인터랙티브 지도, 스프레드시트 에디터\n- \nPiP(Picture-in-Picture) 모드\n적합 용도: 지속 도구, 게임, 실시간 업데이트\n- 컴포넌트가 화면 상단에 작은 창으로 떠 있음\n- 사용자가 채팅을 계속하는 동안 계속 보여야 하는 것에 적합\n- 예: 타이머, 음악 플레이어, 틱택토 게임\n- 디스플레이 모드 간 전환을 계획할 경우 UX 패턴에 더 많은 고려 필요\n제약 사항\n- \n턴당 하나의 컴포넌트\nChatGPT가 컴포넌트를 반환하는 도구를 호출하면 해당 턴이 종료됨\n- 여러 UI 반환 도구를 자동으로 연결할 수 없음\n- 예: \"레스토랑 예약하고 Uber 불러줘\" 요청 시 레스토랑 컴포넌트를 먼저 표시하고, 예약 후 후속 트리거로 Uber 컴포넌트 표시\n- \n컴포넌트 상태의 범위 제한\n각 컴포넌트 인스턴스는 해당 특정 메시지 내에서만 유지되는 자체 상태 보유\n- ChatGPT가 컴포넌트와 새 메시지를 생성하면 기본적으로 빈 상태의 새 인스턴스\n- 백엔드에서 지속 및 조회를 통해 상태 관리 필요\n- \n직접 DOM 접근 불가\n컴포넌트는 보안 샌드박스(iframe) 에서 실행되어 상위 ChatGPT 페이지 접근이나 임의 스크립트 실행 불가\n- 모든 통신은 window.openai API를 통해 수행\n- \n성능 중요\n컴포넌트 상태가 각 요청마다 AI 모델에 전송됨\n- 대용량 페이로드는 응답 속도를 늦추므로 필요한 것만 전송\n실제 활용 사례\n- \n이커머스 & 쇼핑\n가격 필터링, 장바구니 추가, 결제(보통 메인 앱으로 푸시)가 가능한 인터랙티브 제품 카탈로그 구축\n- \n비즈니스 도구\n사용자가 컬럼 간 태스크 드래그, 마감일 지정, 상태 업데이트가 가능한 칸반보드 생성\n- ChatGPT가 프로젝트에 대한 질문에 답하면서 동시에 사용자가 조치를 취할 수 있게 하는 내부 도구에 특히 강력\n- \n부킹 & 예약\n가용성, 메뉴, 리뷰를 보여주는 레스토랑 예약 컴포넌트 구축\n- 사용자가 ChatGPT에 추천을 요청하고 인터페이스를 통해 직접 예약 가능\n- \n데이터 대시보드\n인터랙티브 차트로 판매 분석 표시\n- \"Q4 매출은 어때?\"라고 물으면 특정 지역이나 제품으로 드릴다운할 수 있는 대시보드 제공\n- \n지도 & 위치\n커피숍, 부동산 또는 위치 기반 검색용 마커가 있는 인터랙티브 지도 표시\n- \n전체화면 모드에서 특히 효과적\n첫 번째 앱 빌드 가이드 (레스토랑 검색 앱 예시)\n- \nStep 1: 컴포넌트 빌드 (프론트엔드)\niframe 컴포넌트와 ChatGPT 간 통신을 위해 window.openai.* API와 상호작용하는 React 컴포넌트 생성\n- OpenAI가 API 상호작용을 돕는 글로벌 제공\n- \nStep 2: 도구 정의 (백엔드)\nMCP 서버가 ChatGPT와의 \"계약\" 정의\n- 모델에게 무엇을 할 수 있고 도구 호출 완료 시 어떤 컴포넌트를 표시할지 알려줌\n- \nStep 3: 리소스 등록 (백엔드)\nReact 컴포넌트를 HTML로 번들링하고 MCP 서버에 리소스로 호스팅\n- ChatGPT가 이전 단계의 outputTemplate URI를 가져와 iframe될 코드를 요청\n- \nStep 4: 흐름 테스트\n\n---\n\n## 서버 배포 후\nChatGPT를 열고 Developer Mode 활성화\n- Settings → Connected Apps (또는 \"My Apps\")로 이동\n- \"Connect new app\" 클릭\n- 서버 URL 입력 (예: https://my-mcp-server.com)\n- OAuth 없이 연결\n- ChatGPT가 search_restaurants 도구를 감지하면 \"Find me Italian food in Brooklyn\" 입력으로 테스트\n- ChatGPT가 도구를 호출하고 데이터를 가져와 텍스트 대신 인터랙티브 React 목록 렌더링\n향후 전망\n- ChatGPT가 먼저 시작했지만, 곧 MCP 표준이 MCP apps를 통해 동일한 패턴 지원 예정\n- Claude와 다른 에이전트들도 대화형 경험에 미니 앱을 주입할 수 있게 됨\n- 이러한 시스템 아키텍처와 사용자를 위한 의미 있는 경험 구축 방법 이해가 AI 제품과 일반 SaaS 애플리케이션 간 새로운 통신 방식 개척",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25267",
    "category": "AI"
  },
  {
    "title": "2025년 말에 돌아본 AI에 대한 고찰",
    "content": "- LLM이 단순한 확률적 앵무새에 불과하다는 주장은 2025년 들어 거의 사라졌으며, 프롬프트의 의미와 응답 방향에 대한 내부 표상이 존재함을 대부분 인정하게 됨\n- \nChain of Thought(CoT) 는 모델 표상 내 샘플링과 강화학습을 통한 토큰 순차 학습의 결합으로, LLM 출력 품질을 향상시키는 핵심 기법이 됨\n- \n검증 가능한 보상을 활용한 강화학습이 토큰 수 제한을 넘어선 확장 가능성을 열었으며, 이 분야가 AI의 다음 핵심 발전 방향이 될것으로 예상\n- \nLLM 기반 프로그래밍 지원에 대한 저항이 크게 줄어들었고, 웹 인터페이스 협업 방식과 독립 코딩 에이전트 방식으로 활용 형태가 나뉨\n- \nTransformer 대안 연구와 AGI 가능성이 병행되며, 다양한 아키텍처가 독립적으로 일반지능에 도달할 수 있다는 관점이 부상함\n---\n- 오랫동안 LLM을 의미를 이해하지 못하는 2가지 특징을 가진 확률적 기계(stochastic parrots) 라던 주장이 있었음\n1. 프롬프트의 의미에 대한 정보를 전혀 가지고 있지 않음\n- 2. 자신이 무엇을 말할지에 대한 정보도 전혀 가지고 있지 않음\n- 기능적 성과와 과학적 단서가 지속적으로 누적되며 이러한 관점이 점차 설득력을 잃고, 2025년에 이르러 해당 주장은 거의 사라진 상태로 전환됨\n- Chain of Thought(CoT)는 현재 LLM 성능을 개선하는 핵심 기법으로 자리 잡음\n- CoT의 효과는 관련 정보와 개념을 컨텍스트에 올려 모델 내부 표현 공간에서의 샘플링, 즉 내부 탐색을 가능하게 함\n- \n강화학습과 결합될 경우, 토큰을 하나씩 배치하며 모델 상태를 변화시켜 유용한 답변으로 수렴하는 과정을 학습하게 됨\n- 스케일링의 한계가 토큰 수에 의해 결정된다는 기존 관점은 더 이상 유효하지 않음\n- \n검증 가능한 보상 기반 강화학습(RLVR) 도입으로 스케일링의 범위가 확장됨\n- \n프로그램 속도 개선처럼 명확한 보상 신호가 존재하는 작업에서는 이론적으로 장기간 지속적인 개선 가능성이 있음\n- LLM에 적용되는 강화학습의 개선이 차세대 AI의 핵심 기술이 될 것\n- \nAI 보조 프로그래밍에 대한 개발자들의 저항은 눈에 띄게 감소함\n- LLM이 오류를 내더라도 유용한 코드와 힌트를 제공하는 능력이 크게 향상됨\n- \n투자 대비 효용이 명확해지며 회의적이었던 개발자들까지 활용을 시작함\n- LLM을 웹 인터페이스 기반의 동료처럼 사용하는 방식과 독립적인 코딩 에이전트로 사용하는 방식이 공존함\n- Transformer 이후에도 또 다른 돌파구가 가능하다는 인식이 일부 저명한 AI 과학자들 사이에서 확산됨\n트랜스포머 대안, 명시적 기호 표현(symbolic representation), 세계 모델(world model)을 탐구하는 팀과 회사들이 등장\n- LLM이 이산적 추론 단계를 근사할 수 있는 공간에서 훈련된 미분 가능한 기계라고 생각함\n- 근본적으로 새로운 패러다임 없이도 LLM을 통해 AGI에 도달할 가능성이 있다고 생각\n- \n다양한 아키텍처를 통해 독립적으로 인공 일반 지능(AGI)에 도달할 수 있을 가능성이 있음\n- Chain of Thought가 LLM의 본질을 바꿨다는 주장도 제기됨\n- 과거 LLM을 제한적이라 평가하던 이들이 CoT 이후 입장을 바꾸는 현상이 보임\n- 그들은 CoT 때문에 LLM이 완전히 달라졌다고 말하지만, 이는 거짓임\n- 여전히 동일한 아키텍처와 다음 토큰 목표를 가지고 있으며, CoT는 토큰이 하나씩 생성되는 방식 그대로임\n- 과거 LLM의 한계를 검증하기 위한 ARC 테스트가 이제는 LLM 성능을 입증하는 지표로 전환됨\n- ARC 테스트는 초기와 달리 더 이상 극복 불가능한 과제로 보이지 않음\n- 특정 작업에 최적화된 소형 모델들이 ARC-AGI-1에서 의미 있는 성과를 냄\n- 많은 사람들이 결과를 내지 못할거라 생각했던 아키텍처로 대형 LLM과 광범위한 CoT를 활용하여 ARC-AGI-2에서 인상적인 결과를 달성함\n- 향후 20년간 AI가 직면한 가장 근본적인 도전 과제는 인류 멸종을 피하는 문제가 될 것",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25266",
    "category": "AI"
  },
  {
    "title": "당신이 커리어를 설계하지 않으면, 누군가가 대신 설계할 것이다 (2014)",
    "content": "- 일상과 업무에 몰두한 나머지 자신의 커리어를 성찰하지 못하는 함정을 지적하며, 이를 벗어나기 위한 구체적 절차를 제시\n- 지난 12개월을 검토하고 주요 프로젝트·성과를 목록화한 뒤, 그 안의 흐름과 의미를 분석하도록 권장\n- 제한 없는 사고로 이상적인 커리어 방향을 구상하고, 현실적 제약으로 지나치게 빨리 포기하지 말 것을 강조\n- 목표를 여섯 가지로 정리한 후 단 하나의 핵심 목표만 남기고 집중하며, 이를 방해하는 ‘좋은 일들’을 과감히 거절해야 함\n- 짧은 시간의 성찰이 향후 수천 시간의 삶의 질을 바꿀 수 있음을 강조하며, 스스로 커리어를 설계할 필요성을 환기\n---\n\n## 커리어를 스스로 설계해야 하는 이유\n- 사람들은 종종 삶을 사느라 바빠서 삶을 생각하지 못하는 함정에 빠짐\n커리어에서도 마찬가지로, 일에 몰두한 나머지 커리어 자체를 돌아보지 못하는 경우가 많음\n- 이러한 상태를 피하기 위해 휴일 중 몇 시간을 투자해 커리어를 성찰하는 과정을 제안\n8단계 커리어 성찰 프로세스\n- \n1단계: 지난 12개월 검토\n한 해를 월별로 돌아보며 시간 사용처, 주요 프로젝트, 책임, 성과를 목록화\n- 복잡하게 만들 필요 없이 단순한 기록으로 충분함\n- \n2단계: ‘무슨 일이 일어나고 있는가?’ 질문\n목록을 검토하며 실제로 어떤 일이 진행 중인지, 왜 중요한지, 어떤 추세가 있는지 분석\n- 이러한 추세가 계속될 경우의 결과를 생각\n- \n3단계: ‘무엇이든 할 수 있다면 무엇을 할 것인가?’ 질문\n비판 없이 자유롭게 아이디어를 적어내며, 제약 없는 사고를 유도\n- \n4단계: 3단계 아이디어 확장\n현실적 제약 때문에 지나치게 빨리 포기하지 말고, 진정 원하는 방향을 더 깊이 탐색\n- “비현실적이다”라는 이유로 배제된 길이 실제로는 정당한 커리어 경로일 수 있음\n- \n5단계: 향후 12개월 목표 6가지 작성\n달성하고 싶은 주요 커리어 목표를 우선순위에 따라 정리\n- \n6단계: 하위 5개 목표 삭제\n단 하나의 ‘진정한 북극성’ 목표에 집중\n- 업무의 소용돌이 속에서도 방향을 잃지 않게 함\n- \n7단계: 이번 달 실행 계획 수립\n3~4주 내 달성 가능한 단기 성과(quick wins) 를 설정\n- \n8단계: ‘무엇을 거절할 것인가’ 결정\n핵심 목표 달성을 방해하는 ‘좋은 일들’을 목록화하고, 삭제·연기·위임 방안을 마련\n- Ralph Waldo Emerson의 말을 인용해, “주된 목적에서 벗어나 여기저기 일하는 것이 개인과 국가를 파산시킨다”고 경고\n개인적 경험과 교훈\n- 저자는 이 과정을 직접 실행한 결과, 법학대학원을 그만두고 영국을 떠나 미국으로 이주, 교사이자 작가의 길을 걷게 됨\n이 결정이 인생에서 가장 중요한 커리어 전환점이 되었음을 언급\n- 단 두 시간의 집중적 성찰이 다음 해 8,760시간의 삶의 질을 향상시킬 수 있음을 강조\n- 결론적으로, 자신이 커리어를 설계하지 않으면 누군가가 대신 설계하게 됨을 경고",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25262",
    "category": "스타트업"
  },
  {
    "title": "미국, 모든 해상 풍력 건설 중단…이유는 ‘기밀’로 분류",
    "content": "- 미국 내무부(Department of the Interior) 가 현재 진행 중인 5개 해상 풍력 발전소의 임대 및 건설을 전면 중단한다고 발표\n- 내무부는 국방부(Department of Defense) 의 기밀 분석 보고서를 근거로 “국가 안보 위험”이 있다고 주장\n- 중단 대상에는 Empire Wind, Revolution Wind, Sunrise Wind, Vineyard Wind 1, Coastal Virginia Offshore Wind 등 주요 프로젝트 포함\n- 일부 프로젝트는 이미 완공 직전 단계이며, 관련 주 정부와 기업들은 법적 대응을 검토 중\n- 이번 조치는 법원에서 이미 무효화된 이전 행정명령을 우회하려는 시도로 비판받으며, 미국 재생에너지 정책의 불확실성을 키우는 사안\n---\n\n## 해상 풍력 건설 전면 중단 발표\n- 미국 내무부가 현재 건설 중인 5개 해상 풍력 발전소의 임대 및 허가를 일시 중단한다고 발표\n해당 프로젝트들은 이미 해상 및 육상 설비가 상당 부분 설치된 상태이며, 일부는 완공을 앞둔 단계\n- 내무부는 이번 조치의 근거로 국방부의 기밀 보고서를 제시하며, 구체적 내용은 공개하지 않음\n- 내무부는 보고서가 “국가 안보 위험” 을 지적했다고 밝혔으며, 이는 법적 검토를 피하기 위한 조치로 보인다는 지적이 있음\n트럼프 행정부의 해상 풍력 반대 기조\n- \n트럼프 행정부 2기는 취임 첫날부터 해상 풍력 발전에 적대적 입장을 보이며, 신규 프로젝트 허가를 일시 중단하는 행정명령을 발표\n- 그러나 12월 초 법원이 해당 행정명령을 무효화, 정부가 약속한 재평가 절차를 진행하지 않았다고 판결\n- 그럼에도 불구하고 행정부는 이미 허가를 받은 프로젝트들까지 불규칙적으로 중단시켜 왔음\n중단 대상 5개 프로젝트\n- \nCoastal Virginia Offshore Wind: 버지니아 해안 인근 2.6GW 규모, 육상 설비와 해상 기초 공사 완료\n- \nEmpire Wind: 뉴욕·뉴저지 해안 인근 810MW 규모, 초기 공사 단계\n- \nRevolution Wind: 코네티컷·로드아일랜드 해역 700MW 규모, 공정률 **80%**로 완공 임박\n- \nSunrise Wind: 롱아일랜드 인근 925MW 규모, 전력 육상 연결 설비 공사 중\n- \nVineyard Wind 1: 매사추세츠 남쪽 800MW 규모, 연내 완공 예정\n‘기밀’로 분류된 위험 요인\n- 내무부는 풍력 터빈이 레이더 감지에 간섭할 수 있음을 언급했으나, 이는 이미 알려진 사실\n- \n내무장관 Doug Burgum은 “적대국 기술의 빠른 진화”를 언급했지만, 국방부 분석 내용은 기밀로 분류되어 공개 불가\n- 이로 인해 법적 대응이 어려워질 가능성이 있으며, 실제로 어떤 위험이 존재하는지는 확인 불가\n주 정부와 기업의 반발\n- \n코네티컷주 법무장관 William Tong은 이번 조치를 “** 불법적이고 변덕스러운 공사 중단 명령의 재현**”이라 비판\n그는 “법원이 이전 중단 명령을 이미 차단했으며, 이번 조치는 그 판결을 우회하려는 시도”라고 언급\n- 주 정부는 현재 법적 대응 방안을 검토 중\n- 프로젝트를 추진 중인 기업들은 이미 대부분의 자금을 투입한 상태로, 발전소 가동을 통해 투자 회수를 기대하고 있음\n행정부의 일관성 결여와 정책 불확실성\n- 과거 두 차례의 법적 분쟁에서 정부는 모두 패소, 정책 변경의 실질적 근거를 제시하지 못함\n- 법원 기록에 따르면, 행정부의 결정 과정은 대통령의 개인적 반감 외에 명확한 이유가 없음\n- 이번 기밀 평가가 이전과 실질적으로 다른 점이 있는지는 불명확, 단지 정보 접근이 더 어려워졌다는 점만 확인됨",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25261",
    "category": "스타트업"
  },
  {
    "title": "실내 태닝이 젊은 피부를 유전적으로 훨씬 더 늙게 만든다",
    "content": "- 인공 자외선 노출이 세포 돌연변이를 일으켜 향후 피부암의 씨앗이 되는 것으로 확인됨\n- 연구에 따르면 젊은 실내 태닝 이용자의 피부는 70~80대 일반인보다 더 많은 유전자 변이를 보유\n- 특히 햇빛 노출이 적은 허리 부위 피부에서도 돌연변이가 집중적으로 발견됨\n- 세계보건기구는 태닝 베드를 1급 발암물질로 분류하지만, 미국에서는 여전히 합법적 사용이 지속\n- 연구진은 돌연변이는 되돌릴 수 없으므로 인공 자외선 노출을 피하는 것이 필수적이라 강조\n---\n\n## 연구 개요\n- UC San Francisco와 Northwestern University가 공동으로 수행한 연구로, 인공 자외선원이 세포 수준에서 유전적 노화와 암 유발 돌연변이를 촉진함을 확인\n연구 결과는 Science Advances에 12월 12일자로 게재\n- 태닝 베드 이용자의 피부는 멜라노마로 이어질 수 있는 변이 세포로 가득 차 있었음\n- 연구진은 “30~40대 태닝 이용자의 피부가 70~80대 일반인보다 더 많은 돌연변이를 보유했다”고 설명\n주요 발견\n- \n32,000명 이상의 피부과 환자 기록을 분석해 태닝 이용 여부, 일광화상 이력, 가족력 등을 조사\n- \n26명의 기증자 피부 샘플에서 총 182개의 세포를 유전체 분석\n- 젊은 태닝 이용자들의 피부에서 두 배 연령대의 사람보다 더 많은 돌연변이가 발견됨\n특히 햇빛 노출이 적은 하부 등 부위에서 변이가 집중적으로 나타남\n- 이러한 변이는 멜라노마를 포함한 피부암의 주요 원인으로 작용\n자외선과 피부암의 연관성\n- 자외선(UV)은 자연광뿐 아니라 태닝 베드의 인공광에서도 발생\n- 미국암학회에 따르면 피부암은 미국에서 가장 흔한 암이며, 멜라노마는 전체의 약 **1%**지만 사망의 대부분을 차지\n- 매년 약 11,000명의 미국인이 멜라노마로 사망하며, 주된 원인은 자외선 노출\n- 최근 태닝 베드 사용 증가와 함께 멜라노마 발생률도 상승, 특히 젊은 여성층에서 두드러짐\n규제 및 공중보건 맥락\n- 다수의 국가에서는 태닝 베드를 사실상 금지하고 있으며, WHO는 이를 담배 연기·석면과 동일한 1급 발암물질로 분류\n- 그러나 미국에서는 여전히 합법적이고 인기 있는 미용 수단으로 남아 있음\n연구진의 권고\n- “돌연변이는 한 번 발생하면 되돌릴 수 없기 때문에, 처음부터 누적을 최소화하는 것이 중요”\n- 가장 간단한 방법으로 인공 자외선 노출 회피를 제시\n- 연구는 국립암연구소, 국방부 멜라노마 연구 프로그램, Melanoma Research Alliance 등의 지원을 받음\n- 이해상충 보고는 없음",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25258",
    "category": "튜토리얼"
  },
  {
    "title": "디즈니 이매지니어링, 차세대 로봇 캐릭터 ‘올라프’ 공개",
    "content": "- \n디즈니랜드 파리에서 공개된 로봇 올라프는 영화 겨울왕국 속 캐릭터를 실제로 구현한 차세대 로보틱 캐릭터임\n- 움직임, 표정, 눈처럼 빛나는 외형까지 세밀하게 재현해 가장 생동감 있는 캐릭터 중 하나로 평가됨\n- 제작 과정에서 강화학습 기반 인공지능을 활용해 자연스러운 동작과 감정 표현을 구현\n- 입, 눈, 코, 팔을 자유롭게 조작하며 대화와 상호작용이 가능한 로봇으로 설계됨\n- 디즈니는 이를 통해 스토리텔링과 기술 융합의 새로운 시대를 열고, 전 세계 파크에 더 많은 감정형 캐릭터를 도입할 계획임\n---\n\n## 디즈니랜드 파리에서의 공개\n- \nWalt Disney Imagineering의 브루스 본과 디즈니랜드 파리의 나타샤 라팔스키가 로봇 올라프를 공개\n올라프는 겨울왕국의 눈사람 캐릭터를 실물 크기로 구현한 로봇\n- 이번 공개는 기술·스토리텔링·협업이 결합된 새로운 혁신 단계로 소개됨\n- 영화 속 올라프의 움직임과 표정, 눈처럼 빛나는 질감을 재현\n눈의 반짝임은 무지갯빛 섬유(iridescent fibers) 로 구현\n- 영화 애니메이터들과 협업해 창작자의 의도와 감정 표현을 그대로 반영\n기술적 혁신과 인공지능 활용\n- 디즈니는 모든 프로젝트에서 스토리 중심의 기술 개발을 우선시함\n목표는 캐릭터에 생명을 불어넣는 스토리텔링 기술 구축\n- 기존의 BDX 드로이드(스타워즈 자유주행 로봇)보다 높은 난이도의 과제였음\n올라프는 물리적 제약이 없는 애니메이션형 캐릭터로, 움직임 구현이 복잡\n- 이를 위해 강화학습(Deep Reinforcement Learning) 을 적용\n인간이 수년 걸리는 걷기와 섬세한 동작을 단기간에 학습 가능\n- 올라프의 외형은 기존 로봇의 단단한 외피와 달리 눈처럼 유연한 움직임을 가짐\n입, 눈, 당근 코, 팔을 완전하게 조작 가능\n- \n음성 대화와 상호작용 기능을 갖춘 독보적 캐릭터\n디즈니의 로보틱 캐릭터 발전\n- \nBDX 드로이드, H.E.R.B.I.E. 자율균형 로봇, 그리고 올라프는 디즈니의 로보틱 퍼포먼스 진화 단계를 상징\n점점 더 감정적이고 표현력 있는 캐릭터 개발로 확장 중\n- 새로운 캐릭터를 빠른 속도로 제작·도입할 수 있는 역량 확보\n전 세계 파크에서 감정적 몰입과 놀라움을 주는 경험 확대 목표\n올라프를 만날 수 있는 장소\n- \n디즈니랜드 파리의 World of Frozen 내 Arendelle Bay Show에서 첫 등장 예정\n- \n홍콩 디즈니랜드 리조트의 World of Frozen에서도 한정 기간 특별 출연 예정\n- 올라프의 개발 과정은 We Call It Imagineering 최신 에피소드에서 공개됨",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25256",
    "category": "AI"
  },
  {
    "title": "샌프란시스코 정전 사태 중 교통 혼잡을 일으킨 후 Waymo 서비스 중단",
    "content": "- 샌프란시스코 전역에서 발생한 대규모 정전 중 Waymo의 자율주행 차량들이 교통 혼잡을 유발하며 멈춰섬\n- Waymo는 승차 서비스 전면 중단을 발표하고, 승객 안전 확보와 긴급 구조 인력의 접근 보장을 이유로 제시\n- 정전으로 도시의 약 3분의 1이 전력 공급을 잃었으며, 신호등이 작동하지 않아 차량들이 교차로에서 정지\n- 시민들은 도로 곳곳에 멈춰 선 Waymo 차량 영상을 SNS에 다수 게시, 일부 구간에서는 여러 대가 일렬로 정체\n- 이번 사태는 도시 인프라 장애 시 자율주행 시스템의 대응 한계를 드러낸 사례로 주목\n---\n\n## Waymo 서비스 중단과 정전 상황\n- Waymo는 12월 20일 토요일, 샌프란시스코 전역의 정전 사태로 인해 자율주행 차량 서비스 중단을 결정\n회사 대변인 Suzanne Philion은 “광범위한 정전으로 인해 서비스를 일시 중단했다”며, 승객 안전과 긴급 인력의 원활한 이동 보장을 강조\n- 정전은 Pacific Gas & Electric(PG&E) 의 순환 단전으로 인해 발생, 약 **12만** 5천 가구와 사업체가 영향을 받음\nPG&E는 오후 8시 30분경 “오늘 밤 늦게 전력 복구가 시작될 것”이라고 발표\n교통 혼잡과 시민 반응\n- \n신호등이 꺼진 교차로에서 Waymo 차량들이 멈춰 서며 교통 정체가 발생\n시민들은 트위터(X) 에 차량들이 정지한 영상과 사진을 다수 게시\n- 일부 영상에서는 2~6대의 Waymo 차량이 일렬로 정지하거나, 빨간 경고등을 깜박이며 비를 맞고 멈춰 있는 모습이 포착\n- “정전으로 Waymo들이 도로를 막았다”, “훈련되지 않은 상황 같다” 등 비판적 반응이 이어짐\n한 이용자는 “North Beach 지역에서 Waymo 차량들이 대규모 정체를 일으켰다”고 게시\n온라인 확산과 여론\n- 정전 당일 오후, Waymo 차량이 멈춘 영상이 수백 건 이상 게시되며 SNS에서 급속히 확산\n한 게시물은 “샌프란시스코 전역의 Waymo 차량이 멈춰 도시 전체가 교통 마비 상태”라고 언급\n- 일부 이용자는 원격 제어 시스템이 통신 불안정으로 작동하지 않았을 가능성을 언급했으나, Waymo 측의 관련 언급은 없음\n서비스 재개 공지\n- 다음 날 오후, Waymo는 차량들이 다시 도로에 복귀했다고 발표\n구체적인 복구 과정이나 기술적 원인에 대한 추가 설명은 제공되지 않음\n\n---\n\n## 사건의 의미\n- 이번 정전 사태는 자율주행차가 도시 인프라 장애 상황에서 어떻게 대응하는지를 보여준 사례\n- \n신호 체계나 통신망 의존성이 높은 시스템의 취약성이 드러났으며, 비상 상황 대응 체계의 필요성이 부각됨",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25255",
    "category": "개발"
  },
  {
    "title": "a16z에서 발행한 2025 B2C AI 앱 결산 [번역글]",
    "content": "1. OpenAI (ChatGPT/GPT-4o)\n- 사용자 **10억** 돌파 1위지만 활성률 **20%** 미만, 신규성 소진 후 포기 심각 (원문: \"usage drop-off after novelty wears off\").\n- 멀티모달(텍스트·이미지·음성·비디오) 강점으로 일상 창작·대화 도구화, 생산성 **25%**↑. 그러나 환각(정확도 **85%**)으로 신뢰 한계.\n- 월 **$20** 가격 인하로 접근성↑, 에이전트(자율 작업) 베타 단계. 소비자용 '올라운드'지만 지속 사용이 과제.\n2. Google Gemini\n- Android·Search 통합 엣지 AI 선도, 온디바이스 처리로 프라이버시·속도 우위. Apple Intelligence 경쟁.\n- 멀티모달 비디오·음성 동률 1위, 검색 정확도 **90%**. Google 생태계 연동으로 Retention **30%**↑.\n- 무료+창의성 약점에도 실생활 유용성 높아 '잠재 2위' 평가. 과대광고 이미지 털어내는 중.\n3. Anthropic Claude (3.5 Sonnet)\n- 코드 생성·논리 최강, Cursor 통합으로 개발자·전문직 **40%** 선호. 안전성 강조로 기업 채택↑ (원문: \"Claude 3.5 멀티모달 통합\").\n- 긴 컨텍스트(200K 토큰) 문서 분석 강점, 환각 최소. 소비자용 '정확하지만 지루' 이미지.\n- 토큰당 저가로 오픈소스 경쟁, 빅테크 독점 약화 기여. Retention **35%** 수준.\n4. Perplexity AI\n- 실시간 검색·요약 특화, 'AI 검색 엔진'으로 정보 탐색 생산성 **40%**↑. 소비자 질문 해결에 최적화 (원문: 멀티모달 통합 트렌드 내 언급).\n- 정확도 **92%**로 환각 적고, 소스 인용으로 신뢰↑. 모바일 앱 Retention 높음.\n- 무료 기본+Pro(**$20**) 모델, Grok과 검색 대결 중. '지식 노동자 필수' 포지션 굳힘.\n5. xAI Grok\n- 유머·대화 스타일 독특, X(트위터) 통합으로 소셜·트렌드 분석 강점. 실시간 데이터 접근 우위 (원문: 에이전트 AI 부상 맥락).\n- 멀티모달 초기지만 이미지 생성·밈 이해 탁월, 젊은 층 Retention **28%**.\n- 무료+프리미엄, '재미있지만 실무 약함' 평가. Musk 팬덤 기반 성장 중.\n6. Meta (Llama 3 및 오픈소스 생태)\n- 오픈소스 Llama 3 무료 배포로 개발자·스타트업 채택 폭발, 빅테크 독점 약화 주역 (원문: \"오픈소스 모델 주도\").\n- 커스텀 파인튜닝 쉬워 도메인 특화(예: 마케팅·e커머스) AI 급증. 비용 1/10 수준.\n- WhatsApp·Instagram 통합으로 소비자 엣지 AI 확산, Retention **25%**. 그러나 상용 모델만큼 세련되지 않음.\n전체 트렌드와 전망\n- 모델 성능 격차 좁혀짐(85~**95%**), 성공 열쇠는 UX 통합·맥락 이해·Retention. 에이전트·개인화·멀티모달 표준화 중.\n- 2026년 오픈소스(Llama) vs 클로즈드(GPT) 경쟁 심화, 'AI 컴패니언' 보편화 예상.",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25252",
    "category": "AI"
  },
  {
    "title": "인터넷의 끝",
    "content": "축하합니다!\n드디어 인터넷의 끝까지 도달하셨군요!\n더 이상 볼 것도, 방문할 링크도 없습니다.\n모든 것을 해내셨습니다.\n이곳은 인터넷의 가장 먼 끝에 위치한 마지막 서버의 마지막 페이지입니다.\n이제 컴퓨터를 끄고 남은 인생을 유용하게 보내시기 바랍니다. *\n\n---\n\n## 추천 사항\n- 책을 읽으세요\n- 사회 봉사를 하세요\n- 아마 온라인에서만 만났을 이웃들과 직접 교류하세요\n- 나무를 심으세요\n- 같은 집에 사는 다른 사람들(가족)에게 자신을 소개하세요.\n* 나갈 때 불 끄는 거 잊지 마세요.\n시간을 절약하기 위해, 지금부터 인터넷을 로컬 드라이브로 다운로드할 것입니다.",
    "source": "GeekNews",
    "sourceUrl": "https://news.hada.io/topic?id=25251",
    "category": "개발"
  }
]