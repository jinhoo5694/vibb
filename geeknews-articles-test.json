[
  {
    "title": "Z80-μLM, 40KB 안에 담긴 ‘대화형 AI’",
    "content": "# Z80-μLM: A Retrocomputing Micro Language Model\n [](#z80-μlm-a-retrocomputing-micro-language-model)\n Z80-μLM is a 'conversational AI' that generates short character-by-character sequences, with quantization-aware training (QAT) to run on a Z80 processor with 64kb of ram. The root behind this project was the question: how small can we go while still having personality, and can it be trained or fine-tuned easily? With easy self-hosted distribution? The answer is Yes! And a 40kb . com binary (including inference, weights & a chat-style UI) running on a 4MHz processor from 1976. It won't pass the Turing test, but it might make you smile at the green screen. For insight on how to best train your own model, see [TRAINING. md](/HarryR/z80ai/blob/main/TRAINING. md). ## Examples\n [](#examples)\n Two pre-built examples are included:\n ### [tinychat](/HarryR/z80ai/blob/main/examples/tinychat)\n\n[](#tinychat)\n A conversational chatbot trained on casual Q&A pairs. Responds to greetings, questions about itself, and general banter with terse personality-driven answers. ```\n > hello\n HI\n > are you a robot\n YES\n > do you dream\n MAYBE\n ```\n ### [guess](/HarryR/z80ai/blob/main/examples/guess)\n [](#guess)\n A 20 Questions game where the model knows a secret topic and answers YES/NO/MAYBE to your questions. Guess correctly to WIN. ```\n > is it alive\n YES\n > is it big\n YES\n > does it have a trunk\n YES\n > is it grey\n MAYBE\n > elephant\n WIN\n ```\n Includes tools for generating training data with LLMs (Ollama or Claude API) and balancing class distributions. ## Features\n [](#features)\n - **Trigram hash encoding**: Input text is hashed into 128 buckets - typo-tolerant, word-order invariant\n - **2-bit weight quantization**: Each weight is {-2, -1, 0, +1}, packed 4 per byte\n\n- **16-bit integer inference**: All math uses Z80-native 16-bit signed arithmetic\n - **~40KB . COM file**: Fits in CP/M's Transient Program Area (TPA)\n - **Autoregressive generation**: Outputs text character-by-character\n - **No floating point**: Everything is integer math with fixed-point scaling\n - **Interactive chat mode**: Just run `CHAT` with no arguments\n ## Interaction Style\n [](#interaction-style)\n The model doesn't understand you. But somehow, it *gets* you. Your input is hashed into 128 buckets via trigram encoding - an abstract \"tag cloud\" representation. The model responds to the *shape* of your input, not the exact words:\n ```\n \"hello there\" → [bucket 23: 64, bucket 87: 32, . . . ]\n \"there hello\" → [bucket 23: 64, bucket 87: 32, . . . ] (same! )\n \"helo ther\" → [bucket 23: 32, bucket 87: 32, . . . ] (similar - typo tolerant)\n ```\n\nThis is semantically powerful for short inputs, but there's a limit: longer or order-dependent sentences blur together as concepts compete for the same buckets. \"Open the door and turn on the lights\" will likely be too close to distinguish from \"turn on the door and open the lights. \"\n ### Small Responses, Big Meaning\n [](#small-responses-big-meaning)\n A 1-2 word response can convey surprising nuance:\n - `OK` - acknowledged, neutral\n - `WHY? ` - questioning your premise\n - `R U? ` - casting existential doubt\n - `MAYBE` - genuine uncertainty\n - `AM I? ` - reflecting the question back\n This isn't necessarily a limitation - it's a different mode of interaction. The terse responses force you to infer meaning from context or ask probing direct yes/no questions to see if it understands or not (e. g.\n\n'are you a bot', 'are you human', 'am i human' displays logically consistent memorized answers)\n ### What It's Good At\n [](#what-its-good-at)\n - Short, varied inputs with consistent categorized outputs\n - Fuzzy matching (typos, rephrasing, word order)\n - Personality through vocabulary choice\n - Running on constrained 8-bit hardware\n ### What It's Not\n [](#what-its-not)\n - A chatbot that generates novel sentences\n - Something that tracks multi-turn context deeply\n - A parser that understands grammar\n - Anything approaching general intelligence\n It's small, but functional. And sometimes that's exactly what you need. ## Architecture\n [](#architecture)\n - **Input**: 128 query trigram buckets + 128 context buckets\n - **Hidden layers**: Configurable depth/width, e. g. , 256 → 192 → 128\n - **Output**: One neuron per character in charset\n - **Activation**: ReLU between hidden layers\n\n### Quantization Constraints\n [](#quantization-constraints)\n The Z80 is an 8-bit CPU, but we use its 16-bit register pairs (HL, DE, BC) for activations and accumulators. Weights are packed 4-per-byte (2-bit each) and unpacked into 8-bit signed values for the multiply-accumulate. The 16-bit accumulator gives us numerical stability (summing 256 inputs without overflow), but the model's expressiveness is still bottlenecked by the 2-bit weights, and naive training may overflow or act 'weirdly' without QAT. ### Z80 Inner Loops\n [](#z80-inner-loops)\n The core of inference is a tight multiply-accumulate loop. Weights are packed 4-per-byte:\n ```\n ; Unpack 2-bit weight from packed byte\n ld a, (PACKED) ; Get packed weights\n and 03h ; Mask bottom 2 bits\n sub 2 ; Map 0,1,2,3 → -2,-1",
    "source": "github.com",
    "sourceUrl": "https://github.com/HarryR/z80ai",
    "category": "AI"
  },
  {
    "title": "CEO는 너무 비싸다. 자동화할 수는 없을까?",
    "content": "- \n [Business\n ](https://www. newstatesman. com/business)\n **\n - \n [Companies\n ](https://www. newstatesman. com/business/companies)\n **\n # CEOs are hugely expensive. Why not automate them? If a single role is as expensive as thousands of workers, it is surely the prime candidate for robot-induced redundancy. By\n [Will Dunn](https://www. newstatesman. com/author/will-dunn) \n *\n (Photo By Chung Sung-Jun/Getty images)\n - \n [\n ](mailto:type%20email%20address%20here? subject=I%20wanted%20to%20share%20this%20post%20with%20you%20from%20New%20Statesman&body=CEOs%20are%20hugely%20expensive. %20Why%20not%20automate%20them? %20-%20https://www. newstatesman. com/business/companies/2023/05/ceos-salaries-expensive-automate-robots)\n - \n [\n ](http://www. linkedin. com/shareArticle? mini=true&url=https://www. newstatesman. com/business/companies/2023/05/ceos-salaries-expensive-automate-robots)\n - \n [\n\n](https://twitter. com/intent/tweet? url=https://www. newstatesman. com/business/companies/2023/05/ceos-salaries-expensive-automate-robots)\n - \n [\n ](https://www. facebook. com/sharer/sharer. php? u=https://www. newstatesman. com/business/companies/2023/05/ceos-salaries-expensive-automate-robots)\n On Wednesday 31 May, it was reported that Alex Mahon, CEO of Channel 4, could receive[ record annual pay of £1. 4m](https://www. theguardian. com/uk-news/2023/may/31/channel-4-boss-alex-mahon-could-get-stations-biggest-ever-payday-of-14m). This article was originally published on 26 April 2021 and asks, as Executive pay continues to rise, does a company need a CEO at all? *\n Over the next two weeks, the boards of [BAE Systems](https://news. sky. com/story/bae-systems-on-defensive-over-golden-handcuffs-deal-for-sought-after-ceo-12280257), [AstraZeneca](https://news. sky.\n\ncom/story/looming-astrazeneca-pay-row-provides-latest-headache-for-ceo-soriot-12284677), [Glencore](https://www. reuters. com/article/glencore-ceo-pay-idUSL1N2M90QZ), [Flutter Entertainment](https://www. thetimes. co. uk/article/flutter-entertainment-shareholder-revolt-chief-executive-pay-rise-hostelworld-v6j3khkx2) and the [London Stock Exchange](https://news. sky. com/story/london-stock-exchange-faces-investor-backlash-over-chiefs-25-pay-rise-12262725) all face the possibility of shareholder revolts over executive pay at their forthcoming annual general meetings (AGMs). As the AGM season begins, there is a particular focus on pay. Executive pay is often the most contentious item at an AGM, but this year is clearly exceptional. The people running companies that have been severely impacted by [Covid-19](https://www. newstatesman.\n\ncom/tag/covid-19) can’t be blamed for the devastation of their revenues by the pandemic, but they also can’t take credit for the government stimulus that has kept them afloat. Last week, for example, nearly 40 per cent of shareholders in the estate agents Foxtons [voted against](https://propertyindustryeye. com/last-weeks-foxtons-agm-must-serve-as-a-wake-up-call-for-the-board/) its chief executive officer, Nicholas Budden, receiving a bonus of just under £1m; Foxtons has received about £7m in direct government assistance and is benefiting from the government’s [continued inflation of the housing market](https://www. newstatesman. com/business/sectors/2021/04/who-benefits-when-government-pumps-housing-market). The person who has done most to ensure Foxtons’ ongoing good fortune is not Nicholas Budden but [Rishi Sunak](https://www. newstatesman. com/tag/rishi-sunak).\n\nUnder the Enterprise and Regulatory Reform Act, executive pay is voted on at least every three years, and this process forces shareholders and the public to confront how much the people at the top take home. Tim Steiner, the highest-paid CEO in the FTSE 100, was paid £58. 7m in 2019 for running Ocado, which is [2,605 times the median income of his employees](https://www. cipd. co. uk/Images/ftse-100-executive-pay-report_tcm18-82375. pdf) for that year, while the average FTSE100 CEO makes more than £15,000 a day. [*](javascript(void);)\n Treat yourself or a friend this Christmas to a New Statesman subscription for just £2 \n [Subscribe](https://secure. newstatesman. com/W5KXPOPN)\n As the High Pay Centre’s annual assessment of CEO pay points out, a top-heavy wage bill extends beyond the CEO, and could be unsustainable for any company this year.\n\n“When one considers high earners beyond the CEO”, says the report, ”there is actually quite significant potential for companies to safeguard jobs and incomes by asking higher-paid staff to make sacrifices”. In the longer term, as companies commit to greater automation of many roles, it’s pertinent to ask whether a company needs a CEO at all. A few weeks ago Christine Carrillo, an American tech CEO, raised this question herself when she tweeted a spectacularly tone-deaf appreciation of her executive assistant, whose work allows Carrillo to “write [and] surf every day” as well as “cook dinner and read every night”. In Carrillo’s unusually frank description of the work her EA does – most of her emails, most of the work on fund",
    "source": "newstatesman.com",
    "sourceUrl": "https://www.newstatesman.com/business/companies/2023/05/ceos-salaries-expensive-automate-robots",
    "category": "AI"
  },
  {
    "title": "작업을 공개하면 행운이 따른다",
    "content": "[\n ](/readme)\n # \n Publishing your work increases your luck\n For every snarky comment, there are 10x as many people admiring your work. *\n Artwork: [Ariel Davis](https://arielrdavis. com/)\n ! [Photo of Aaron Francis](//images. ctfassets. net/s5uo95nf6njh/6o81SACP0ugjHy1SYtPCSt/4340ed9255f5e7bc8ff2e8dc5ea1d4a5/DSC_4446__1_. jpg? w=240&fm=jpg)\n ! [Tuple logo](//images. ctfassets. net/s5uo95nf6njh/7gheNu5VYCPxhHlk9lFUXP/916b37d5b5a5af0c0b2ac238ab10725d/Tuple_logo. jpeg? w=72&fm=jpg)\n **Aaron Francis** // Marketing Engineer, Tuple \n **The ReadME Project** amplifies the voices of the open source community: the maintainers, developers, and teams whose contributions move the world forward every day. - \n [\n ](https://twitter. com/intent/tweet? url=https%3A%2F%2Fgithub. com%2Freadme%2Fguides%2Fpublishing-your-work&text=)\n - \n [\n ](https://www. facebook. com/sharer/sharer. php?\n\nu=https%3A%2F%2Fgithub. com%2Freadme%2Fguides%2Fpublishing-your-work&quote=)\n - \n [\n ](https://www. linkedin. com/sharing/share-offsite/? url=https%3A%2F%2Fgithub. com%2Freadme%2Fguides%2Fpublishing-your-work)\n [\n The ReadME Project](/readme)\n ** \n No matter how hard you work, it still takes a little bit of luck for something to hit. That can be discouraging, since luck feels like a force outside our control. But the good news is that we can increase our chances of encountering good luck. That may sound like magic, but it’s not supernatural. The trick is to increase the number of opportunities we have for good fortune to find us. The simple act of publishing your work is one of the best ways to invite a little more luck into your life. Before we get into the “how,” it’s important to get on the same page about the “what. ” What are we talking about when we say “luck?\n\n” There are a lot of definitions that could apply, but let’s stick with a simple one: Luck is when something unexpected and good happens to you. Unexpected and good. Who doesn’t want to increase the odds of something unexpected and good? In our world, luck can include:\n - Having your OSS library take off\n - Being invited to speak at a conference\n - Landing a new job\n - Getting a new consulting client\n - Being invited onto a podcast\n - Making new friends in your community\n None of these things are totally in your control, which can at times feel frustrating. How can we increase the odds of finding luck? By being a person who works in public. By doing work and being public about it, you build a reputation for yourself. You build a track record. You build a public body of work that speaks on your behalf better than any resume ever could.\n\nThe goal is not to become famous, the goal is to increase the chances of luck finding us. For me, one of the most helpful ways to think about this has always been the concept of the “Luck Surface Area,” described in an [old post by Jason Roberts](https://www. codusoperandi. com/posts/increasing-your-luck-surface-area). He wrote (and note, the emphasis is mine): \n \"The amount of serendipity that will occur in your life, your Luck Surface Area, is directly proportional to the degree to which you **do something**** you’re passionate about combined with the total number of people to whom this is ****effectively communicated****. \"*\n Going further, he codifies it into a formula where:\n 1\n ```\n Luck = [Doing Things] * [Telling People]\n ```\n The more things you do multiplied by the more people you tell, the larger your Luck Surface Area becomes.\n\nThe larger your Luck Surface Area, the more likely you are to catch luck as it flows by. *\n ### Source:[ Jason Roberts](https://www. codusoperandi. com/posts/increasing-your-luck-surface-area)\n ---\n ## Doing the work\n Before you can publish your work, you have to actually do* the work. The good news for you is that by even reading this Guide on The ReadME Project, you’ve probably already self-selected into a group of people for whom “doing things” comes somewhat naturally. You’re a developer, a designer, a creator, an author, or something else entirely. Whatever moniker you want to give yourself, you’re built to *do* things, and that’s the important part. If that doesn’t ring true for you, you may fall into one of two groups:\n - You actually *are* doing things, you’ve just trained yourself to think that anything you do isn’t worth sharing.\n\n- You *want* to be doing things, but you can’t bring yourself to get started. If you’re in the first group, you may need to step back and reframe the work you’re already doing. This is a common blind spot for people who are executing at a high level! They’ve forgotten just how much they know. They think that they’re not doing anything interesting because they assume that everyone knows as much as they do. This effect is only exacerbated when everyone in your immediate vicinity is at a similar—or higher—skill level. As you become more of an expert, your quality bar gets higher and higher and you forget that everything you know is not known by everyone. If you’re in this group I want to give you a challenge: Watch the communities where you hang out",
    "source": "github.com",
    "sourceUrl": "https://github.com/readme/guides/publishing-your-work",
    "category": "AI"
  },
  {
    "title": "Google Ads는 죽었다. 이제 어디로 가야 할까?",
    "content": "# Google is dead. Where do we go now? It’s anecdotal, I know, but my main [entertainment business](https://bigtop. co. za) revenue is down 50% over the past 3 months. Our main paid source of leads was Google Ads, which have served us well over the past 10 years or so – I think I know what I am doing in adwords by now. Once per month I check the analytics, updating keywords and tweaking ad campaigns. Over the past year we increased our budget, and then I started looking at it once per week, running simultaneous campaigns with different settings, just trying to get SOMETHING. Last month Google gave us a bonus – free money! This was 5x our monthly ad spend, to spend just when we needed it most – over the December holidays. I added another new campaign, updated the budgets for the existing ones. Still no change. The last week there was money to burn, left over from unused ad spend.\n\nI increased our budget to 10x. ZERO RETURN. The money ran out. I am not putting more in. Where do we go from here? ## What is next\n Research shows that many young people are getting their information from short video platforms like TikTok and Instagram. We are trying ads on there. Our customer base is comprised of 50% returning customers (I am proud of that statistic! ). We have an email newsletter, we started sending them regularly over the past 2 months. Remember us? We also plan to do some actual physical advertising – I am going to a market next weekend, doing a free show or two, handing out cards. Also, we are branching out – I have some projects I want to make, related to the [Magic Poi project](http://www. patreon. com/c/CircusScientist), and hopefully sell. We ordered supplies last week. Right now, though – I’m broke. Anyone need a [website or IOT project built](https://devsoft.\n\nco. za)? 저는 AI 지원, 매우 빠릅니다!",
    "source": "circusscientist.com",
    "sourceUrl": "https://www.circusscientist.com/2025/12/29/google-is-dead-where-do-we-go-now/",
    "category": "AI"
  },
  {
    "title": "GOG가 원 공동 창업자에게 인수됨",
    "content": "Hey everyone, GOG Team here. **\n Today, Michał Kiciński, one of the co-founders of CD PROJEKT, and the co-founder of GOG, has acquired GOG from CD PROJEKT. # Why GOG and Michal Kicinski are getting together\n We believe the games that shaped us deserve to stay alive: easy to find, buy, download, and play forever. But time is annoyingly good at erasing them. Rights get tangled, compatibility breaks, builds disappear, and a nostalgic evening often turns into a troubleshooting session. That’s the difference between “I’m playing today” (the game lives on) and “I’ll play someday” (the game dies). As Michał put it: “GOG stands for freedom, independence, and genuine control. ”\n GOG has always been built on strong values and clear principles.\n\nWhen Marcin Iwiński and Michał Kiciński first came up with the idea for GOG in 2007, the vision was simple: bring classic games back to players, and make sure that once you buy a game, it truly belongs to you, forever. In a market increasingly defined by mandatory clients and closed ecosystems, that philosophy feels more relevant than ever. This new chapter is about doubling down on that vision. We want to do more to preserve the classics of the past, celebrate standout games of today, and help shape the classics of tomorrow, including new games with real retro spirit. # What it means for you\n First of all, DRM-free is more central to GOG than ever. Your library stays yours to enjoy: same access, same offline installers, same sense of ownership. Your data stays with GOG, and GOG GALAXY remains optional. We’ll keep our relationship with CD PROJEKT.\n\nCD PROJEKT RED games will continue to be available on [GOG](http://gog. com), and upcoming titles from the studio will also be released on the platform. If you’re a GOG Patron, or you donate to support the Preservation Program, those funds stay within GOG. Your support has been huge this year, and we think that with your help, we can undertake even more ambitious rescue missions in 2026 and 2027. We’ll have more to say about that sometime in 2026. GOG will remain independent in its operations. We will continue building a platform that’s ethical, non-predatory, and made to last, while helping indie developers reach the world. We’re also committed to giving the community a stronger voice, with new initiatives planned for 2026. Thanks for being the reason this all matters. A lot of companies sell games.\n\nFewer do the unglamorous work of making sure the games that shaped people’s lives don’t quietly rot into incompatibility. Thanks for caring about this mission with us. We’ll keep you posted as we ship, and in the meantime, you can dig into the full FAQ for the detailed answers. ## FAQ\n ### What is happening? **\n Michał Kiciński, the original co-founder of GOG and co-founder of CD PROJEKT, has acquired GOG from CD PROJEKT. GOG will continue operating as GOG, a distinct company, with the same mission to Make Games Live Forever. ### **What is GOG’s position in this? **\n To us at GOG, this feels like the best way to accelerate what is unique about GOG. Michał Kiciński is one of the people who created GOG around a simple idea: bring classic games back, and make sure that once you purchase a game, you have control over it forever.\n\nWith him acquiring GOG, we keep long-term backing that is aligned with our values: freedom, independence, control, and making games stay playable over time. ### **Why is Michał Kiciński doing this? **\n Because he wants to preserve and grow the original philosophy behind GOG. In a PC market that keeps moving toward mandatory clients and closed ecosystems, he believes GOG’s approach is more relevant than ever: no lock-in, no forced platforms, sense of ownership. His goal is to keep supporting both gamers and developers, and strengthen GOG’s mission: preserve the classics of the past, celebrate standout games of today, and help shape the classics of tomorrow. ### **Why is CD PROJEKT doing this? **\n Selling GOG fits CD PROJEKT’s long-term strategy.\n\nCD PROJEKT wants to focus its full attention on creating top-quality RPGs and providing our fans with other forms of entertainment based on our brands. This deal lets CD PROJEKT keep that focus, while GOG gets stronger backing to pursue its own mission. ### **Does the mission of GOG change? **\n No. Our mission remains to Make Games Live Forever. ### **Is DRM-free still central to GOG? **\n Yes. DRM-free is more central to GOG than ever. ### **What happens to my GOG account? **\n Nothing changes. Your account stays a GOG account. ### **Is GOG financially unstable? **\n No. GOG is stable and has had a really encouraging year. In fact, we’ve seen more enthusiasm from gamers towards our mission than ever before. ### **Will my tips or GOG Patrons donations be shared with Michał Kiciński, or any other party? **\n No.\n\n이 기금은 보존 작업을 지원하기 위해 GOG 내에 남아 있으며 게시자 또는 다른 회사와 공유되지 않습니다. # # # * * 내 라이브러리는 어떻게 되나요? * *\n 없습니다. 라이브러리를 계속 즐기실 수 있습니다.",
    "source": "gog.com",
    "sourceUrl": "https://www.gog.com/blog/gog-is-getting-acquired-by-its-original-co-founder-what-it-means-for-you/",
    "category": "AI"
  },
  {
    "title": "FastLanes – 차세대 빅데이터 파일 포맷",
    "content": "[*](/cwida/FastLanes/blob/dev/assets/logo. svg)\n [! [License: MIT](https://img. shields. io/badge/License-MIT-yellow. svg)](/cwida/FastLanes/blob/dev/LICENSE)\n **Like Parquet, but with 40% better compression and 40× faster decoding. **\n FastLanes features:\n - \n **Fully SIMD-friendly with zero explicit SIMD instructions**\n - \n **Zero dependencies**:\n no external libraries required\n ---\n ## Documentation\n [](#documentation)\n - [Typing System](/cwida/FastLanes/blob/dev/docs/typing_system. md)\n - [File Format Specification (PDF)](/cwida/FastLanes/blob/dev/docs/specification. pdf)\n ---\n ## Getting Started\n [](#getting-started)\n ### Python\n [](#python)\n ```\n import pyfastlanes\n # Connect to FastLanes\n conn = pyfastlanes. connect()\n # Convert a CSV directory to FastLanes format\n conn. inline_footer(). read_csv(\"path/to/csv_dir\"). to_fls(\"data. fls\")\n # Read back and write to CSV\n reader = conn.\n\nread_fls(\"data. fls\")\n reader. to_csv(\"decoded. csv\")\n ```\n ### C++\n [](#c)\n Add FastLanes as a dependency via CMake:\n ```\n include(FetchContent)\n FetchContent_Declare(\n fastlanes\n GIT_REPOSITORY https://github. com/cwida/FastLanes. git\n GIT_TAG dev\n )\n FetchContent_MakeAvailable(fastlanes)\n add_executable(example example. cpp)\n target_link_libraries(example PRIVATE FastLanes)\n ```\n Example usage:\n ```\n #include \"fastlanes. hpp\"\n int main() {\n fastlanes::Connection conn;\n conn. read_csv(\"data/csv_dir\"). to_fls(\"data. fls\");\n auto reader = fastlanes::Connection(). read_fls(\"data. fls\");\n reader->to_csv(\"decoded. csv\");\n return EXIT_SUCCESS;\n }\n ```\n ### Rust\n [](#rust)\n Add FastLanes Rust bindings to your `Cargo. toml`:\n ```\n [dependencies]\n fls-rs = { path = \". /rust\" }\n ```\n ```\n use anyhow::Result;\n use fls_rs::connect;\n fn main() -> Result<()> {\n let mut conn = connect();\n conn.\n\ninline_footer()\n . read_csv(\"data/csv_dir\")\n . to_fls(\"data. fls\");\n conn. read_fls(\"data. fls\")\n . to_csv(\"decoded. csv\");\n Ok(())\n }\n ```\n ### Coming Soon\n [](#coming-soon)\n - **CUDA** support for FastLanes CUDA reader\n ---\n ## Publications\n [](#publications)\n - \n **Azim Afroozeh & Peter Boncz**, “The FastLanes Compression Layout: Decoding > 100 Billion Integers per Second with\n Scalar Code,” PVLDB*, 16(9): 2132–2144, May 2023\n [Read the paper](https://www. vldb. org/pvldb/vol16/p2132-afroozeh. pdf)\n - [Source code](/cwida/FastLanes/blob/dev/publications/data_parallelized_encodings)\n - \n **Azim Afroozeh, Lotte Felius & Peter Boncz**, “Accelerating GPU Data Processing Using FastLanes Compression,” *DaMoN\n ’24*, Proceedings of the 20th International Workshop on Data Management on New Hardware, Santiago, Chile, June 2024\n [Read the paper](https://doi. org/10. 1145/3662010. 3663450)\n\n- [Source code](https://github. com/cwida/FastLanesGPU)\n - \n **Azim Afroozeh, Leonardo Kuffó & Peter Boncz**, “ALP: Adaptive Lossless Floating-Point Compression,” *SIGMOD ’24*,\n ACM SIGMOD, June 2024\n [Read the paper](https://doi. org/10. 1145/3626717)\n - [Source code](https://github. com/cwida/ALP)\n - \n **Sven Hielke Hepkema, Azim Afroozeh, Charlotte Felius, Peter Boncz & Stefan Manegold**, “G‑ALP: Rethinking\n Light‑weight Encodings for GPUs,” *DaMoN ’25*, July 2025\n [Read the paper](https://dl. acm. org/doi/pdf/10. 1145/3736227. 3736242)\n - [Source code](https://github. com/cwida/FastLanesGpu-Damon2025)\n ---\n ## How to Cite\n [](#how-to-cite)\n If you use FastLanes in your research or projects, please cite:\n ```\n @article{afroozeh2023fastlanes,\n author = {Afroozeh, Azim and Boncz, Peter},\n\ntitle = {The FastLanes Compression Layout: Decoding > 100 Billion Integers per Second with Scalar Code},\n journal = {Proceedings of the VLDB Endowment},\n volume = {16},\n number = {9},\n pages = {2132--2144},\n month = may,\n year = {2023},\n publisher = {VLDB Endowment}\n }\n @inproceedings{afroozeh2024gpu,\n author = {Afroozeh, Azim and Felius, Lotte and Boncz, Peter},\n title = {Accelerating GPU Data Processing Using FastLanes Compression},\n booktitle = {DaMoN ’24: Proceedings of the 20th International Workshop on Data Management on New Hardware},\n pages = {1--11},\n month = jun,\n year = {2024},\n organization = {ACM},\n doi = {10. 1145/3662010. 3663450}\n }\n @inproceedings{afroozeh2024alp,\n author = {Afroozeh, Azim and Kuffó, Leonardo and Boncz, Peter},\n title = {ALP: Adaptive Lossless Floating-Point Compression},\n\nbooktitle = {SIGMOD ’24: Proceedings of the 2024 ACM SIGMOD International Conference on Management of Data},\n pages = {1--13},\n month = jun,\n year = {2024},\n organization = {ACM},\n doi = {10. 1145/3626717}\n }\n @inproceedings{hepke2025galp,\n author = {Hepkema, Sven Hielke and Afroozeh, Azim and Felius, Charlotte and Boncz, Peter and Manegold, Stefan},\n title = {G‑ALP: Rethinking Light‑weight Encodings for GPUs},\n booktitle = {DaMoN ’25: Proceedings of the 21st International Workshop on Data Management on New Hardware},\n pages = {11:1--11:10},\n month = jul,\n year = {2025},\n organization = {ACM},\n url = {https://dl. acm. org/doi/pdf/10. 1145/3736227. 3736242}\n }\n @article{afroozeh2025fastlanes,\n author = {Afroozeh, Azim and Boncz, Peter},\n title = {The FastLanes File Format},\n year = {2025},\n issue_date = {July 2025},\n publisher = {VLDB Endowment},\n volume = {18},\n number = {11},\n\nISSN = {2150",
    "source": "github.com",
    "sourceUrl": "https://github.com/cwida/FastLanes",
    "category": "AI"
  },
  {
    "title": "창업자가 집중해야 할 '회사를 만드는 초석'들",
    "content": "Had **[Dharmesh Shah](https://www. linkedin. com/in/dharmesh/? ref=review. firstround. com)** kept a promise to his wife, **HubSpot** may have never come to fruition. “I had sold a couple of startups and joined the grad program at MIT Sloan to figure out my next move,” he says. After a decade of long hours and startup sacrifices, he assured his wife that he was done with his founder journey. Then, just a few months into his time at MIT, he met [Brian Halligan](https://www. linkedin. com/in/brianhalligan/? ref=review. firstround. com). There was no clear path towards a particular company idea, but Shah and Halligan found plenty of common ground in their passion for startups, tech and small business. “We came together as co-founders before forming the idea, and I convinced my wife that I had one more startup at-bat left,” says Shah.\n\nThat final at-bat turned into a grand slam hit that players dream of. Fifteen years on, the company has hit over [$1B in annual revenue and 100,000 paying customers](https://www. hubspot. com/company-news/hubspot-surpasses-100000-customers-and-1-billion-in-annual-recurring-revenue? ref=review. firstround. com). The duo stuck together through the rocketship growth, with Shah in the CTO’s chair and Halligan as CEO (until he recently [stepped down](https://www. hubspot. com/company-news/yamini-rangan-ceo-brian-halligan-executive-chairman? ref=review. firstround. com) in 2021). For a repeat founder with such zeal for the “starting something new” days, you might not expect that, as Shah puts it, he’s having a better time at HubSpot now — 15 years in, with 5,000 employees. “Those early years can be fun — you’re super nimble and can execute quickly.\n\nBut there’s something to be said for having scaled and being at the grownups’ table. You have the money, the people and the talent that you can put up against an idea, in a way that you couldn’t as a 10-person scrappy startup,” says Shah. In this exclusive interview, Shah traces back the 15-year journey building HubSpot and highlights five company-building cornerstones that founders face and shares his advice — from choosing a co-founder, architecting your role and bringing aboard the early team, to getting feedback as the company grows and crafting a culture that will scale. Let’s dive in. ## **CORNERSTONE #1: SELECTING YOUR CO-FOUNDER — STAY STRATEGIC TO MAKE SURE YOUR STARTUP GOES THE DISTANCE. **\n When it comes to [choosing a co-founder](https://review. firstround.\n\ncom/Looking-for-Love-in-All-The-Wrong-Places-How-to-Find-a-Co-Founder/), Shah points to what seems like an obvious misstep — but it’s one he sees founding teams make over and over again. “People assess the idea, and they’ll maybe go have a beer together, but you have to really interrogate whether you enjoy spending time with this person,” he says. While at MIT, the future HubSpot co-founders were extremely methodical about assessing their co-founder fit. “After we met, Brian and I knew we might want to start a company together someday, so every class in our grad program that had a project, we intentionally put ourselves on the same team,” says Shah. “To take it even one step further, we made sure every project could tie back to this startup that we called HubSpot. It wasn’t a company at a time — it was just an idea that was constantly shifting.\n\nThe goal was to simulate what it would be like to work with this person for eight hours at a time. ”\n **In our personal relationships, you’ll often date for a long time before you make a lifelong commitment. But when it comes to co-founders, people aren’t as deliberate about getting to know potential partners. While the academic environment is a useful tool to get to know a potential co-founder, Shah outlines a couple of other avenues. “Take an online class or find a short-term project to work on together — something small that helps you understand what makes each other tick,” he says. ### Look for a Venn diagram, not two overlapping circles. **\n Another common tripwire? Looking for your co-founder clone. “We gravitate to the people who are most like us, but it’s critical that you have different skill-sets.\n\nBrian’s background was in sales and marketing, while I grew up in the product and engineering orgs,” says Shah. That said, common values and working patterns are key. - **Passion for learning: **“To this day, we will send each other emails at 1 or 2 AM with a new book recommendation (and we’re both night owls — I’m not sure if the relationship would have thrived otherwise). We know what’s on each other’s reading list, we discuss talks we watched. This extended to our early employees, who were similarly academically-oriented,” says Shah. - **The power of healthy debate: **“In the early days, we could pick anyone on the management team and say, ‘We’ve got this issue to work through, argue this side of the debate,’ even if they didn’t necessarily agree with that particular side. That process of learning together and pushing each other, even if we disagreed, helped k",
    "source": "review.firstround.com",
    "sourceUrl": "https://review.firstround.com/the-company-building-cornerstones-every-founder-needs-to-focus-on-advice-from-hubspots-dharmesh-shah",
    "category": "AI"
  },
  {
    "title": "Unity의 Mono 문제: 왜 당신의 C# 코드는 기대보다 느리게 실행되는가",
    "content": "# Unity's Mono problem: Why your C# code runs slower than it should\n Posted 27 Dec 2025\r[\r*\r](/blog/mono-vs-dot-net-in-unity/img/MonoVsDotNet-thn. 551. jpg)\rBy\rMarek Fiser *\r*10 min* read\rExecution of C# code in Unity’s Mono runtime is slow by today’s standards, much slower than you might expect! Our game runs 2-3x faster on modern . NET compared to Unity’s Mono, and in a few small benchmarks I measured speedups of up to 15x. I’ve spent some time investigating what’s going on and in this article I will present my findings and why everyone should want Unity’s . NET modernization to become production-ready as soon as possible. ## How did we get here\n Unity uses the Mono framework to run C# programs and back in 2006 it was one of the only viable multi-platform implementations of . NET. Mono is also open-source, allowing Unity to do some tweaks to better suit game development.\n\nAn interesting twist happened nearly 10 years later. In 2014, Microsoft began open-sourcing . NET (notably . NET Core later that year) and in June 2016, . NET Core 1. 0 shipped with official cross-platform support. Since then, the . NET ecosystem gained momentum and lots of improvements have been made, including the Roslyn compiler platform, a new JIT (just-in-time compiler), performance improvements, more features, etc. In 2018, Unity engineers discussed that they are working on porting the engine to . NET CoreCLR, the multi-platform version of Common Language Runtime (CLR), a component that runs . NET programs. Their main motivations behind this project were performance and convergence. In their post they said:\r**. . .\n\nCoreCLR could be great for Unity game developers, as it will provide a significant boost in performance, by an order of 2x to 5x compare to the Mono runtime sometimes up to x10 on some workload! Unfortunately, now it’s the end of 2025 and we still can’t run games on CoreCLR. ## The performance gap\n We don’t hear about the performance gap between Mono and . NET much, likely because it is not possible to run games written for Unity under modern . NET. But we can still do a direct comparison with code that does not depend on Unity directly. Our game has a unique architecture – we strictly separate the game simulation code (business logic) from rendering. So much so that the simulation code does not depend on Unity’s libraries and can be compiled and run under any . NET version.\n\nOne day I was debugging an issue in map generation and it was time-consuming because it was taking over 2 minutes to start a game. To make debugging faster, I’ve written a unit test, hoping to cut down on the turn-around time since Unity takes 15+ seconds just to crunch new DLLs and reload the domain before the game can be launched and it also initializes rendering stuff that I did not care about. When I ran the test, it finished in 40 seconds. I was quite surprised that it was more than 3x faster, so I started digging deeper. Long story short, Figure 1 shows traces from a profiler showing the difference between the game launching in Unity running under Mono vs. a unit test running under . NET. *Note that all shown benchmarks are using either Unity 6. 0 or . NET 10. *\r- *Unity editor compiled in Debug mode: 100 seconds. - . NET unit test compiled in Debug mode: 38 seconds.\n\nFigure 1: A trace of game startup in Unity and . NET, Debug mode. So our benchmark shows that loading a save file, generating a map, and initializing the simulation takes 100 seconds in Unity/Mono but only 38 seconds in . NET. This result alone is already something that may raise eyebrows and has real consequences of how you may want to approach debugging and testing. I also know from experience with Unity that Release mode running as a standalone executable (without the Unity editor) is much faster, so I decided to test that next. ## . NET vs. Mono in standalone Release mode\n Debug mode slowness is not great, but even non-optimized C++ code can be slow. To compare the real performance gap between Mono and . NET, let’s run the same benchmark as above but in release mode, standalone executable. First up: Unity. I’ve run our deploy script to get an optimized executable and run it directly.\n\nUnsurprisingly, optimized standalone executable is beating Unity editor by a big margin, more than 3x faster. Next, the same code running under . NET in Release mode. Figure 2 shows the results. - Mono as standalone executable compiled in Release mode: 30 seconds. - . NET unit test compiled in Release mode: 12 seconds. Figure 2: A trace of game startup in Unity and . NET, Release mode\n Yep. 12 seconds. It’s actually mind-boggling how much work is being done in these 12 seconds and when I saw this for the first time, I was not only shocked, but also impressed. Just so you know, a 4k × 4k map is being generated using all available threads out of hundreds of combined noise functions in like 3 seconds. Figure 3 shows the trace expanded. Figure 3: The expanded trace of the 12-second run from above. The blue boxes after the highlighted are",
    "source": "marekfiser.com",
    "sourceUrl": "https://marekfiser.com/blog/mono-vs-dot-net-in-unity/",
    "category": "AI"
  },
  {
    "title": "자바스크립트를 HTML만으로 대체하기",
    "content": "# Replacing JS with just HTML\n by [Aaron T. Grogg](https://aarontgrogg. com) published on Dec 27, 2025\n For many years now, JavaScript has been the workhorse of the web. If you wanted to do something that couldn't be done with just HTML and CSS, you could usually find a way to do it with JS. **And that is great! JS has helped push user experiences forward, and honestly helped push HTML and CSS forward! But as time marches on, and the HTML and CSS methods [gain traction](https://webstatus. dev/? q=%28group%3Ahtml+OR+css%29&sort=name_asc), we need to start replacing the old JS methods that feel so comfy with new methods that require less JS. *Nothing against JS***, but it has better things to do than setup and manage your accordions or offscreen navigation menus. . .\n\nPlus, JS needs to be downloaded, decompressed, evaluated, processed, and then often consumes memory to monitor and maintain features. If we can *hand-off* any JS functionality to native HTML or CSS, then users can download less stuff, and the remaining JS can pay attention to more important tasks that HTML and CSS can't handle (yet). Below are a few examples; any you care to add? ## Table of Contents:\n - [Accordions / Expanding Content Panels](#accordions-expanding-content-panels)\n - [Input with Autofilter Suggestions Dropdown](#input-with-autofilter-suggestions-dropdown)\n - [Modals / Popovers](#modals-popovers)\n - [Offscreen Nav / Content](#offscreen-nav-content)\n ## Accordions / Expanding Content Panels\n ### Description:\n The `details` and `summary` HTML elements provide an HTML-only replacement to the typical JS accordion:***\n CopePen: [Accordion / Expanding Content](https://codepen.\n\nio/aarontgrogg/pen/GgoOqVX)\n ### Use cases:\n - Hiding/showing content\n - Expanding content sections\n ### Basic implementation:\n ```html\n <details>\n <summary>Initially closed, click to open</summary>\n Content is initially hidden, but can be revealed by clicking the summary. </details>\n ```\n Add an `open` attribute to set the default appearance as \"open\":\n ```html\n <details open>\n <summary>Initially open, click to close</summary>\n Content is initially visible, but can be hidden by clicking the summary. </details>\n ```\n Use the same `name` attribute on all related `details` (like radio buttons) to restrict only one open panel at a time:\n ```html\n <details name=\"foo\" open>\n <summary>Initially open, clicking others will close this</summary>\n Content is initially visible, but can be hidden by clicking the summary; only one panel can be open at a time. </details>\n <details name=\"foo\">\n\n<summary>Initially closed, clicking will open this, and close others</summary>\n Content is initially hidden, but can be revealed by clicking the summary; only one panel can be open at a time. </details>\n <details name=\"foo\">\n <summary>Initially closed, clicking will open this, and close others</summary>\n Content is initially hidden, but can be revealed by clicking the summary; only one panel can be open at a time. </details>\n ```\n You can also customize the appearance with CSS and trigger the open/close via JS. Learn more about the `details` element in the previously-published “[For the Love of <details>](https://www. htmhell. dev/adventcalendar/2025/23)\". ### Resources:\n - [MDN `details` page](https://developer. mozilla. org/en-US/docs/Web/HTML/Element/details)\n - [<details> element](https://www. codewithshripal. com/playground/html/details-element)\n ### Browser compatibility:\n\n- [MDN `details` Browser Compatibility](https://developer. mozilla. org/en-US/docs/Web/HTML/Element/details#browser_compatibility)\n ## Input with Autofilter Suggestions Dropdown\n ### Description:\n Combining the HTML `input` and `datalist` elements can create a dropdown of options that autofilters as you type:\n ! [Examples of HTML and providing autofilter dropdown](input-with-datalist. gif)\n CodePen: [Input with Autofilter Suggestions Dropdown](https://codepen. io/aarontgrogg/pen/yyePPor)\n ### Use cases:\n - Site search\n - Product search or filter\n - Filter any list of data\n ### Basic implementation:\n ```html\n <label for=\"browser\">Browser</label>\n <input type=\"text\"\n list=\"browsers\"\n id=\"browser\" name=\"browser\"\n size=\"50\"\n autocomplete=\"off\" />\n <datalist id=\"browsers\">\n <option value=\"Arc\"></option>\n <option value=\"Brave\"></option>\n <option value=\"Chrome\"></option>\n\n<option value=\"DuckDuckGo\"></option>\n <option value=\"Firefox\"></option>\n <option value=\"Microsoft Edge\"></option>\n <option value=\"Opera\"></option>\n <option value=\"Safari\"></option>\n <option value=\"Tor\"></option>\n <option value=\"Vivaldi\"></option></option>\n </datalist>\n ```\n You can also use other input types:\n ```html\n <label for=\"quantity\">Quantity</label>\n <input type=\"number\" \n list=\"quantity-options\"\n id=\"quantity\" name=\"quantity\" />\n <datalist id=\"quantity-options\">\n <option value=\"1\"></option>\n <option value=\"2\"></option>\n <option value=\"5\"></option>\n <option value=\"10\"></option>\n <option value=\"20\"></option>\n <option value=\"50\"></option>\n </datalist>\n <label for=\"appointment\">Appointment</label>\n <input type=\"time\" \n list=\"appointments\"\n id=\"appointment",
    "source": "htmhell.dev",
    "sourceUrl": "https://www.htmhell.dev/adventcalendar/2025/27/",
    "category": "AI"
  },
  {
    "title": "AI를 비용 낭비 없이 운영하기 위한 실전 통합 모범 사례",
    "content": "# AI Best Practices for Bootstrappers (That Actually Save You Money)\n [November 28, 2025November 27, 2025](https://thebootstrappedfounder. com/ai-best-practices-for-bootstrappers-that-actually-save-you-money/) ~ [Arvid Kahl](https://thebootstrappedfounder. com/author/arvid53d1a2b65a/) \n Reading Time: 19 minutes\n I recently realized something while building Podscan, my podcast database system that does a lot of background data extraction and analysis for my users. I’ve stumbled upon a couple of AI integration best practices that a lot of people might not be fully aware of. So today, I want to dive into the concepts I found not just useful, but essential for maintaining and operating mission-critical data handling with LLMs and AI platforms. *\n A quick word from our Sponsor, [Paddle. com](http://paddle. com/). I use Paddle as my Merchant of Record for all my software projects.\n\nThey take care of all the taxes, the currencies, tracking declined transactions and updated credit cards so that I can focus on dealing with my competitors (and not banks and financial regulators). If you think you’d rather just build your product, check out [Paddle. com](http://paddle. com/) as your payment provider. *\n I was reminded of the AI practices I have established a tweet I read from Greg Eisenberg. He said that keeping up 100% with all the new AI tools and models is impossible, and that something that works today might fail tomorrow. That observation has been my biggest learning, and I turned it into not just a process, but an implementation style. Let me share what I’ve built. ## The Migration Pattern\n\nWherever I use an AI call—be that to a local model or a cloud model on OpenAI, Anthropic, Google Gemini, wherever it might be—I always have a migration pattern implemented in the code. I extract all of my API calls into services. These services internally handle all the connection, all the prompt massaging and prompt construction, in addition to the specific prompt I want for each API call. And these services always operate on what I call a state of permanent migratability. That means they can always use the latest version and the latest model, or at the very least, the version of the prompt and model that I used previously. I realized I needed this when I started implementing GPT-5 a couple of months ago after having used GPT 4. 1 for a long while. The first experiments were horrible.\n\nA lot of API changes happened, and a lot of nuanced parts of the prompt didn’t work as well as they did before. My prompt that was perfectly crafted for 4. 1—which it ran on for at least half a year—was just not reusable for GPT-5. Here’s what happened in the details: my prompt for GPT 4. 1 had a JSON format expectation, which is part of the OpenAI API where you can say, “I want this to be a JSON that comes back,” and you’re guaranteed to get JSON. Everything in the prompt was aimed at creating that JSON most effectively. When I migrated that call to GPT-5, it would still create JSON data, but it would drop certain keys. It wasn’t as reliable as the 4. 1 version of that prompt. I tried to figure out the reason, and it turned out that GPT-5 was more aimed at using structured JSON schemas.\n\nInstead of just saying “this is going to be JSON,” you say “this is the exact JSON that you’re going to output,” and you define the keys and the potential values. Then you explain in your prompt how these are filled. It’s fundamentally similar to what it was before, but technically different. Since I didn’t just want to deploy new experiments with a new model and hope they produced the same data, I came up with this migration strategy: for either a certain time period or for my testing and staging environments, I could say, “Use the old prompt with the old model, and then use the new prompt with the new model. ” Maybe even a completely different data structure and a completely different API call—because OpenAI also wants people to move away from the chat-style API to the response-style API.\n\nI would log both results, see if there were major differences, and if there were, the system would tell me and show me the diff, making it accessible for debugging. Then it would respond with the new data to whatever function or procedure called it. For those servers running this dual approach, they incurred twice the cost. But I was able to see what the old model did, what the new model did, and how the differences between prompts would impact the quality, expectability, and reliability of the data. I found this extremely useful for almost anything you do with AI tool calls where you’re using an LLM to get some kind of response or data. Unless it’s purely conversational, where this probably doesn’t matter as much.\n\n그러나 배경 분석, 데이터 번호 크런칭, 의미 분석을 수행하는 경우 테스트 목적으로 두 버전을 모두 실행하고 두 결과를 모두 볼 수 있는 마이그레이션 패턴을 갖는 것이 실제로 도움이 됩니다. 새로운 결과가 생각보다 좋지 않다면 항상",
    "source": "thebootstrappedfounder.com",
    "sourceUrl": "https://thebootstrappedfounder.com/ai-best-practices-for-bootstrappers-that-actually-save-you-money/",
    "category": "AI"
  }
]